For comparison: which Benchmark Dataset fits the problem the best?
\begin{itemize}
    \item~\cite{liao_mask_2020}:
        \begin{itemize}
            \item Rotated ICDAR 2013 (changed normal icdar): rotation robustness
            \item Total-Text: shape ropustness
            \item MSRA-TD500: aspect ratio ropustness
        \end{itemize}
    \item~\cite{yang_learning_2021}: commonly used for oriented text: ICDAR2015, ICDAR2017 MLT,
        MSRA-TD500
\end{itemize}

Scene text detection and recognition~\cite{long_scene_2021}
Detection:
\begin{itemize}
    \item sub-text components: \\
        better flexibility and generalization over shapes and aspect ratios\\
        drawback: module or post-processing step used to group segments into text instances
        may be vulnerable to noise and the efficiency
\end{itemize}
Recognition
\begin{itemize}
    \item CTC:\ less dependant on language models and has better character to pixel alignment
    \item Encoder-Decoder: decoder is an implicit language model: can incorporate more linguistic priors
    \item both: assume text is straight and can therefore not adapt to irregular text
        challenge: represent oriented characters and curved text that are distributed over a
        2-dimensional space
            (rather than 1-dim/horizontal) in order to fit decoding modules (whose decodes require
            1-dimensional inputs)
        \item evaluation of recognition methods falls behind the time robustness of recognition when
            cropped with slightly differend bounding box is seldom verified
\end{itemize}
Character level annotations are more accurate and better. However, most existing datasets do not
provide character- level annotating. Since characters are smaller and close to each other,
character-level annotation is more costly and inconvenient.

%%%%%%%%%%%%%%%%%%%%%%%
%% NOTES for comparison
%%%%%%%%%%%%%%%%%%%%%%%
Detection
\begin{itemize}
    \item reference tables found in overview!
        \begin{itemize}
            \item only curved or multi-orientated text solutions
            \item mostly focused on performance/robustness rather than efficiency
            \item for regresseion based: how to deal with orientation?
            \item segmentation mostly curved, regression mostly oriented
            \item while BB methods deal with orientation representations, pixel segmentation methods
                mostly find innovations that help separate text instances
            \item how to distinguish text instances (neighbor~\citep{deng_pixellink_2018},
                textfields~\citep{xu_textfield_2019},segmentation scales~\citep{wang_shape_2019})
        \end{itemize}
    \item~\cite{wang_efficient_2019}: two main challenges for \ac{STD}:
        trade-off between speed and accuracy; model arbitrary-shaped text instance
    \item BB regression
        \begin{itemize}
            \item~\cite{ferrari_textsnake_2018}: many methods have strong assumption that text
                instances are in linear shape and therefore adopted simple representations
                (rectangles --- axis aligned or rotated, quadrangles)
                $\rightarrow$ problem with irregular and curved text
            \item~\citep{long_scene_2021}: Curved text
            \item~\cite{long_scene_2021,shi_detecting_2017}: Higher aspect ratios
            \item~\cite{shi_detecting_2017}: Text orientation
            \item~\cite{wang_shape_2019}: fail to deal with curved text
            \item~\cite{xu_textfield_2019}: anchor box bias assumes text has linear shape
                $\rightarrow$ bottleneck for adapting to curved
            \item One Stage
                \begin{itemize}
                    \item~\cite{lu_mimicdet_2020}: more straightforward architecture
                        $\rightarrow$ more efficient
                \end{itemize}
            \item Two Stage
                \begin{itemize}
                    \item~\cite{lu_mimicdet_2020}: more acurate/performant $\rightarrow$ refinement
                \end{itemize}
        \end{itemize}
    \item Segmentation
        \begin{itemize}
            \item~\cite{xu_textfield_2019}: many segmentation based methods focus on separating
                text regions
            \item~\cite{wang_shape_2019}: distinguishing text instances is hard
            \item~\cite{liao_real-time_2019}: require complex post-processing for grouping
                pixel-level prediction results into text instances
            \item~\cite{xie_aggregation_2019}: often need time consuming post-processing steps
                while performance is unsatisfying
            \item~\cite{dai_fused_2018}: well suited for arbitrary shapes but often involves several
                pipelines $\rightarrow$ inefficient
            \item~\cite{ferrari_textsnake_2018}: some models are specifically designed for
                arbitrary shapes
            \item~\cite{shi_detecting_2017}: modular approach instead of whole \acp{BB}
                $\rightarrow$ good with long aspect ratio and orientation because of flexibility
            \item~\cite{dai_fused_2018}: often need several stages and components for final result
                $\rightarrow$ inefficient
            \item~\cite{qiao_text_2021}: often relies on complicated postprocessing to separate
                different text instances
            \item~\cite{long_scene_2021}: better flexibility and generalization over shapes and
                aspect ratios
            \item~\cite{long_scene_2021}: module or post-processing step to group segments
                into text instances is vulnerable to noise and efficiency
        \end{itemize}
\end{itemize}

Recognition
\begin{itemize}
    \item reference tables found in overview!
        \begin{itemize}
            \item mostly attention based
            \item concerned with curved text $\rightarrow$ a lot of rectification innovation
            \item both unconstrained and with lexicons
        \end{itemize}
    \item Segmentation based
        \begin{itemize}
            \item~\cite{xie_aggregation_2019}: require separate training targets for each segment
                or time-step in the input sequence $\rightarrow$ inconvenient pre-segmentation and
                post-processing stages
            \item~\cite{shi_end--end_2017}: character annotations needed for training
            \item~\citep{chen_text_2021}: Accurate detection of individual characters is hard
            \item~\citep{chen_text_2021}: Contextual information between character gets lost
                $\rightarrow$ poor word-level results
            \item~\cite{cheng_aon_2018}: require segmentation of each character which is challenging
                with scene backegrounds
            \item~\cite{cheng_aon_2018}: require character annotations $\rightarrow$ time consuming
            \item~\cite{zhan_esir_2019}: localizing individual characters is resource hungry and
                prone to errors (complex background and character overlap)
            \item~\cite{liu_abcnet_2020}: require complex pipeline $\rightarrow$ slow interference
        \end{itemize}
    \item Sequence based
        \begin{itemize}
            \item~\cite{li_show_2019}: deal with curved text by rectifying into 1d text
            \item~\cite{long_scene_2021} --- challenge: represent oriented and curved text
                distributed over 2d space $\rightarrow$ decoding modules need 1d input
            \item~\cite{cheng_aon_2018}: image is encoded into 1D based sequence
                $\rightarrow$ cannot handle 2d text
                $\rightarrow$ apply text rectification to make 2d text 1d
            \item~\cite{shi_end--end_2017}: directly learned from sequence labels (words/instances)
            \item~\cite{xie_aggregation_2019}: no need for separate training targets for each segment
                or time step
            \item~\cite{liao_scene_2018}: limitations caused by difference between one dimensional
                distribution of feature sequences and two dimentional distribution of text in
                images
            \item~\cite{liao_scene_2018}: dominates current \ac{STR} approaches
            \item CTC
                \begin{itemize}
                    \item~\cite{chen_text_2021}:CTC prone to overfitting
                    \item~\cite{xie_aggregation_2019}: implementation of forward-backward algorithm
                        is complicated $\rightarrow$ large computation consumption
                    \item~\cite{cheng_focusing_2017,xie_aggregation_2019}: can hardly be applied
                        to 2D prediction problems
                    \item~\cite{long_scene_2021}: better character to pixel alignment
                    \item~\cite{cong_comparative_2019}: perform better on sentence recognition tasks
                \end{itemize}
            \item Attention
                \begin{itemize}
                    \item~\cite{cong_comparative_2019}: perform better on isolated word recognition
                        tasks
                    \item~\cite{wan_vocabulary_2020}: prove weak in generalizing to words
                        outside vocabulary, but strong with vocabulary
                    \item~\cite{bai_edit_2018}: attention can lead to missing or superfluous
                        characters because of attention drift
                    \item~\cite{liao_scene_2018}: misalignment between ground truth strings and
                        output probability distribution caused by missing or superfluous characters
                    \item~\cite{chen_text_2021}: good for isolated word recognition
                    \item~\cite{chen_text_2021}: Attention has problems with long sequences
                    \item~\cite{xie_aggregation_2019}: relies on attention module for label alignment
                        $\rightarrow$ large storage requirement and computation consumption
                    \item~\cite{xie_aggregation_2019}: misalignment problem can confuse and mislead
                        training problem, leading to degradation of accuracy
                    \item~\cite{xie_aggregation_2019}: can be adopted to 2D prediction but
                        memory and time consumption are to big then
                    \item~\cite{cheng_focusing_2017}: attention drift bad
                    \item~\cite{long_scene_2021}: more conducive to incorporate more linguistic
                        priors (decoder is implicit language model)
                    \item~\cite{chen_text_2021}: attention modul is storage and computation
                        intensive
                    \item~\cite{chen_text_2021}: attention drift for long text instances can lead
                        to misalignment
                \end{itemize}
        \end{itemize}
\end{itemize}

E2E
\begin{itemize}
    \item 2 step
        \begin{itemize}
            \item~\cite{chen_text_2021}: errors can accumulate between STD and STR
            \item~\cite{long_scene_2021}: propagation of error between detection and recognition
            \item~\cite{qiao_text_2021}: pipelined approaches (not e2e) have suboptimal performance
                because errors from recognition cannot be utilized for optimizing detection
            \item~\cite{liu_fots_2018}: feature extaction usually takes the most time
                $\rightarrow$ having to do it twice is expensive
        \end{itemize}
    \item 2 stage
        \begin{itemize}
            \item~\cite{chen_text_2021}: joint optimization
            \item~\cite{chen_text_2021}: can prevent errors from being accumulated during training
            \item~\cite{chen_text_2021}: share information between STD and STR
                $\rightarrow$ optimize jointly to improve performance
            \item~\cite{chen_text_2021}: exhibit competitive performance with faster interference
                and smaller storage requirements
            \item~\cite{liu_fots_2018}: sharing features significantly speeds up the network
                by not needing a second \ac{CNN} feature extractor
            \item~\cite{chen_text_2021}: real-time and efficient
            \item~\cite{chen_text_2021}: competitive performance with faster interference and
                smaller storage requirements
        \end{itemize}
\end{itemize}

\section{Reflection}
Threats to validity!
\begin{itemize}
    \item~\cite{arpteg_software_2018}: different papers have different components
        $\rightarrow$ Hardware, Platform, Source Code, Configuration
        $\rightarrow$ studies can't really be compared
    \item~\cite{arpteg_software_2018}: `A major challenge in developing DL systems is the
        difficulties in estimating the results before a system has been trained and tested.'
    \item~\cite{long_scene_2021}: different interpretations of metrics (matching for \ac{STD},
        word/char for \ac{STR})
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: all entities of
        \ac{MLS} should be inspected when developing a solution
    \item~\cite{baek_what_2019}: different papers use different evaluation and testing environments
    \item~\cite{baek_what_2019}: different papers use different subsets of the same dataset
        $\rightarrow$ discrepancies in performance
    \item~\cite{long_unrealtext_2020}: half of the widely adopted benchmark datasets have imperfect
        annotations $\rightarrow$ ignoring case-sensitivities and punctuations, and provide new
        annotations for those datasets
    \item~\cite{chen_text_2021}: inconsistency of datasets, priors and testing environments make
        comparison difficult
    \item cutoff at 100 cit $\rightarrow$ newer publications might be equally impactful but not
        not enough time for more citations
\end{itemize}

\section{Outlook}

\begin{itemize}
    \item~\cite{watanabe_preliminary_2019}:next steps to practically solve problem
        $\rightarrow$  Data Collection, Data Cleaning, Data Labeling, Model Training,
        Model Evaluation, Model Deployment, Model Monitoring
    \item~\cite{zhao_improving_2020}: Use Neural Architecture Search to automatically find right
        feature extractor
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: build system around
        model $\rightarrow$ e.g.\ supervision mechanism
    \item~\cite{shi_icdar2017_2017,he_icpr2018_2018}: adjust field to better metrics for evaluation
    \item~\cite{long_scene_2021}: general trend to move towards simpler,shorter pipeline
\end{itemize}
