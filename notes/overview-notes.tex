no transformers $\rightarrow$ self-attention mechanism is too computationally expensive???

model-pruning $\rightarrow$ remove connections for better performance

`The great advances that have been made in fields such as computer vision and speech recognition,
have been accom- plished by replacing a modular processing pipeline with large neural networks
that are trained end-to-end [37]. In essence, transparency is traded for accuracy.
This is an unavoidable reality.'\citep{arpteg_software_2018}


Two models that can be used in conjunction
\textbf{detection}~\citep{beom_text_2021}\\
uses RetinaNet structure~\citep{lin_focal_2018}\\
applies techniques from textboxes++~\citep{liao_textboxes_2018}

\textbf{character recognition}~\citep{beom_crnn_2021}\\
needs cropped text area as input\\
uses CRNN~\citep{shi_end--end_2015} $\rightarrow$ end-to-end learning, LSTM fir arbitrary length of
input and output, no need to apply detection and cropping to each single character

Open Source OCR engine~\citep{smith_overview_2007}
\begin{itemize}
    \item uses Deep Learning (found c++ code for layers in repo)
    \item Processing in step-by-step pipeline, some unusual stages\\
        1. Line and Word finding\\
        1.1. Line finding\\
        1.2. Baseline Fitting\\
        1.3. Fixed Pitch Detection and Chopping\\
        1.4. Proportional Word Finding\\
        2. Word Recognition\\
        2.1 Chopping Joined Characters\\
        2.2 Accociating Broken Characters\\
        3. Static Character Classifier\\
        3.1 Features\\
        3.2 Classification\\
        3.3 Training Data\\
        4. Linguistic Analysis\\
        5. Adaptive Classifier
\end{itemize}
Performs poorly with unstructured text with significant noise

An Efficient and Accurate Scene Text Detector~\citep{zhou_east_2017}

SOFT:\ Softmax-free Transformer with Linear Complexity~\citep{lu_soft_2021}

Generative Pretraining from Pixels~\citep{chen_generative_2021}
\begin{itemize}
    \item unsupervised representation learning (approach transfered from NLP)
    \item training of sequence Transformer to auto-regressively predict pixels without incorporating
        knowledge of 2D input structure
    \item Active part: GPT-2 scale model learns image representations and performs extremely well even
        when compared to supervised models
\end{itemize}

Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler
Divergence~\citep{yang_learning_2021}
\begin{itemize}
    \item Deductive approach to rotated object detection
    \item box is `translated' to 2D-Gaussian $\rightarrow$ KLD with prediction and true gaussian as Loss
    \item LIMIT:\ cannot be directly applied to quadrilateral detection
\end{itemize}

DP-SSL:\ Towards Robust Semi-supervised Learning with A Few Labeled Samples~\citep{xu_dp-ssl_2021}
\begin{itemize}
    \item Semi-supervised learning:
        \begin{itemize}
            \item provides way to leverage unlabeled data by pseudo labels
            \item performs poorly and unstable when size of labeled data is very small (low quality
                of pseudo labels)
        \end{itemize}
    \item Data programming:
        \begin{itemize}
            \item paradigm for the programmatic creation of training sets
            \item existing methods rely on human experts to provide initial labeling functions (LF)
        \end{itemize}
    \item DP-SSL
        \begin{itemize}
            \item multiple-choice learning (MCL) based approach to automatically generate labeling functions
            \item scheme to generate probabilistic labels for unlabeled data
        \end{itemize}
\end{itemize}

which aspects to compare? quantitative, qualitative
