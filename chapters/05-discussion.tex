\chapter{Discussion}\label{ch:discussion}
\section{Analysis}\label{se:analysis}
Go down hierarchy and compare most important advances
\begin{itemize}
    \item Compare: two step, two stage
    \item Compare: innovations for specific categories
\end{itemize}

Comparing numbers on benchmarks and not looking at quanitative data would result in fallacy.

\begin{table}[ht]
    \centering\scriptsize
    \begin{tabular}{p{.1\textwidth}p{.15\textwidth}p{.1\textwidth}p{.55\textwidth}}
        Task & \multicolumn{2}{c}{Approach} & Shortcommings \\
        \toprule
        STD & Seg free
              & & Curved text~\citep{long_scene_2021} \\
            & & & Higher aspect ratios~\citep{long_scene_2021,shi_detecting_2017}\\
            & & & Text orientation~\citep{shi_detecting_2017} \\
            & & 1-stage & \\
            & & 2-stage & \\
            & Seg based & & \\
            & & Pixel-level & \\
            & & Component-level & \\
            & & Character-level & \\
        \midrule
        STR & Seg based
              & & Accurate detection of individual characters is hard~\citep{chen_text_2021} \\
            & & & Contextual information between character gets lost~\citep{chen_text_2021}\\
            & Seg free & & \\
            & & CTC based &  \\
            & & Attention based
                & Sentence recognition \\
        \midrule
        E2E & 2-step & & \\
            & Parallel & & \\
        \bottomrule
    \end{tabular}
    \caption{Tasks, method categories and their shortcomings\label{tb:steps-properties}}
\end{table}

For comparison: which Benchmark Dataset fits the problem the best?
\begin{itemize}
    \item~\cite{liao_mask_2020}:
        \begin{itemize}
            \item Rotated ICDAR 2013 (changed normal icdar): rotation robustness
            \item Total-Text: shape ropustness
            \item MSRA-TD500: aspect ratio ropustness
        \end{itemize}
    \item~\cite{yang_learning_2021}: commonly used for oriented text: ICDAR2015, ICDAR2017 MLT,
        MSRA-TD500
\end{itemize}

E2E
\begin{itemize}
    \item 2 step
        \begin{itemize}
            \item~\cite{chen_text_2021}: errors can accumulate between STD and STR
            \item~\cite{long_scene_2021}: propagation of error between detection and recognition
        \end{itemize}
    \item 2 stage
        \begin{itemize}
            \item~\cite{chen_text_2021}: can prevent errors from being accumulated during training
            \item~\cite{chen_text_2021}: share information between STD and STR
                $\rightarrow$ optimize jointly to improve performance
            \item~\cite{chen_text_2021}: exhibit competitive performance with faster interference
                and smaller storage requirements
        \end{itemize}
\end{itemize}

Detection
\begin{itemize}
    \item~\cite{wang_efficient_2019}: two main challenges for STD:
        trade-off between speed and accuracy; model arbitrary-shaped text instance
    \item BB regression
        \begin{itemize}
            \item~\cite{ferrari_textsnake_2018}: many methods have strong assumption that text
                instances are in linear shape and therefore adopted simple representations
                (rectangles --- axis aligned or rotated, quadrangles)
                $\rightarrow$ problem with irregular and curved text
        \end{itemize}
    \item Segmentation
        \begin{itemize}
            \item~\cite{ferrari_textsnake_2018}: some models are specifically designed for
                arbitrary shapes
            \item~\cite{shi_detecting_2017}: modular approach instead of whole \acp{BB}
                $\rightarrow$ good with long aspect ratio and orientation because of flexibility
            \item~\cite{dai_fused_2018}: often need several stages and components for final result
                $\rightarrow$ inefficient
        \end{itemize}
\end{itemize}

Recognition
\begin{itemize}
    \item research~\cite{wan_vocabulary_2020} $\rightarrow$ vocabulary reliance
    \item Ability to cope with 2d text:
        CTC has problems,
        Attention/Encoder-Decoder based can be extended to work
    \item never use lexicons $\rightarrow$ alphanumeric strings
    \item Segmentation based
        \begin{itemize}
            \item~\cite{xie_aggregation_2019}: require separate training targets for each segment
                or time-step in the input sequence $\rightarrow$ inconvenient pre-segmentation and
                post-processing stages
            \item~\cite{shi_end--end_2017}: character annotations needed for training
        \end{itemize}
    \item Sequence based
        \begin{itemize}
            \item~\cite{shi_end--end_2017}: directly learned from sequence labels (words/instances)
            \item~\cite{xie_aggregation_2019}: no need for separate training targets for each segment
                or time step
            \item CTC
                \begin{itemize}
                    \item~\cite{chen_text_2021}:CTC prone to overfitting
                    \item~\cite{xie_aggregation_2019}: implementation of forward-backward algorithm
                        is complicated $\rightarrow$ large computation consumption
                    \item~\cite{xie_aggregation_2019}: can hardly be applied to 2D prediction
                        problems
                \end{itemize}
            \item Attention
                \begin{itemize}
                    \item~\cite{chen_text_2021}: good for isolated word recognition
                    \item~\cite{chen_text_2021}: Attention has problems with long sequences
                    \item~\cite{xie_aggregation_2019}: relies on attention module for label alignment
                        $\rightarrow$ large storage requirement and computation consumption
                    \item~\cite{xie_aggregation_2019}: misalignment problem can confuse and mislead
                        training problem, leading to degradation of accuracy
                    \item~\cite{xie_aggregation_2019}: can be adopted to 2D prediction but
                        memory and time consumption are to big then
                    \item~\cite{cheng_focusing_2017}: attention drift bad
                \end{itemize}
            \item look into~\cite{cong_comparative_2019} for CTC --- Attention comparison
        \end{itemize}
\end{itemize}

\section{Reflection}
Threats to validity!
\begin{itemize}
    \item~\cite{arpteg_software_2018}: different papers have different components
        $\rightarrow$ Hardware, Platform, Source Code, Configuration
        $\rightarrow$ studies can't really be compared
    \item~\cite{arpteg_software_2018}: `A major challenge in developing DL systems is the
        difficulties in estimating the results before a system has been trained and tested.'
    \item~\cite{long_scene_2021}: different interpretations of metrics (matching for \ac{STD},
        word/char for \ac{STR})
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: all entities of
        \ac{MLS} should be inspected when developing a solution
    \item~\cite{baek_what_2019}: different papers use different evaluation and testing environments
    \item~\cite{baek_what_2019}: different papers use different subsets of the same dataset
        $\rightarrow$ discrepancies in performance
    \item~\cite{long_unrealtext_2020}: half of the widely adopted benchmark datasets have imperfect
        annotations $\rightarrow$ ignoring case-sensitivities and punctuations, and provide new
        annotations for those datasets
    \item~\cite{chen_text_2021}: inconsistency of datasets, priors and testing environments make
        comparison difficult
\end{itemize}

\section{Outlook}

\begin{itemize}
    \item~\cite{watanabe_preliminary_2019}:next steps to practically solve problem
        $\rightarrow$  Data Collection, Data Cleaning, Data Labeling, Model Training,
        Model Evaluation, Model Deployment, Model Monitoring
    \item~\cite{zhao_improving_2020}: Use Neural Architecture Search to automatically find right
        feature extractor
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: build system around
        model $\rightarrow$ e.g.\ supervision mechanism
    \item~\cite{shi_icdar2017_2017,he_icpr2018_2018}: adjust field to better metrics for evaluation
    \item~\cite{long_scene_2021}: general trend to move towards simpler,shorter pipeline
\end{itemize}
