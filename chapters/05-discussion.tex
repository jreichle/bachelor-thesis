\chapter{Discussion}\label{ch:discussion}
This chapter contains a comparison between the taxonomy categories that were introduced in
Section~\ref{se:taxonomy}.
The analysis is followed by a reflection on the methodology and the thesis' results.

\section{Analysis}\label{se:analysis}
The comparison in this section is synthesized from information found in literature as well as
some observations regarding recent innovations that are examined in Section~\ref{se:innovations}.
The analysis is structured similar to the overview sections: first compare approach categories
for \ac{STD} and \ac{STR} and then move on towards \ac{STS}.
The compared aspects of the approaches are the qualities that were identified as relevant in
Section~\ref{se:relevant-qualities}: appropriateness, performance in conjuntion with robustness,
efficiency.
Note that appropriateness entails the subqualities that were derived from the use
case (see Table~\ref{tb:useCaseQualities}):
While the offline capability requirement is met by all of the approaches that are identified in this
thesis, semantics retention and alphanumeric recognition have to be analyzed further.
\ac{STD} is the relevant subtask for semantics retention and \ac{STR} for alphanumeric recognition.

The main challenges for \ac{STD} involve the tradeoff between speed and accuracy as well as
representing arbitrary shaped text instance~\citep{wang_efficient_2019}.
The innovations regarding \ac{STD} deal with multi-oriented and curved text (as can be seen in
Table~\ref{tb:STD-steps-properties}).
\begin{table}[h]
    \centering\scriptsize
    \begin{tabular}{p{.09\textwidth}p{.10\textwidth}p{0.70\textwidth}}
        \multicolumn{2}{c}{\textbf{Approach category}} & \textbf{Shortcommings} \\
        \toprule
        Seg free & & Curved text representation~\citep{long_scene_2021,wang_shape_2019} \\
        & & Linear anchor box bias with curved text~\citep{wang_shape_2019,ferrari_textsnake_2018} \\
        & & High aspect ratio test~\citep{shi_detecting_2017,long_scene_2021} \\
        & One Stage & More efficient, straightforward architecture~\citep{lu_mimicdet_2020} \\
        & Twe Stage & More accurate predictions due to refinement~\citep{lu_mimicdet_2020} \\
        \midrule
        Seg based & & Separating different text instances~\citep{wang_shape_2019} \\
        & & Text instance construction is complex and computation
            intensive~\citep{xie_aggregation_2019,liao_real-time_2019,qiao_text_2021} \\
        & & Incorporate several computation intensive stages~\citep{dai_fused_2018} \\
        & & More vulnerable to noise~\citep{long_scene_2021} \\
        \bottomrule
    \end{tabular}
    \caption{STD approach category comparison\label{tb:STD-comparison}}
\end{table}
Segmentation free or \ac{BB} regression based approaches have trouble with curved text because of
the anchor boxes' linear bias~\citep{wang_shape_2019,ferrari_textsnake_2018}.
Additionally, the regressed \acp{BB} have trouble in accurately representing the shape of curved
text instances~\citep{long_scene_2021,wang_shape_2019}.
Representing multi-oriented text is not a problem on the other
hand~\citep{liao_textboxes_2018,jiang_r2cnn_2017}.

When it comes to comparing \ac{BB} regression approaches, one stage approaches are generally more
efficient  because of their straightforward architecture while two stage approaches can generally
predict more accurately  because of the refinement process~\citep{lu_mimicdet_2020}.
Long aspect ratios are another problem of scene text which degrades \ac{BB} regression
performance~\citep{shi_detecting_2017,long_scene_2021}.

Segmentation based approaches on the other hand take advantage of the fact that every part of the
text instance can locally be verified as such~\citep{long_scene_2021}.
Note that no noteworthy comparison of subcomponent or pixel level segmentation could be identified.
The bottom-up approach alleviates the problem of long aspect ratios~\citep{shi_detecting_2017}.

However, approach is more computation intensive, as in incorporates more complex
stages~\citep{dai_fused_2018} and complicated text instance
construction~\citep{xie_aggregation_2019,liao_real-time_2019,dai_fused_2018}.

The text instance construction goes hand in hand with the separation of different instances.
This is a challenging task which is vulnerable to noise~\citep{long_scene_2021} and the focus of
many new innovations (see Figure~\ref{fig:curved-text-representations}).

The segmentation based approaches facilitate more natural representation of curved
text~\citep{dai_fused_2018,long_scene_2021} (see Figure~\ref{fig:curved-text-representations}).
What \ac{STD} is concerned, the bottom line seems to be: Is it curved text detection or efficiency
more important?

\ac{STR} research focuses on accurately recognizing 2d text instance.
The innovations found for \ac{STR} (see Table~\ref{tb:STR-steps-properties}) show that the research
is mostly focused on robustely recognizing curved text instances (rectification approaches and
2d attention).
Additionally, it can be noted that the innovative approaches are mostly attention based.
However, attention too has shortcommings (see Figure~\ref{tb:STR-comparison}).
\begin{table}[h]
    \centering\scriptsize
    \begin{tabular}{p{.09\textwidth}p{.15\textwidth}p{0.66\textwidth}}
        \multicolumn{2}{c}{\textbf{Approach category}} & \textbf{Shortcommings} \\
        \toprule
        Seg based & & Accurate detection of individual
                        characters~\citep{chen_text_2021,cheng_aon_2018} \\
        & & Disregard for contextual information between characters~\citep{chen_text_2021} \\
        & & Incorporate several computation intensive stages~\citep{liu_abcnet_2020} \\
        \midrule
        \ac{EnDe} based & & Curved and multi-oriented text~\citep{cheng_aon_2018, long_scene_2021} \\
        & CTC based & Prone to overfitting~\citep{chen_text_2021} \\
        & & Computation extensive CTC probabilities~\citep{xie_aggregation_2019} \\
        & & Isolated word recognition~\citep{cong_comparative_2019} \\
        & & Not applicable to recognition of 2d text instance~\citep{cheng_focusing_2017,
            xie_aggregation_2019,chen_text_2021} \\
        & Attention based & Sentence and long sequence
            recognition~\citep{cong_comparative_2019,chen_text_2021} \\
        & & Problems without vocabulary~\citep{wan_vocabulary_2020} \\
        & & Attention drift leads to missing or superfluous
            characters~\citep{liao_scene_2018,xie_aggregation_2019,chen_text_2021}\\
        & & Adapted 2d attention requires a lot of storage and
            computation~\citep{xie_aggregation_2019,chen_text_2021} \\
        \bottomrule
    \end{tabular}
    \caption{STR approach category comparison\label{tb:STR-comparison}}
\end{table}
For segmentation based approaches the main shortcommings are as follows.
Localizing individual characters is computationally expensive~\citep{zhan_esir_2019}.
A complex pipeline is needed to predict characters and align them~\citep{liu_abcnet_2020}.
Take~\cite{wan_textscanner_2020} for example which leverages a separate geometry branch
to which helps with word formation with the predicted charcters from the classifying branch.

Segmentation is also susceptible to errors~\citep{zhan_esir_2019,cheng_aon_2018,chen_text_2021}.
The errors are reinforced by the nature of scene text: complex backgrounds, noise, perspective
and other factors~\citep{hu_gtc_2020,chen_text_2021,baek_what_2019}.
Additionally, segmentation based approaches cannot make us of contextual information between
charcters~\citep{chen_text_2021}.

\ac{EnDe} based \ac{STR} on the other hand is modeled to take contextual information into
account~\citep{long_scene_2021,chen_text_2021}.
The problem manifests itself in dealing with 2d-text
instances~\citep{long_scene_2021,liao_scene_2018}.
The \ac{EnDe} basic approaches use decoders which work with sequences (and therefore
1d)~\citep{long_scene_2021,cheng_aon_2018}.
The introduced innovations deal with this in two ways: rectify 2d text into 1d
text~\citep{zhan_esir_2019,luo_multi-object_2019,shi_aster_2019,liu_char-net_2018}, adapt the
attention mechanism to 2d input~\citep{li_show_2019}.
Note that \ac{CTC} cannot be adapted to 2d input~\citep{cheng_focusing_2017,xie_aggregation_2019}
and the attention adaption by~\cite{li_show_2019} is memory and computation
intensive~\citep{xie_aggregation_2019}.

When comparing \ac{CTC} to attention based approaches it can be noted that \ac{CTC} deals better
with long text instances or sentences while attention is better with singular
words~\citep{cong_comparative_2019,chen_text_2021}.
This is because the misalignment in attention can cause missing or superfluous charcters (attention
drift)~\citep{bai_edit_2018,liao_scene_2018,cheng_focusing_2017}.

Attention based approaches have the upper hand when it comes to incorporate implicit language models
or lexicons but \ac{CTC} perform better without language priors~\citep{cong_comparative_2019}.
This is especially important because the text in the use case under consideration contains
alphanumeric strings which are not part of any lexicon.
When it comes to computational efficiency both \ac{CTC} and attention are expensive and time
consuming~\citep{chen_text_2021}.

2 stage approaches have the upper hand, when it comes to end to end \ac{STS}.
These approaches are in the focus for research because they have crucial
advantages (see Figure~\ref{tb:E2E-comparison})~\citep{chen_text_2021}.
\begin{table}[h]
    \centering\scriptsize%
    \begin{tabular}{p{.11\textwidth}p{.70\textwidth}}
        \textbf{Category} & \textbf{Shortcommings} \\
        \toprule
        2 step & Error propagation between detection and
            recognition~\citep{chen_text_2021,long_scene_2021}\\
        & No joint optimization~\citep{qiao_text_2021, chen_text_2021}\\
        & Computation requirements for two feature extraction
            CNNs~\citep{liu_fots_2018,chen_text_2021} \\
        2 stage & --- \\
        \bottomrule
    \end{tabular}
    \caption{STS approach category comparison\label{tb:E2E-comparison}}
\end{table}
2 stage approaches are more efficient than 2 step approaches because they don't compute feature maps
twice~\citep{liu_fots_2018,chen_text_2021}.
This carries a lot of weight since feature extraction is usually the most time and computation
consuming step~\citep{liu_fots_2018}.
The direct combination of \ac{STD} and \ac{STR} also has impact on the performance.
Because of the connecting both stages together are fully differentiable and can be jointly
optimized~\citep{chen_text_2021,long_scene_2021,qiao_text_2021}.
Without joint optimization error that happen in the detection step can be propagated to the
recognition which results in performance degradation~\citep{chen_text_2021,qiao_text_2021}.

\section{Reflection}
The goal of this thesis was to create an overview over \ac{STS} approaches in order to facilitate
making a choice in regards to a pracitcal problem.
This goal is impeded by a a couple of factors.
The first being that all entities of a \ac{MLS} need to be examined in order to develop a proper
solution~\citep{siebert_construction_2021,nakamichi_requirements-driven_2020}.
Because not all entities (most importantly the data entity) are available, this was not possible.

Another reason is the chosen cutoff point of 100 citations as a requirement for innovative literature.
Even though it is useful to filter for the most groundbreaking advancements in the field,
newer innovations that have not been revealed long enough to earn enough citations might be left
out of the equation.

The analysis does not contain quanitative data on performance or efficiency regarding
singular approaches or even regarding task categories.
This is because literature on \ac{ML} and \ac{DL} benchmarking suggests that it is not a good
practice to combine and compare results from different
studies~\citep{baek_what_2019,arpteg_software_2018,long_scene_2021}.
There's different rationales for this.
Because different studies incorporate different components such as Hardware, Platform, Source Code
or Configuration the comparison would be flawed~\citep{arpteg_software_2018,baek_what_2019}.
Additionally, different studies might present a different interpretation of
metrics~\citep{long_scene_2021}.
Yet another reason has to do with the benchmark datasets in use: datasets are used inconsitently in
many cases.
Often certain images are left out for various reasons~\citep{baek_what_2019}.
