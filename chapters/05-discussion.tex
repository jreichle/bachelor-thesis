\chapter{Discussion}\label{ch:discussion}
\section{Analysis}
Plan
\begin{itemize}
    \item Go down hierarchy and compare most important advances
\end{itemize}

For comparison: which Benchmark Dataset fits the problem the best?
\begin{itemize}
    \item~\cite{liao_mask_2020}:
        \begin{itemize}
            \item Rotated ICDAR 2013 (changed normal icdar): rotation robustness
            \item Total-Text: shape ropustness
            \item MSRA-TD500: aspect ratio ropustness
        \end{itemize}
    \item~\cite{yang_learning_2021}:commonly used for oriented text: ICDAR2015, ICDAR2017 MLT, MSRA-TD500
\end{itemize}

Important points:
\begin{itemize}
    \item ResNet is pretty much the Feature Extractor Benchmark
\end{itemize}

Recognition
\begin{itemize}
    \item Ability to cope with 2d text:
        CTC has problems,
        Attention/Encoder-Decoder based can be extended to work
    \item CTC prone to overfitting
    \item Attention has problems with long sequences
\end{itemize}

\section{Reflection}
Threats to validity!
\begin{itemize}
    \item~\cite{arpteg_software_2018}: different papers have different components
        $\rightarrow$ Hardware, Platform, Source Code, Configuration
        $\rightarrow$ studies can't really be compared
    \item~\cite{arpteg_software_2018}: `A major challenge in developing DL systems is the
        difficulties in estimating the results before a system has been trained and tested.'
    \item~\cite{long_scene_2021}: different interpretations of metrics (matching for \ac{STD},
        word/char for \ac{STR})
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: all entities of
        \ac{MLS} should be inspected when developing a solution
    \item~\cite{baek_what_2019}: different papers use different evaluation and testing environments
    \item~\cite{baek_what_2019}: different papers use different subsets of the same dataset
        $\rightarrow$ discrepancies in performance
    \item~\cite{long_unrealtext_2020}: half of the widely adopted benchmark datasets have imperfect
        annotations $\rightarrow$ ignoring case-sensitivities and punctuations, and provide new
        annotations for those datasets
    \item~\cite{chen_text_2021}: inconsistency of datasets, priors and testing environments make
        comparison difficult
\end{itemize}

\section{Outlook}

\begin{itemize}
    \item next steps to practically solve problem $\rightarrow$  Data Collection, Data Cleaning,
        Data Labeling, Model Training, Model Evaluation, Model Deployment, Model
        Monitoring~\citep{watanabe_preliminary_2019}
    \item  Use Neural Architecture Search to automatically find right feature
        extractor~\citep{zhao_improving_2020}
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: build system around
        model $\rightarrow$ e.g.\ supervision mechanism
    \item~\cite{shi_icdar2017_2017,he_icpr2018_2018}: adjust field to better metrics for evaluation
    \item~\cite{long_scene_2021}: general trend to move towards simpler,shorter pipeline
\end{itemize}
