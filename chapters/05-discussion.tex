\chapter{Discussion}\label{ch:discussion}
\section{Analysis}\label{se:analysis}
Go down hierarchy and compare most important advances
\begin{itemize}
    \item Compare: two step, two stage
    \item Compare: STD/STR pipelines
    \item In which way are innovations going?
\end{itemize}
Comparing numbers on benchmarks and not looking at quanitative data would result in fallacy.

E2E
\begin{itemize}
    \item 2 step
        \begin{itemize}
            \item~\cite{chen_text_2021}: errors can accumulate between STD and STR
            \item~\cite{long_scene_2021}: propagation of error between detection and recognition
        \end{itemize}
    \item 2 stage
        \begin{itemize}
            \item~\cite{chen_text_2021}: can prevent errors from being accumulated during training
            \item~\cite{chen_text_2021}: share information between STD and STR
                $\rightarrow$ optimize jointly to improve performance
            \item~\cite{chen_text_2021}: exhibit competitive performance with faster interference
                and smaller storage requirements
            \item~\cite{liu_fots_2018}: sharing features significantly speeds up the network
                by not needing a second \ac{CNN} feature extractor
        \end{itemize}
\end{itemize}

Detection
\begin{itemize}
    \item reference tables found in overview!
        \begin{itemize}
            \item only curved or multi-orientated text solutions
            \item mostly focused on performance/robustness rather than efficiency
            \item for regresseion based: how to deal with orientation?
            \item segmentation mostly curved, regression mostly oriented
            \item while BB methods deal with orientation representations, pixel segmentation methods
                mostly find innovations that help separate text instances
        \end{itemize}
    \item~\cite{wang_efficient_2019}: two main challenges for \ac{STD}:
        trade-off between speed and accuracy; model arbitrary-shaped text instance
    \item BB regression
        \begin{itemize}
            \item~\cite{ferrari_textsnake_2018}: many methods have strong assumption that text
                instances are in linear shape and therefore adopted simple representations
                (rectangles --- axis aligned or rotated, quadrangles)
                $\rightarrow$ problem with irregular and curved text
            \item~\citep{long_scene_2021}: Curved text
            \item~\cite{long_scene_2021,shi_detecting_2017}: Higher aspect ratios
            \item~\cite{shi_detecting_2017}: Text orientation
        \end{itemize}
    \item Segmentation
        \begin{itemize}
            \item~\cite{ferrari_textsnake_2018}: some models are specifically designed for
                arbitrary shapes
            \item~\cite{shi_detecting_2017}: modular approach instead of whole \acp{BB}
                $\rightarrow$ good with long aspect ratio and orientation because of flexibility
            \item~\cite{dai_fused_2018}: often need several stages and components for final result
                $\rightarrow$ inefficient
        \end{itemize}
\end{itemize}

Recognition
\begin{itemize}
    \item reference tables found in overview!
        \begin{itemize}
            \item mostly attention based
            \item concerned with curved text $\rightarrow$ a lot of rectification innovation
            \item both unconstrained and with lexicons
        \end{itemize}
    \item research~\cite{wan_vocabulary_2020} $\rightarrow$ vocabulary reliance
    \item Ability to cope with 2d text:
        CTC has problems,
        Attention/Encoder-Decoder based can be extended to work
    \item never use lexicons $\rightarrow$ alphanumeric strings
    \item Segmentation based
        \begin{itemize}
            \item~\cite{xie_aggregation_2019}: require separate training targets for each segment
                or time-step in the input sequence $\rightarrow$ inconvenient pre-segmentation and
                post-processing stages
            \item~\cite{shi_end--end_2017}: character annotations needed for training
            \item~\citep{chen_text_2021}: Accurate detection of individual characters is hard
            \item~\citep{chen_text_2021}: Contextual information between character gets lost
        \end{itemize}
    \item Sequence based
        \begin{itemize}
            \item~\cite{shi_end--end_2017}: directly learned from sequence labels (words/instances)
            \item~\cite{xie_aggregation_2019}: no need for separate training targets for each segment
                or time step
            \item CTC
                \begin{itemize}
                    \item~\cite{chen_text_2021}:CTC prone to overfitting
                    \item~\cite{xie_aggregation_2019}: implementation of forward-backward algorithm
                        is complicated $\rightarrow$ large computation consumption
                    \item~\cite{xie_aggregation_2019}: can hardly be applied to 2D prediction
                        problems
                \end{itemize}
            \item Attention
                \begin{itemize}
                    \item~\cite{chen_text_2021}: good for isolated word recognition
                    \item~\cite{chen_text_2021}: Attention has problems with long sequences
                    \item~\cite{xie_aggregation_2019}: relies on attention module for label alignment
                        $\rightarrow$ large storage requirement and computation consumption
                    \item~\cite{xie_aggregation_2019}: misalignment problem can confuse and mislead
                        training problem, leading to degradation of accuracy
                    \item~\cite{xie_aggregation_2019}: can be adopted to 2D prediction but
                        memory and time consumption are to big then
                    \item~\cite{cheng_focusing_2017}: attention drift bad
                \end{itemize}
            \item look into~\cite{cong_comparative_2019} for CTC --- Attention comparison
        \end{itemize}
\end{itemize}

\section{Reflection}
Threats to validity!
\begin{itemize}
    \item~\cite{arpteg_software_2018}: different papers have different components
        $\rightarrow$ Hardware, Platform, Source Code, Configuration
        $\rightarrow$ studies can't really be compared
    \item~\cite{arpteg_software_2018}: `A major challenge in developing DL systems is the
        difficulties in estimating the results before a system has been trained and tested.'
    \item~\cite{long_scene_2021}: different interpretations of metrics (matching for \ac{STD},
        word/char for \ac{STR})
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: all entities of
        \ac{MLS} should be inspected when developing a solution
    \item~\cite{baek_what_2019}: different papers use different evaluation and testing environments
    \item~\cite{baek_what_2019}: different papers use different subsets of the same dataset
        $\rightarrow$ discrepancies in performance
    \item~\cite{long_unrealtext_2020}: half of the widely adopted benchmark datasets have imperfect
        annotations $\rightarrow$ ignoring case-sensitivities and punctuations, and provide new
        annotations for those datasets
    \item~\cite{chen_text_2021}: inconsistency of datasets, priors and testing environments make
        comparison difficult
    \item cutof at 100 cit $\rightarrow$ newer publications might be equally impactful but not
        not enough time for more citations
\end{itemize}

\section{Outlook}

\begin{itemize}
    \item~\cite{watanabe_preliminary_2019}:next steps to practically solve problem
        $\rightarrow$  Data Collection, Data Cleaning, Data Labeling, Model Training,
        Model Evaluation, Model Deployment, Model Monitoring
    \item~\cite{zhao_improving_2020}: Use Neural Architecture Search to automatically find right
        feature extractor
    \item~\cite{siebert_construction_2021,nakamichi_requirements-driven_2020}: build system around
        model $\rightarrow$ e.g.\ supervision mechanism
    \item~\cite{shi_icdar2017_2017,he_icpr2018_2018}: adjust field to better metrics for evaluation
    \item~\cite{long_scene_2021}: general trend to move towards simpler,shorter pipeline
\end{itemize}
