\chapter{Technique Overview}\label{ch:research}
This part of the thesis constitutes the main part.
The objective is to create an overview of techniques in the field which may be used to solve the
problem detailed in Chapter~\ref{ch:problem}.
First the search for literature which lays the foundation for the subsequent sections, is documented.
The a taxonomy is introduced which facilitates the analysis and comparison of techniques.
Lastly, the advances in the field are placed in the correct position and analyzed.

\section{Taxonomy of Pipeline Steps}
In order to create the overview the necessary steps in the process of \ac{STS} need to be highlighted,
from preprocessing to classifying the identified
text~\citep{long_scene_2021, sourvanos_challenges_2018}.
The ways in which the respective issues for the steps are solved are identified from literature,
listed and explained alongside.
\begin{figure}[h]
    \centering
    \include{figure-code/tikz/taxonomie-tikz}
\caption{Pipeline taxonomy and respective steps\label{fig:pipelineSteps}}
\end{figure}
Figure~\ref{fig:pipelineSteps} shows tasks which are associated with \ac{STS}.
\ac{STD} and \ac{STR} only incorporate a part of \ac{STS}, while E2E
incorporate both \ac{STD} and \ac{STR} techniques to solve \ac{STS}~\citep{long_scene_2021}.
Therefore this section will first discuss the two parts, to then combine them.

For \ac{STD} two main categories of approaches can be identified: segmentation based and anchor
based~\citep{long_scene_2021,sheng_centripetaltext_2021,liu_accurate_2020}.
% FIXME: anchor based can also be regression based -> change to BB regression
% FIXME: check whether convolutional predictor always has softmax!!!
The anchor based category draws heavy inspiration from the field of object
detection~\citep{long_scene_2021,liu_accurate_2020}.
This is only natural as text detection can be seen as a type of object
detection~\citep{liu_accurate_2020,long_scene_2021}.
For object detection inspired \ac{STD} there are two methods: one stage and two
stage~\citep{long_scene_2021}.
% FIXME: why one / two
Both localize text instances as a whole~\citep{long_scene_2021,sheng_centripetaltext_2021}.
One stage approaches are modelled after~\cite{liu_ssd_2016}, Single Shot MultiBox
Detector (SSD) and~\cite{redmon_you_2016}, You Only Look Once (YOLO).
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/STD-seg-free-Liao-TextBoxes-2017.png}
    \caption[One stage, anchor based STD architecture]{Example for a one stage, anchor based STD
    architecture~\citep{liao_textboxes_2017}\label{fig:STD-segfree-ssd}}
\end{figure}
The basic approach is explained with the example of~\cite{liao_textboxes_2017} (see
Figure~\ref{fig:STD-segfree-ssd}) which is based on SSD.%
It uses 13 layer convolutional network (three blocks of: two or three $3\times3$ conv layers followed
by a $2\times2$ max pooling layer) for feature extraction~\citep{liao_textboxes_2017}.
Afterwards come nine additional layers which continuously downsample, the output of six of them
is separately used as feature maps for \ac{BB} regression~\citep{liao_textboxes_2017}.
The downsampling and \ac{BB} regression for different layers helps detect text instances of different
scales~\citep{liu_ssd_2016}.
Each spatial location on the feature map can be traced back to a region on the input
image~\citep{long_scene_2021}.
The \ac{BB} regression is carried out by six text-box layers which predict how certain ($c_1,c_2$)
the prediction is a text instance or background and where the text instance is ($x,y,w,h$).
Note that the output is not the location of a \ac{BB} but the offset to the
respective anchor box~\citep{liao_textboxes_2017,long_scene_2021}.
Anchor boxes are predefined to give bias towards sizes and aspect ratios of
text~\citep{liao_textboxes_2017}.
The text-box layers are the difference to the SSD approach for normal object
detection~\citep{liao_textboxes_2017,liu_ssd_2016}.
These layers use $1\times5$ filters to adjust to larger aspect ratios~\citep{liao_textboxes_2017}.
Each text-box layer has 72 filters (12 anchor boxes $\cdot$ 6 values per prediction), the filters
are slided accross the input features generating 12 predicted \ac{BB} per
position~\citep{liao_textboxes_2017}.
The \acp{BB} of all layers are then subjected to the process of \ac{NMS} to filter out the best
\ac{BB} for each possible text instance~\citep{liao_textboxes_2017}.
For this \ac{NMS} is used: of all detections which overlap more than a threshhold $\phi$ only the
with the highest confidence score ($c$) is kept~\citep{hosang_learning_2017}.

The R-CNN which builds the foundation for \ac{ROI} based text detection, was introduced
by~\cite{girshick_rich_2014} and improved by~\cite{girshick_fast_2015} (Fast
R-CNN),~\cite{ren_faster_2015} (Faster R-CNN) and~\cite{he_mask_2018} (Mask R-CNN).
Note that 2-stage methods are fully differentiable and thus end to end trainable since
Faster R-CNN (like 1-stage methods)~\citep{ren_faster_2015,long_scene_2021}.
The model introduced by~\cite{jiang_r2cnn_2017} (see Figure~\ref{fig:STD-segfree-rcnn}) uses
Faster R-CNN.%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/STD-seg-free-Jiang-R2CNN-2017.png}
    \caption[Two stage, anchor based STD architecture]{Example for a two stage, anchor based STD
        architecture~\citep{jiang_r2cnn_2017}\label{fig:STD-segfree-rcnn}}
\end{figure}
Like with the one stage approach, the two stage approach starts with feature extraction from the
image with a convolutional layers~\citep{jiang_r2cnn_2017}.
The generated feature map is used by a \ac{RPN}.
Like with the previously explained approach, the feature maps are used to regress the offset respective
to bounding boxes.
The bounding boxes are still axis aligned at this point~\citep{jiang_r2cnn_2017}.
In contrast to the previous SSD based approach, only one feature map is used in conjunction with
anchor box regression~\citep{jiang_r2cnn_2017}.
The \ac{RPN} from Faster R-CNN is adjusted to use smaller scale anchor boxes to adapt to
text~\citep{jiang_r2cnn_2017}.
Note that R-CNN and Fast-RCNN used the slower Sequential Search algorithm instead of an
\ac{RPN}~\citep{girshick_rich_2014,girshick_fast_2015}.
The resulting \acp{BB} are called \acp{ROI}~\citep{ren_faster_2015,jiang_r2cnn_2017}.
They are used for \ac{ROI} pooling in conjunction with the original feature maps.
This layer uses max pooling to convert the spatial features corresponding to the location of the
\ac{ROI} to a small feature map~\citep{girshick_fast_2015}.
In the case of this example, \ac{ROI} pooling is used to create three feature maps with different
aspect ratios ($7\times7, 3\times11, 11\times3$) which are concatenated for the next
step~\citep{jiang_r2cnn_2017}.
The second stage is to predict a confidence score (text, background) for each \ac{ROI} and to
refine them by regressing values ($x_1,y_1,x_2,y_2,h$) that allow for inclined boxes to account for
rotated text~\citep{jiang_r2cnn_2017}.
At last the resulting \acp{BB} are filtered by inclined \ac{NMS} which is adjusted to the
incline \ac{BB}~\citep{jiang_r2cnn_2017}.

The basis for the segmentation based methods is the fact that every part of the text instance can
be used to verify that there is text~\citep{long_scene_2021}.
Because of this, sub-text components can be detected separately and then used to re-construct a text
instance~\citep{long_scene_2021}.
Segmentation based methods can roughly be summed up in two categories: pixel based and component
based~\citep{long_scene_2021}.
Like with anchor based methods, example architectures are explained in order to describe their
categories more clearly.
The paper~\cite{deng_pixellink_2018} introduced a pixel based \ac{STD} approach.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{img/STD-seg-based-architecture-Deng-PixelLink-2018.png}
    \caption[Pixel, segmentation based STD architecture]{%
        Example for a pixel, segmentation based STD
        architecture~\citep{deng_pixellink_2018}\label{fig:STD-segbased-pixel-architecture}
    }
\end{figure}
Figure~\ref{fig:STD-segbased-pixel-architecture} shows the approch's architecture.
Figure~\ref{fig:STD-segbased-pixel-CNN} shows the \ac{CNN} structure for feature extraction
(until fc6\&7) followed by the head which either predicts text/non-text or
links~\citep{deng_pixellink_2018}.
The feature extraction structure is inspired by~\cite{long_fully_2015}.
Continuous downsampling and and combining those layers with later, upsampled layers helps
to combine coarse, higher level information with fine, lower level
information~\citep{long_fully_2015}.
The upsampling is performed with bilinear interpolation~\citep{deng_pixellink_2018}.
% FIXME: is this Conv Attention/Encoder-Decoder?
Depending on which head is used, the $1\times1$ convolution layers either have 2 or $2\cdot8$ filters.
Counted together, the model has 18 output channels~\citep{deng_pixellink_2018}.
The $1\times1$ convolution layers are also used for upsampling (deconvolution,
see~\cite{noh_learning_2015,long_fully_2015} for an in depth explanation)~\citep{deng_pixellink_2018}.
The 2 filters are used to predict text/non-text for each pixel, while the other 16 filters predict
the links~\citep{deng_pixellink_2018}.
The text/non-text head essentially performs semantic segmentation, that is to categorize each
pixel to its type~\citep{deng_pixellink_2018}.
A pixel has eight neighbors: left, left-down, left-up, right, right-down, right-up, up, down.
Each of the $2\cdot8$ filters is responsiblde for link a neighbor~\citep{deng_pixellink_2018}.
% FIXME: look into seglink (next approach)
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{img/STD-seg-based-CNN-Deng-PixelLink-2018.pdf}
    \caption[Feature extractor and prediction head for pixel segmentation]{%
        PixelNet CNN feature extractor with head structure for pixel
        segmentation\label{fig:STD-segbased-pixel-CNN}
    }
\end{figure}
After both links and text/non-text pixels have been predicted, they are combined for instance
segmentation~\citep{deng_pixellink_2018}.
The link layers are used to indicate whether two text pixels are grouped together an thus belong to
the same instance~\citep{deng_pixellink_2018}.
A bounding box can then be extracted by laying minimum area rectangles over the
instances~\citep{deng_pixellink_2018}.
% FIXME: final output same spatial structure as inout image

The second segmentation based category for \ac{STD} segments components which are local regions of
text that can overlap one or more characters~\citep{long_scene_2021}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{img/STD-seg-based-architecture-Shi-Detecting-2017.png}
    \caption[Sub-component, segmentation based STD architecture]{%
        Example for a sub-component, segmentation based STD
        architecture~\citep{shi_detecting_2017}\label{fig:STD-segbased-component-architecture}
    }
\end{figure}
The architecture (see Figure~\ref{fig:STD-segbased-component-architecture})
from~\cite{shi_detecting_2017} is used as an example for this category.
Like the one-stage, anchor based \ac{STD} approach, the feature extraction \ac{CNN} of this approach
is taken from SSD, the difference is reflected in the prediction
layers~\citep{shi_detecting_2017,liu_ssd_2016}.
Instead of detecting whole \acp{BB}, the networks predicts both subcomponents and links at multiple
scales~\citep{shi_detecting_2017}.
The convolutional prediction is carried out with seven $3\times3$ filters followed by a softmax
nonlinearity for normalization.
The segments are given by the values $x_s,y_s,w_s,h_s,\theta_s$ which offset an anchor box as well
as confidence scores $c_1,c_2$~\citep{shi_detecting_2017}.
Links are used to separate the segments and are accordingly used to separate neraby
words~\citep{shi_detecting_2017}.
% FIXME: how exactly is link predicted --- conv or this calc?
Within layer links are predicted by $2\cdot$ for the neighbors of a space in the feature map
(see Figure~\ref{fig:STD-segbased-component-links} (a),
Equation~\ref{eq:STD-segbased-subcomp-same-layer})~\citep{shi_detecting_2017}.
\begin{equation}\label{eq:STD-segbased-subcomp-same-layer}
    \mathcal{N}_{s^{x,y,l}}^w =
        \frac{{\{s^(x',y',l)\}}_{x-1\leq x'\leq x+1,y-1\leq y'\leq y+1}}{s^{(x,y,l)}}
\end{equation}
The cross layer links on the other hand are predicted by using the 4 cross layer neighors of the
feature map of the preceding predictor~\citep{shi_detecting_2017}.
(see Figure~\ref{fig:STD-segbased-component-links} (b),
Equation~\ref{eq:STD-segbased-subcomp-cross-layer})~\citep{shi_detecting_2017}.
\begin{equation}\label{eq:STD-segbased-subcomp-cross-layer}
    \mathcal{N}_{s^{x,y,l}}^c = {\{s^{(x',y',l-1)}\}}_{2x\leq x'\leq 2x+1,2y\leq y'\leq 2y+1}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/STD-seg-based-links-Shi-Detecting-2017.png}
    \caption[Predicting links for segmentation based STD]{%
        Visualization for prediction of links within and cross layers for segmentation based
        STD\label{fig:STD-segbased-component-links}
    }
\end{figure}

\begin{itemize}
    \item Links: connect segments and separate nearby instances
        \begin{itemize}
            \item within layer: connect on same layer
            \item cross layer: connect between layers
        \end{itemize}
    \item use within and cross layer links to connect adjacent locations, scales
    \item Find whole instance by depths-first search
\end{itemize}

% FIXME: after all explained: Note that mixtures of the categories are possible in practice,
%           the categorization is applied to help compare approaches (give example for mixture)

STR
\begin{itemize}
    \item segmentation-free approach (segmentation-based implicit and explicit out of date)
    \item Steps: Preprocessing, Feature Extraction, Sequence Modelling, Prediction
    \item preprocessing, text enhancement: remove distortions, background; improve resolution,
        recover degraded text
        \begin{itemize}
            \item Spatial Transformer Networks (for distortions)
        \end{itemize}
    \item feature extraction: encode image into feature space
        \begin{itemize}
            \item ResNet
                \begin{itemize}
                    \item aggressive downsampling
                    \item $3\times3$ best kernel size
                    \item Gradient saving $\rightarrow$ Res block, Res Bottleneck, Res Grouped
                    \item batch normalization
                    \item 32,50,150,\ldots
                \end{itemize}
            \item GoogLeNet
        \end{itemize}
    \item Sequence and Prediction
        \begin{itemize}
            \item CTC
            \item Encode-Decoder/Attention
            \item Lexicon Free Models!
        \end{itemize}
\end{itemize}

E2E
\begin{itemize}
    \item trainable as one?
    \item Bestandteile nicht zwangsweise sequentiell
    \item two step or two stage pipeline
\end{itemize}

\begin{table}[ht]
    \centering\scriptsize
    \begin{tabular}{p{.05\textwidth}p{.1\textwidth}p{.15\textwidth}p{.55\textwidth}}
        Task & \multicolumn{2}{c}{Approach} & Identifying properties \\
        \toprule
        STD & Seg free & & Localize text instance whole \\
            & & 1-stage & direct \ac{BB} regression \\
            & & 2-stage & find \acp{ROI}, adjust \acp{ROI} for better fit \\
            & Seg based & & Localize sub text components \\
            & & Pixel-level & Pixel level segmentation \\
            & & Component-level & Sub-component level segmentation \\
        \midrule
        STR & Seg based & & Character segmentation and classification\\
            & Seg free & & Text instance recognition \\
            & & CTC based &  \\
            & & Attention based & Encoder-Decoder Mechanism \\
        \midrule
        E2E & 2-step & & \\
            & Parallel & & \\
        \bottomrule
    \end{tabular}
    \caption{Tasks, method categories and identifying propertis\label{tb:steps-properties}}
\end{table}

\section{Literature Search}\label{se:litSearch}
This section documents the search for literature which provides the content for the subsequent
overview of innovation.
For this, important pipelines and notable advances along with their properties are researched,
analysed and presented.
The literature is identified through searching in the Google Scholar database.
The search is executed with keywords such as, but not only: Deep Learning, Text Detection,
Text Recognition, Text Spotting, Scene Text, Pipeline.
% FIXME: update keywords
A criterion for further examination is an appropriate amount of citations for the piece of literature
in question.
Additionally, literature is selected through citations for and by literature which has already been
identified as important.
All research after 2018 which pertains to extracting scene text is regarded as relevant.
Standard \ac{OCR} solutions may not hold validity in practice, as the image and text conditions can
vary in the defined problem~\citep{chen_text_2021}.
The delimination from Section~\ref{se:problem} of course holds for this chapter and only literature
which concerns advances for the \ac{DL} model architecture will be regarded as important for the
scope of this thesis.
This extends to the whole pipeline from preprocessing an image to the final result of the model.

% FIXME: hierarchical graphic with found papers
% FIXME: describe proceeding


\section{State of the Art Methods}

text enhancement:~\cite{chen_text_2021}
model pruning:~\cite{niu_26ms_2019}
integer inference:~\cite{ignatov_ai_2019}
