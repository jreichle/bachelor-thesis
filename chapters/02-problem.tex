\chapter{Problem analysis}\label{ch:problem}
This chapter entails an analysis of the problem which is the research question's foundation.
It is crucial, as the quality of requirements ultimately determines the quality of the literature review.

%\section{Requirements}
For traditional software projects requirements engineering classifies requirements into
two categories~\cite{zowghi_requirements_2014}.
\Acp{FR} specify functionality that users can experience~\cite{noauthor_ieee_1998}.
Other requirements that are relevant to the project in a way that shapes the target system,
defines the development process and manages the development project are refered to as
\acp{NFR}~\cite{kotonya_requirements_1998,chung_non-functional_2009}.
For \ac{ML} and thus \ac{DL} projects, data directly influences the performance of the solution.
This results in the need to specify \acp{DR} for data that is used
in conjunction with the \ac{DL} system~\cite{vogelsang_requirements_2019}.

% FIXME: add that requirements where not gathered according to normal processes (like stakeholder
analysis\ldots

% FIXME: only model requirements, maybe get rid of data requirements entirely
% FIXME: find exlusion criteria
% alphanumeric strings, preserve saving and strucure with coordinates,
% computationally efficient (how to define cutof)
% --> all FR without performance metric?

% functional requirements
The basic functionality can be described as extraction of textual data from images.
The relevant text information is framed by a label.
The label contains printed text which can be structured and spaced differently from label to
label (see figure~\ref{fig:examples}).
\begin{figure}
    \centering
    \subfigure[Positive example\label{fig:good-example}]{\includegraphics[width=0.48\textwidth]{img/Image-Example-Positive.jpg}}
    \subfigure[Negative example\label{fig:bad-example}]{\includegraphics[width=0.48\textwidth]{img/Image-Example-Negative.jpg}}
    \caption{Examples for label images\label{fig:examples}}
\end{figure}
The text carries semantic information which can be important for later processing in the scope of
a business process.
The goal is to extract the text and preserve semantics from structure and space.
This means text and the respective coordinates, height, width and a possible rotation angle must
be output as the result~\cite{yang_learning_2021}.
Those values can then be transformed into other formats such as JSON or HTML as needed.
% TODO: text that is not in straight line? I think not
The images can contain arbitrary alpha-numeric strings (see figure~\ref{fig:examples}).
This results in the requirement that the \ac{DL} model has to be able to recognize sequences that
are not part of a predefined lexicon~\cite{ghosh_visual_2017}.
% FIXME: performance, change reliability to performance after checking source
For \ac{ML} projects the predictive reliability can be regarded as a
\ac{FR}~\cite{vogelsang_requirements_2019}.
% TODO: does following belong to methodology (until **)
However, due to the fact that the implementation and testing phase is not performed in the scope of
this thesis and the difficulty in assessing the performance ahead of those phases, it is not
possible to base a decision on this factor.
Additionally experimental results from literature can only be compared as long as factors such as
hardware, platform, source code, configuration and dataset are uniform~\cite{arpteg_software_2018}.
This applies to studies that create an overview such as~\cite{chen_text_2021,long_scene_2021}.
% **
% FIXME: robustness
Due to the uncontrolled environment of \ac{STR} in the practical aspect of taking the images on-site
beneficial image properties can not be guaranteed~\cite{chen_text_2021}.
Robust text extraction can be influenced by factors such as complex backgrounds, text form
(text rotation, font variability, arrangement), image noise (lighting conditions, blur,
interference and low resolution) and access (perspective, shape of
text)~\cite{oyedotun_deep_2015,ghosh_visual_2017,chen_text_2021}.
Therefore, these properties have to be accounted for when determining the viability for an approach.
An example for bad image quality in regards to \ac{OCR} can be seen in figure~\ref{fig:bad-example}.


% non-functional requirements
% FIXME: find sources
The \acp{NFR} that derive from the intended use for the solution with mobile phones are led by
power aspects.
Not only are mobile phones limited by a finite battery but also by computational power.
In this regard \acp{NN} can be challenging because they often have an immense amount of parameters
which are computationally demanding and can therefore also be a burden for the phone's power supply.
Therefore, finding an approach which reduces computational complexity is important.
The solution will be used on mobile phones that have no access to the internet.
This is why the extraction must work offline.
Varying aspect ratios in images and such diversities can increase the requirements for preprocessing.
Depending on the approach the complexity can change i.e.\ decrease thus making it more viable.
Maintenance of a \ac{DL} system in regards to changing requirements such as changing the output
format are also an important factor.

% TODO: weglassen?
% data requirements
% FIXME: focus more on annotation, costs or difference for different algorithms
\acp{DR} encompass the data that is required in order to train, tune and test a \ac{DL}
system~\cite{vogelsang_requirements_2019}.
Difficulties arise from the amount of data needed to train a \ac{DL} model and from the need to
annotate the data for supervised learning~\cite{nowruzi_how_2019}.
However, it is possible to pretrain a model on a dataset for a related task.
The pretrained model can then be fine tuned to fit the actual task thus decreasing the needed size
in the dat set that is specific to the problem~\cite{ouyang_factors_2016}.
This procedure allows for achieving good performance.
Additionally there's many available pretraining datasets that are labeled~\cite{ouyang_factors_2016}.
In the context of requirements, quantity refers to diversity of data~\cite{vogelsang_requirements_2019}.
When it comes to the quality of data there's three factors: completeness, consistency,
correctness.
These factors are especially important since better quality as a big influence on
performance~\cite{vogelsang_requirements_2019}.
% FIXME: set into relation with data quantity? after certain amount of data convergence to value
%       but if quality better higher convergence
For completeness, it is important that the dataset that is used for finetuning contains all edge
cases that are relevant for the task~\cite{arpteg_software_2018, vogelsang_requirements_2019}.
`Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being
true'\cite{vogelsang_requirements_2019}.
As implementation and training a \ac{DL} model is not the subject of this theses these \ac{DR} are
not discussed in detail in the following chapters.
% FIXME: add following
Relevant, Complete, Balanced, Accurate~\cite{ashmore_assuring_2021}


% literature notes
Towards Requirements Specification for \ldots~\cite{hu_towards_2020}
requirements that ensure robustness (handle stressfull environmental conditions and unseen or
unexpected data) are crucial

Definition of transformations: modifications in images
--- affine (e.g.\ scaling, rotation), perceptual context transformations (e.g.\ light sources, viewpoint)
$\rightarrow$ invariant (not changing e.g. class label) --- equivariant (e.g. bounding
                box position) requirements

Assuring the Machine Learning Lifecycle~\cite{ashmore_assuring_2021}
Activities in MLC Model Learning
\begin{itemize}
    \item Model Selection: decide model type, variant, strucutre of model
    \item Training
    \item Hyperparameter Selection
    \item Transfer Learning
\end{itemize}

desired properties for ML component: performance, robustness (handle stressfl environmental
conditions and unseen or unexpected data), reusability, interpretability

`Comparing models is not always straightforward, with different models showing superior performance
against different measures. Composite metrics [59, 158] allow for a tradeoff between measures
during the training process.'

Check:~\cite{hu_towards_2020, belani_requirements_2019, siebert_construction_2021, nakamichi_requirements-driven_2020, ishikawa_evidence-driven_2020}

% FIXME: maybe not needed as section?
\begin{comment}
\section{Tradeoff}
Note: tradeoff between accuracy and computational cost $\rightarrow$ mobile phone dilemma

When determining whether automation is an improvement four aspects have to be examined.
These are time, costs, quality and flexibility.
The aspects build a quadrangle that is based on the optimizing trade-off between the
factors~\cite{dumas_fundamentals_2013}.

Without software supporting the task of reading the name of the picture and typing it into
the system, can take long seconds, whereas a trained \ac{DL} model could complete the task
in a mere instant.
Therefor automisation via \ac{DL} should improve the efficiency of the process when compared to
manually reading and typing the information off the image.

Training costs for a \ac{DL} model are very high due to the computing intensive
backpropagation algorithm that tunes the network to the data.
But the usage cost is low.
For manual labor the opposite is the case as training a person to type in a label is done quickly
and labor costs are high in comparison to the expenses for running the model.

Both \ac{DL} models and human labor are not 100\% accurate.
The question is whether the model can be as accurate or even better than its human counterpart.
This is especially interesting when it is applied in the real world where it might have to do good
in subpar situations.
An example is bad image quality.

Flexibility is concerned with how well a process can adjust to changing requirements.
A set of new equipment names that have to be included can pose a problem to a \ac{DL} model
because it is not trained for the new data.
A human on the other hand should not have any problems in this regard.

The main concern for the solution's efficacy is whether it is accurate enough.
Therefor this work focuses on this aspect in particular.
\end{comment}
