\chapter{System Design}
\section{Approach comparison}
include Pipeline differences

% TODO: approaches to research: Faster R-CNN, Mask R-CNN, Yolo v3
% TODO: look at DETR-DC5-R101 and Faster RCNN-R101-FPN+

\subsection{Approach Research}
\subsubsection*{GitHub implementation}
Two models that can be used in conjunction

\textbf{detection}~\cite{beom_text_2021}\\
uses RetinaNet structure~\cite{lin_focal_2018}\\
applies techniques from textboxes++~\cite{liao_textboxes_2018}

\textbf{character recognition}~\cite{beom_crnn_2021}\\
needs cropped text area as input\\
uses CRNN~\cite{shi_end--end_2015} $\rightarrow$ end-to-end learning, LSTM fir arbitrary length of
input and output, no need to apply detection and cropping to each single character

\subsubsection*{Tesseract}
Open Source OCR engine~\cite{smith_overview_2007}
\begin{itemize}
    \item uses Deep Learning (found c++ code for layers in repo)
    \item Processing in step-by-step pipeline, some unusual stages\\
        1. Line and Word finding\\
        1.1. Line finding\\
        1.2. Baseline Fitting\\
        1.3. Fixed Pitch Detection and Chopping\\
        1.4. Proportional Word Finding\\
        2. Word Recognition\\
        2.1 Chopping Joined Characters\\
        2.2 Accociating Broken Characters\\
        3. Static Character Classifier\\
        3.1 Features\\
        3.2 Classification\\
        3.3 Training Data\\
        4. Linguistic Analysis\\
        5. Adaptive Classifier
\end{itemize}
Performs poorly with unstructured text with significant noise


\subsubsection*{current research}
An Efficient and Accurate Scene Text Detector~\cite{zhou_east_2017}

SOFT:\ Softmax-free Transformer with Linear Complexity~\cite{lu_soft_2021}

Generative Pretraining from Pixels~\cite{chen_generative_nodate}
\begin{itemize}
    \item unsupervised representation learning (approach transfered from NLP)
    \item training of sequence Transformer to auto-regressively predict pixels without incorporating
        knowledge of 2D input structure
    \item Active part: GPT-2 scale model learns image representations and performs extremely well even
        when compared to supervised models
\end{itemize}

Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler
Divergence~\cite{yang_learning_2021}
\begin{itemize}
    \item Deductive approach to rotated object detection
    \item box is `translated' to 2D-Gaussian $\rightarrow$ KLD with prediction and true gaussian as Loss
    \item LIMIT:\ cannot be directly applied to quadrilateral detection
\end{itemize}

DP-SSL:\ Towards Robust Semi-supervised Learning with A Few Labeled Samples~\cite{xu_dp-ssl_2021}
\begin{itemize}
    \item Semi-supervised learning:
        \begin{itemize}
            \item provides way to leverage unlabeled data by pseudo labels
            \item performs poorly and unstable when size of labeled data is very small (low quality
                of pseudo labels)
        \end{itemize}
    \item Data programming:
        \begin{itemize}
            \item paradigm for the programmatic creation of training sets
            \item existing methods rely on human experts to provide initial labeling functions (LF)
        \end{itemize}
    \item DP-SSL
        \begin{itemize}
            \item multiple-choice learning (MCL) based approach to automatically generate labeling functions
            \item scheme to generate probabilistic labels for unlabeled data
        \end{itemize}
\end{itemize}



\subsection{Comparison}

\section{Approach selection}
