\chapter{Theoretical Foundation}\label{ch:theoretical}

Work with~\cite{goodfellow_deep_2016} as much as possible

\section{Machine Learning}

\begin{enumerate}
    \item Loss Function / Error Metrics: CTC-loss, Cross entropy loss and maximum likelihood
        estimation (\cite{goodfellow_deep_2016})
    \item Supervised --- Unsupervised / Categorization
    \item Optimization techniques: Stochastic-Batch Gradient Descent, GD Momentum, Adam
    \item Bias-Variance tradeoff / Overfitting --- Underfitting
        % FIXME: how much of optimization and bias-variance tradeoff is actually needed???
\end{enumerate}

\section{Deep Learning}

\begin{enumerate}
    \item ANN / MLP % Node, Feedforward, Backpropagation / Optimization
        \begin{itemize}
            \item Architecture $\rightarrow$ Input, Hidden, Output
            \item Feedforward
            \item Optimization $\rightarrow$ Backpropagation, SGD, ADAM, \ldots
        \end{itemize}
    \item Regularization: L0,L1,L2, Dropout, Dropconnect
    \item important architectures
        \begin{itemize}
            \item CNN % layers --- convolutional, max-pooling
            \item RNN % recurrent layer
            \item Specific foundation architectures for relevant approaches
        \end{itemize}
    \item transfer learning: reuse parameters from pretrained models\\
\end{enumerate}

\section{Optical Character Recognition}
