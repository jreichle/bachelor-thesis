
@misc{beom_text_2021,
	title = {Text {Detector} for {OCR}},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/Text_Detector},
	abstract = {Text detection model that combines Retinanet with textboxes++ for OCR},
	urldate = {2021-09-18},
	author = {Beom},
	month = aug,
	year = {2021},
	note = {original-date: 2019-03-12T05:11:06Z},
}

@misc{beom_crnn_2021,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-09-18},
	author = {Beom},
	month = sep,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
	annote = {CRNN implementation (Keras)},
}

@misc{bhat_rajesh-bhatspark-ai-summit-2020-text-extraction_2021,
	title = {rajesh-bhat/spark-ai-summit-2020-text-extraction},
	copyright = {MIT},
	url = {https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/43eeb1f1a27e6ae84dcb0ef4cc11494dcc61cafb/CRNN_CTC_wandb.ipynb},
	urldate = {2021-09-22},
	author = {Bhat, Rajesh Shreedhar},
	month = aug,
	year = {2021},
	note = {original-date: 2020-06-02T14:21:10Z},
}

@inproceedings{chen_improvement_2018,
	address = {Shanghai, China},
	title = {Improvement {Research} and {Application} of {Text} {Recognition} {Algorithm} {Based} on {CRNN}},
	isbn = {978-1-4503-6605-2},
	url = {http://dl.acm.org/citation.cfm?doid=3297067.3297073},
	doi = {10.1145/3297067.3297073},
	abstract = {This paper is based on CRNN model to recognize the text in the images of football matches scene, and two improvements are proposed. Considering the edge feature of text is strong, this paper adds MFM layers into CRNN model aiming to enhance the contrast. In order to solve the problem of losing details of image static features in the process of getting contextual features, this paper fuses up these two kinds of features. The training and testing experiments carried out on public dataset and manual dataset respectively verify the validity of the improvements, and the recognition accurate rate is higher than original model.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Signal} {Processing} and {Machine} {Learning} - {SPML} '18},
	publisher = {ACM Press},
	author = {Chen, Lei and Li, Shaobin},
	year = {2018},
	pages = {166--170},
	file = {Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:/Users/johannesreichle/Zotero/storage/4JUQTVGL/Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:application/pdf},
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2021-09-24},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/M7XHZX79/backpropagation-in-convolutional-neural-networks.html:text/html},
}

@misc{shperber_gentle_2021,
	title = {A gentle introduction to {OCR}},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa},
	abstract = {How and why to apply deep learning to Optical Character Recognition},
	language = {en},
	urldate = {2021-09-18},
	journal = {Medium},
	author = {Shperber, Gidi},
	month = feb,
	year = {2021},
	annote = {Has a lot of approaches that can be checked},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/JSRM93VX/a-gentle-introduction-to-ocr-ee1469a201aa.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/PFLR88V5/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TSDHL78C/4376991.html:text/html},
}

@article{zhou_east_2017,
	title = {{EAST}: {An} {Efficient} and {Accurate} {Scene} {Text} {Detector}},
	shorttitle = {{EAST}},
	url = {http://arxiv.org/abs/1704.03155},
	abstract = {Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.},
	urldate = {2021-09-18},
	journal = {arXiv:1704.03155 [cs]},
	author = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
	month = jul,
	year = {2017},
	note = {arXiv: 1704.03155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2017, fix equation (3)},
	file = {arXiv Fulltext PDF:/Users/johannesreichle/Zotero/storage/GXDSSU9P/Zhou et al. - 2017 - EAST An Efficient and Accurate Scene Text Detecto.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/2VF6VH7T/1704.html:text/html},
}

@misc{bhat_text_nodate,
	title = {Text {Recognition} {With} {CRNN}-{CTC} {Network} - {Weights} \& {Biases}},
	url = {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2021-09-22},
	journal = {W\&B},
	author = {Bhat, Rajesh Shreedhar},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/9Q9S8C2K/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI.html:text/html},
}

@article{hochreiter_long_nodate,
	title = {Long {Short} {Term} {Memory}},
	abstract = {Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitmry time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-T ime Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
	language = {en},
	author = {Hochreiter, Sepp and Schmidhuber, Jiirgen},
	pages = {12},
	file = {Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:/Users/johannesreichle/Zotero/storage/VPLXUVMT/Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:application/pdf},
}

@inproceedings{kloss_learning_2016,
	address = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	language = {en},
	urldate = {2021-09-26},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	month = oct,
	year = {2016},
	pages = {5238--5245},
	file = {Kloss et al. - 2016 - Learning where to search using visual attention.pdf:/Users/johannesreichle/Zotero/storage/VJ2CKK4W/Kloss et al. - 2016 - Learning where to search using visual attention.pdf:application/pdf},
}

@misc{odegua_how_2020,
	title = {How to put machine learning models into production},
	url = {https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/},
	abstract = {The goal of building a machine learning model is to solve a problem, and a machine learning model can only do so when it is in production and actively in use by consumers. As such, model deployment is as important as model building.},
	language = {en-US},
	urldate = {2021-09-30},
	journal = {Stack Overflow Blog},
	author = {Odegua, Rising},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/ZG37NKW8/how-to-put-machine-learning-models-into-production.html:text/html},
}

@book{dumas_fundamentals_2013,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Business} {Process} {Management}},
	isbn = {978-3-642-33142-8 978-3-642-33143-5},
	url = {http://link.springer.com/10.1007/978-3-642-33143-5},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Dumas, Marlon and La Rosa, Marcello and Mendling, Jan and Reijers, Hajo A.},
	year = {2013},
	doi = {10.1007/978-3-642-33143-5},
	file = {Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:/Users/johannesreichle/Zotero/storage/XSJHQVLD/Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:application/pdf},
}

@article{frank_style-and-citation-guide_nodate,
	title = {style-and-citation-guide},
	language = {en},
	author = {Frank, Brigitte},
	pages = {5},
	file = {Frank - style-and-citation-guide.pdf:/Users/johannesreichle/Zotero/storage/8PYU73DA/Frank - style-and-citation-guide.pdf:application/pdf},
}

@book{johannesson_introduction_2021,
	address = {Cham},
	title = {An {Introduction} to {Design} {Science}},
	isbn = {978-3-030-78131-6 978-3-030-78132-3},
	url = {https://link.springer.com/10.1007/978-3-030-78132-3},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer International Publishing},
	author = {Johannesson, Paul and Perjons, Erik},
	year = {2021},
	doi = {10.1007/978-3-030-78132-3},
	file = {Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:/Users/johannesreichle/Zotero/storage/69A2NAR7/Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:application/pdf},
}

@book{cox_translating_2017,
	address = {Berkeley, CA},
	title = {Translating {Statistics} to {Make} {Decisions}: {A} {Guide} for the {Non}-{Statistician}},
	isbn = {978-1-4842-2255-3 978-1-4842-2256-0},
	shorttitle = {Translating {Statistics} to {Make} {Decisions}},
	url = {http://link.springer.com/10.1007/978-1-4842-2256-0},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Apress},
	author = {Cox, Victoria},
	year = {2017},
	doi = {10.1007/978-1-4842-2256-0},
	file = {Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:/Users/johannesreichle/Zotero/storage/S33M4ZEW/Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:application/pdf},
}

@inproceedings{ponti_everything_2017,
	title = {Everything {You} {Wanted} to {Know} about {Deep} {Learning} for {Computer} {Vision} but {Were} {Afraid} to {Ask}},
	doi = {10.1109/SIBGRAPI-T.2017.12},
	abstract = {Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.},
	booktitle = {2017 30th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ponti, Moacir Antonelli and Ribeiro, Leonardo Sampaio Ferraz and Nazare, Tiago Santana and Bui, Tu and Collomosse, John},
	month = oct,
	year = {2017},
	note = {ISSN: 2474-0705},
	keywords = {CNN, Computational modeling, computer vision, Computer vision, deep learning, Gallium nitride, image processing, Image processing, machine learning, Machine learning, Tensile stress},
	pages = {17--41},
	annote = {Extracted Annotations (14/10/2021, 10:29:11)
"Deep Learning methods are currently the stateof-the-art in many Computer Vision and Image Processing problems," (Ponti et al 2017:17)
"This is mainly due to two reasons: the availability of labelled image datasets with millions of images [1], [2], and computer hardware that allowed to speed-up computations." (Ponti et al 2017:17)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/5DFA5TFR/Ponti et al. - 2017 - Everything You Wanted to Know about Deep Learning .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/U9DQFTKT/8250222.html:text/html},
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, artificial intelligence, backpropagation, Computer architecture, convolution neural network, deep neural network architectures, Feature extraction, Feedforward neural networks, Machine learning algorithm, optimization, Recurrent neural networks, supervised and unsupervised learning, Training},
	pages = {53040--53065},
	annote = {Extracted Annotations (14/10/2021, 10:43:39)
"The painstakingly handcrafted feature extractors used in traditional learning, classication, and pattern recognition systems are not scalable for large-sized data sets." (Shrestha and Mahmood 2019:53040)
"Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures." (Shrestha and Mahmood 2019:53040)
"DNN is a type of neural network modeled as a multilayer perceptron (MLP) that is trained with algorithms to learn representations from data sets without any manual design of feature extractors. As the name Deep Learning suggests, it consists of higher or deeper number of processing layers, which contrasts with shallow learning model with fewer layers of units." (Shrestha and Mahmood 2019:53041)},
	annote = {Must read!
Good explanation of math behind CNN, LSTM but also optimization algoritghms (like SGD)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/A5UHX3PQ/Shrestha and Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@book{balas_handbook_2019,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Handbook of {Deep} {Learning} {Applications}},
	volume = {136},
	isbn = {978-3-030-11478-7 978-3-030-11479-4},
	url = {http://link.springer.com/10.1007/978-3-030-11479-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina Emilia and Roy, Sanjiban Sekhar and Sharma, Dharmendra and Samui, Pijush},
	year = {2019},
	doi = {10.1007/978-3-030-11479-4},
	annote = {Used in Introduction for Motivation part-{\textgreater} different Applications (see TOC)},
	file = {Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:/Users/johannesreichle/Zotero/storage/EHB78HRJ/Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:application/pdf},
}

@book{prince_computer_2012,
	title = {Computer {Vision}: {Models}, {Learning}, and {Inference}},
	isbn = {978-1-107-01179-3},
	shorttitle = {Computer {Vision}},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Prince, Simon J. D.},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: PmrICLzHutgC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics},
}

@book{das_machine_2021,
	address = {Cham, Switzerland},
	series = {Studies in computational intelligence},
	title = {Machine learning algorithms for industrial applications},
	isbn = {978-3-030-50640-7},
	language = {en},
	number = {volume 907},
	publisher = {Springer},
	editor = {Das, Santosh Kumar and Das, Shom Prasad and Dey, Nilanjan and Hassanien, Aboul Ella},
	year = {2021},
	file = {Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:/Users/johannesreichle/Zotero/storage/4ABV9L7G/Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:application/pdf},
}

@book{singh_computer_2021,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Computer {Vision} and {Image} {Processing}: 5th {International} {Conference}, {CVIP} 2020, {Prayagraj}, {India}, {December} 4-6, 2020, {Revised} {Selected} {Papers}, {Part} {I}},
	volume = {1376},
	isbn = {9789811610851 9789811610868},
	shorttitle = {Computer {Vision} and {Image} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-1086-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer Singapore},
	editor = {Singh, Satish Kumar and Roy, Partha and Raman, Balasubramanian and Nagabhushan, P.},
	year = {2021},
	doi = {10.1007/978-981-16-1086-8},
	file = {Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:/Users/johannesreichle/Zotero/storage/QEEQCRND/Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:application/pdf},
}

@misc{singh_optical_nodate,
	title = {Optical {Character} {Recognition} {Techniques}: {A} {Survey}},
	shorttitle = {Optical {Character} {Recognition} {Techniques}},
	abstract = {This paper presents a literature review on English OCR techniques. English OCR system is compulsory to convert numerous published books of English into editable computer text files. Latest research in this area has been able to grown some new methodologies to overcome the complexity of English writing style. Still these algorithms have not been tested for complete characters of English Alphabet. Hence, a system is required which can handle all classes of English text and identify characters among these classes.},
	author = {Singh, Sukhpreet},
	annote = {Extracted Annotations (17/10/2021, 21:45:46)
"Optical Character Recognition [1] - [5] is a process that can convert text, present in digital image, to editable text." (Singh :1)
"The next step is to apply an OCR (Optical Character Recognition) process, meaning that the scanned image of each document will be translated into machine process able text" (Singh :1)
"a post-processing step to correct these errors is a very important part of the post-processing chain." (Singh :1)},
	file = {Singh_Optical Character Recognition Techniques.pdf:/Users/johannesreichle/Zotero/storage/67Q5REKF/Singh_Optical Character Recognition Techniques.pdf:application/pdf;Citeseer - Snapshot:/Users/johannesreichle/Zotero/storage/98UBX8JM/download.html:text/html},
}

@article{oyedotun_deep_2015,
	title = {Deep {Learning} in {Character} {Recognition} {Considering} {Pattern} {Invariance} {Constraints}},
	volume = {7},
	issn = {2074904X, 20749058},
	url = {http://www.mecs-press.org/ijisa/ijisa-v7-n7/v7n7-1.html},
	doi = {10.5815/ijisa.2015.07.01},
	abstract = {Character recognition is a field of machine learning that has been under research for several decades. The particular success of neural networks in pattern recognition and therefore character recognition is laudable. Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex. This research work reviews some of the most successful pre-training approaches to initializing deep networks such as stacked auto encoders, and deep belief networks based on achieved error rates. More importantly, this research also parallels investigating the performance of deep networks on some common problems associated with pattern recognition systems such as translational invariance, rotational invariance, scale mismatch, and noise. To achieve this, Yoruba vowel characters databases have been used in this research.},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {International Journal of Intelligent Systems and Applications},
	author = {Oyedotun, Oyebade K. and Olaniyi, Ebenezer O. and Khashman, Adnan},
	month = jun,
	year = {2015},
	pages = {1--10},
	file = {Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:/Users/johannesreichle/Zotero/storage/IWBWRJCE/Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:application/pdf},
}

@inproceedings{zhao_improving_2020,
	title = {Improving {Deep} {Learning} based {Optical} {Character} {Recognition} via {Neural} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185798},
	abstract = {Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment. We compared it with different methods, and the experimental results proved the effectiveness of our method.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Zhao, Zhenyao and Jiang, Min and Guo, Shihui and Wang, Zhenzhong and Chao, Fei and Tan, Kay Chen},
	month = jul,
	year = {2020},
	keywords = {Optical character recognition software, Text recognition, Computer architecture, Feature extraction, Training, Object detection, Task analysis},
	pages = {1--7},
	annote = {Extracted Annotations (17/10/2021, 23:42:47)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)},
	annote = {Extracted Annotations (22/10/2021, 12:48:20)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)
"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)
"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)
"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)
"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)
"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)
"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)
"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	file = {Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:/Users/johannesreichle/Zotero/storage/SHJA8U7A/Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/G9CV8S98/9185798.html:text/html},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-10-19},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {RetinaNet},
	file = {Lin et al_2018_Focal Loss for Dense Object Detection.pdf:/Users/johannesreichle/Zotero/storage/3NLWH9LE/Lin et al_2018_Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8CJ43KHN/1708.html:text/html},
}

@misc{beom_crnn_2021-1,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-10-19},
	author = {Beom},
	month = oct,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
}

@article{liao_textboxes_2018,
	title = {{TextBoxes}++: {A} {Single}-{Shot} {Oriented} {Scene} {Text} {Detector}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{TextBoxes}++},
	url = {http://arxiv.org/abs/1801.02765},
	doi = {10.1109/TIP.2018.2825107},
	abstract = {Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes\_plusplus},
	number = {8},
	urldate = {2021-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.02765},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3676--3690},
	annote = {Comment: 15 pages},
	file = {Liao et al_2018_TextBoxes++.pdf:/Users/johannesreichle/Zotero/storage/BYY77N9D/Liao et al_2018_TextBoxes++.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NPE7JJW5/1801.html:text/html},
}

@article{shi_end--end_2015,
	title = {An {End}-to-{End} {Trainable} {Neural} {Network} for {Image}-based {Sequence} {Recognition} and {Its} {Application} to {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1507.05717},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	urldate = {2021-10-19},
	journal = {arXiv:1507.05717 [cs]},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.05717},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 figures},
	file = {Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:/Users/johannesreichle/Zotero/storage/8PJQ4H5U/Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NAI47ZEJ/1507.html:text/html},
}

@misc{noauthor_search_nodate,
	title = {Search · · argman/{EAST}},
	url = {https://github.com/argman/EAST},
	abstract = {A tensorflow implementation of EAST text detector. Contribute to argman/EAST development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {Tensorflow EAST implementation},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/DSDTHJ9A/EAST.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {SakuraRiven}/{EAST} at pythonrepo.com},
	url = {https://github.com/SakuraRiven/EAST},
	abstract = {PyTorch Re-Implementation of EAST: An Efficient and Accurate Scene Text Detector - GitHub - SakuraRiven/EAST at pythonrepo.com},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {EAST with PyTorch},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/2XJ788B8/EAST.html:text/html},
}

@misc{noauthor_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Arbitrary}-{Shaped} {Text} ({RRC}-{ArT})},
	url = {https://deepai.org/publication/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art},
	abstract = {09/16/19 - This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped
Text (RRC-ArT) that consists of three major challeng...},
	urldate = {2021-10-19},
	journal = {DeepAI},
	month = sep,
	year = {2019},
	annote = {Challenge that goes in the right direction -{\textgreater} read},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/R524XQNK/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art.html:text/html},
}

@inproceedings{jenckel_anyocr_2016,
	address = {Cancun},
	title = {{anyOCR}: {A} sequence learning based {OCR} system for unlabeled historical documents},
	isbn = {978-1-5090-4847-2},
	shorttitle = {{anyOCR}},
	url = {http://ieeexplore.ieee.org/document/7900265/},
	doi = {10.1109/ICPR.2016.7900265},
	abstract = {Institutes and libraries around the globe are preserving the literary heritage by digitizing historical documents. However, to make this data easily accessible the scanned documents need to be transformed into search-able text. State of the art OCR systems using Long-Short-Term-Memory networks (LSTM) have been applied successfully to recognize text in both printed and handwritten form. Besides the general challenges with historical documents, e.g. poor image quality, damaged characters, etc., especially unknown scripts and old fonds make it difficult to provide the large amount of transcribed training data required for these methods to perform well. Transcribing the documents manually is very costly in terms of manhours and require language specific expertise. The unknown fonds and requirement for meaningful context also make the use of synthetic data unfeasible.},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Jenckel, Martin and Bukhari, Syed Saqib and Dengel, Andreas},
	month = dec,
	year = {2016},
	pages = {4035--4040},
	file = {Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:/Users/johannesreichle/Zotero/storage/YDA4CDN8/Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:application/pdf},
}
