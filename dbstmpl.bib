
@misc{beom_crnn_2021,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-09-18},
	author = {Beom},
	month = sep,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
	annote = {CRNN implementation (Keras)},
}

@misc{bhat_rajesh-bhatspark-ai-summit-2020-text-extraction_2021,
	title = {rajesh-bhat/spark-ai-summit-2020-text-extraction},
	copyright = {MIT},
	url = {https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/43eeb1f1a27e6ae84dcb0ef4cc11494dcc61cafb/CRNN_CTC_wandb.ipynb},
	urldate = {2021-09-22},
	author = {Bhat, Rajesh Shreedhar},
	month = aug,
	year = {2021},
	note = {original-date: 2020-06-02T14:21:10Z},
}

@inproceedings{chen_improvement_2018,
	address = {Shanghai, China},
	title = {Improvement {Research} and {Application} of {Text} {Recognition} {Algorithm} {Based} on {CRNN}},
	isbn = {978-1-4503-6605-2},
	url = {http://dl.acm.org/citation.cfm?doid=3297067.3297073},
	doi = {10.1145/3297067.3297073},
	abstract = {This paper is based on CRNN model to recognize the text in the images of football matches scene, and two improvements are proposed. Considering the edge feature of text is strong, this paper adds MFM layers into CRNN model aiming to enhance the contrast. In order to solve the problem of losing details of image static features in the process of getting contextual features, this paper fuses up these two kinds of features. The training and testing experiments carried out on public dataset and manual dataset respectively verify the validity of the improvements, and the recognition accurate rate is higher than original model.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Signal} {Processing} and {Machine} {Learning} - {SPML} '18},
	publisher = {ACM Press},
	author = {Chen, Lei and Li, Shaobin},
	year = {2018},
	pages = {166--170},
	file = {Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:/Users/johannesreichle/Zotero/storage/4JUQTVGL/Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:application/pdf},
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2021-09-24},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/M7XHZX79/backpropagation-in-convolutional-neural-networks.html:text/html},
}

@misc{shperber_gentle_2021,
	title = {A gentle introduction to {OCR}},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa},
	abstract = {How and why to apply deep learning to Optical Character Recognition},
	language = {en},
	urldate = {2021-09-18},
	journal = {Medium},
	author = {Shperber, Gidi},
	month = feb,
	year = {2021},
	annote = {Has a lot of approaches that can be checked},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/JSRM93VX/a-gentle-introduction-to-ocr-ee1469a201aa.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/PFLR88V5/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TSDHL78C/4376991.html:text/html},
}

@article{zhou_east_2017,
	title = {{EAST}: {An} {Efficient} and {Accurate} {Scene} {Text} {Detector}},
	shorttitle = {{EAST}},
	url = {http://arxiv.org/abs/1704.03155},
	abstract = {Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.},
	urldate = {2021-09-18},
	journal = {arXiv:1704.03155 [cs]},
	author = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
	month = jul,
	year = {2017},
	note = {arXiv: 1704.03155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2017, fix equation (3)},
	file = {arXiv Fulltext PDF:/Users/johannesreichle/Zotero/storage/GXDSSU9P/Zhou et al. - 2017 - EAST An Efficient and Accurate Scene Text Detecto.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/2VF6VH7T/1704.html:text/html},
}

@misc{bhat_text_nodate,
	title = {Text {Recognition} {With} {CRNN}-{CTC} {Network} - {Weights} \& {Biases}},
	url = {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2021-09-22},
	journal = {W\&B},
	author = {Bhat, Rajesh Shreedhar},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/9Q9S8C2K/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI.html:text/html},
}

@article{hochreiter_long_nodate,
	title = {Long {Short} {Term} {Memory}},
	abstract = {Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitmry time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-T ime Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
	language = {en},
	author = {Hochreiter, Sepp and Schmidhuber, Jiirgen},
	pages = {12},
	file = {Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:/Users/johannesreichle/Zotero/storage/VPLXUVMT/Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:application/pdf},
}

@inproceedings{kloss_learning_2016,
	address = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	language = {en},
	urldate = {2021-09-26},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	month = oct,
	year = {2016},
	pages = {5238--5245},
	file = {Kloss et al. - 2016 - Learning where to search using visual attention.pdf:/Users/johannesreichle/Zotero/storage/VJ2CKK4W/Kloss et al. - 2016 - Learning where to search using visual attention.pdf:application/pdf},
}

@book{dumas_fundamentals_2013,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Business} {Process} {Management}},
	isbn = {978-3-642-33142-8 978-3-642-33143-5},
	url = {http://link.springer.com/10.1007/978-3-642-33143-5},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Dumas, Marlon and La Rosa, Marcello and Mendling, Jan and Reijers, Hajo A.},
	year = {2013},
	doi = {10.1007/978-3-642-33143-5},
	file = {Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:/Users/johannesreichle/Zotero/storage/XSJHQVLD/Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:application/pdf},
}

@article{frank_style-and-citation-guide_nodate,
	title = {style-and-citation-guide},
	language = {en},
	author = {Frank, Brigitte},
	pages = {5},
	file = {Frank - style-and-citation-guide.pdf:/Users/johannesreichle/Zotero/storage/8PYU73DA/Frank - style-and-citation-guide.pdf:application/pdf},
}

@book{johannesson_introduction_2021,
	address = {Cham},
	title = {An {Introduction} to {Design} {Science}},
	isbn = {978-3-030-78131-6 978-3-030-78132-3},
	url = {https://link.springer.com/10.1007/978-3-030-78132-3},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer International Publishing},
	author = {Johannesson, Paul and Perjons, Erik},
	year = {2021},
	doi = {10.1007/978-3-030-78132-3},
	file = {Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:/Users/johannesreichle/Zotero/storage/69A2NAR7/Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:application/pdf},
}

@book{cox_translating_2017,
	address = {Berkeley, CA},
	title = {Translating {Statistics} to {Make} {Decisions}: {A} {Guide} for the {Non}-{Statistician}},
	isbn = {978-1-4842-2255-3 978-1-4842-2256-0},
	shorttitle = {Translating {Statistics} to {Make} {Decisions}},
	url = {http://link.springer.com/10.1007/978-1-4842-2256-0},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Apress},
	author = {Cox, Victoria},
	year = {2017},
	doi = {10.1007/978-1-4842-2256-0},
	file = {Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:/Users/johannesreichle/Zotero/storage/S33M4ZEW/Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:application/pdf},
}

@inproceedings{ponti_everything_2017,
	title = {Everything {You} {Wanted} to {Know} about {Deep} {Learning} for {Computer} {Vision} but {Were} {Afraid} to {Ask}},
	doi = {10.1109/SIBGRAPI-T.2017.12},
	abstract = {Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.},
	booktitle = {2017 30th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ponti, Moacir Antonelli and Ribeiro, Leonardo Sampaio Ferraz and Nazare, Tiago Santana and Bui, Tu and Collomosse, John},
	month = oct,
	year = {2017},
	note = {ISSN: 2474-0705},
	keywords = {CNN, Computational modeling, computer vision, Computer vision, deep learning, Gallium nitride, image processing, Image processing, machine learning, Machine learning, Tensile stress},
	pages = {17--41},
	annote = {Extracted Annotations (14/10/2021, 10:29:11)
"Deep Learning methods are currently the stateof-the-art in many Computer Vision and Image Processing problems," (Ponti et al 2017:17)
"This is mainly due to two reasons: the availability of labelled image datasets with millions of images [1], [2], and computer hardware that allowed to speed-up computations." (Ponti et al 2017:17)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/5DFA5TFR/Ponti et al. - 2017 - Everything You Wanted to Know about Deep Learning .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/U9DQFTKT/8250222.html:text/html},
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, artificial intelligence, backpropagation, Computer architecture, convolution neural network, deep neural network architectures, Feature extraction, Feedforward neural networks, Machine learning algorithm, optimization, Recurrent neural networks, supervised and unsupervised learning, Training},
	pages = {53040--53065},
	annote = {Extracted Annotations (14/10/2021, 10:43:39)
"The painstakingly handcrafted feature extractors used in traditional learning, classication, and pattern recognition systems are not scalable for large-sized data sets." (Shrestha and Mahmood 2019:53040)
"Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures." (Shrestha and Mahmood 2019:53040)
"DNN is a type of neural network modeled as a multilayer perceptron (MLP) that is trained with algorithms to learn representations from data sets without any manual design of feature extractors. As the name Deep Learning suggests, it consists of higher or deeper number of processing layers, which contrasts with shallow learning model with fewer layers of units." (Shrestha and Mahmood 2019:53041)},
	annote = {Must read!
Good explanation of math behind CNN, LSTM but also optimization algoritghms (like SGD)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/A5UHX3PQ/Shrestha and Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@book{balas_handbook_2019,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Handbook of {Deep} {Learning} {Applications}},
	volume = {136},
	isbn = {978-3-030-11478-7 978-3-030-11479-4},
	url = {http://link.springer.com/10.1007/978-3-030-11479-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina Emilia and Roy, Sanjiban Sekhar and Sharma, Dharmendra and Samui, Pijush},
	year = {2019},
	doi = {10.1007/978-3-030-11479-4},
	annote = {Used in Introduction for Motivation part-{\textgreater} different Applications (see TOC)},
	file = {Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:/Users/johannesreichle/Zotero/storage/EHB78HRJ/Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:application/pdf},
}

@book{prince_computer_2012,
	title = {Computer {Vision}: {Models}, {Learning}, and {Inference}},
	isbn = {978-1-107-01179-3},
	shorttitle = {Computer {Vision}},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Prince, Simon J. D.},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: PmrICLzHutgC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics},
}

@book{das_machine_2021,
	address = {Cham, Switzerland},
	series = {Studies in computational intelligence},
	title = {Machine learning algorithms for industrial applications},
	isbn = {978-3-030-50640-7},
	language = {en},
	number = {volume 907},
	publisher = {Springer},
	editor = {Das, Santosh Kumar and Das, Shom Prasad and Dey, Nilanjan and Hassanien, Aboul Ella},
	year = {2021},
	file = {Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:/Users/johannesreichle/Zotero/storage/4ABV9L7G/Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:application/pdf},
}

@book{singh_computer_2021,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Computer {Vision} and {Image} {Processing}: 5th {International} {Conference}, {CVIP} 2020, {Prayagraj}, {India}, {December} 4-6, 2020, {Revised} {Selected} {Papers}, {Part} {I}},
	volume = {1376},
	isbn = {9789811610851 9789811610868},
	shorttitle = {Computer {Vision} and {Image} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-1086-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer Singapore},
	editor = {Singh, Satish Kumar and Roy, Partha and Raman, Balasubramanian and Nagabhushan, P.},
	year = {2021},
	doi = {10.1007/978-981-16-1086-8},
	file = {Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:/Users/johannesreichle/Zotero/storage/QEEQCRND/Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:application/pdf},
}

@article{oyedotun_deep_2015,
	title = {Deep {Learning} in {Character} {Recognition} {Considering} {Pattern} {Invariance} {Constraints}},
	volume = {7},
	issn = {2074904X, 20749058},
	url = {http://www.mecs-press.org/ijisa/ijisa-v7-n7/v7n7-1.html},
	doi = {10.5815/ijisa.2015.07.01},
	abstract = {Character recognition is a field of machine learning that has been under research for several decades. The particular success of neural networks in pattern recognition and therefore character recognition is laudable. Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex. This research work reviews some of the most successful pre-training approaches to initializing deep networks such as stacked auto encoders, and deep belief networks based on achieved error rates. More importantly, this research also parallels investigating the performance of deep networks on some common problems associated with pattern recognition systems such as translational invariance, rotational invariance, scale mismatch, and noise. To achieve this, Yoruba vowel characters databases have been used in this research.},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {International Journal of Intelligent Systems and Applications},
	author = {Oyedotun, Oyebade K. and Olaniyi, Ebenezer O. and Khashman, Adnan},
	month = jun,
	year = {2015},
	pages = {1--10},
	annote = {Extracted Annotations (22/10/2021, 16:59:10)
"Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex." (Oyedotun et al 2015:1)
"Neural networks, conversely, can learn the features of task on which they are designed and trained; they can also adapt to some moderate variations such as noise on the data they have been trained with, hence considered intelligent. The success of neural networks in contrast to other non-intelligent recognition approaches is striking, based on performance, and somewhat ease of design considering the capability of neural networks in approximation any mapping function of inputs to outputs while requiring „least‟ domain specific knowledge for its programming each time it is embedded in different applications. i.e. self-programming." (Oyedotun et al 2015:1)
"consideration on the amount of common pattern invariance achievable in learning has also changed; such common invariances include relatively moderate translation, rotation, scale mismatch, and noisy patterns." (Oyedotun et al 2015:2)
"Conversely, this paper takes an alternative approach, by presenting a work which focuses on invariance learning based on the neural network architectures, rather than complex training data manipulation schemes and painstaking mathematical foundations. It is noteworthy that this research did not employ any invariant feature extraction technique; hence, investigates how neural network structures and learning paradigms affect invariance learning. Also, to reinforce the application importance of this work, real life data, „handwritten characters‟, have been used to train and simulate the considered networks." (Oyedotun et al 2015:3)
"Unfortunately, the difficulty is to synthesize, and then to efficiently compute, the classification function that maps objects to categories, given that objects in a category can have widely varying input representations [7]; bearing in mind also that this approach increases computational load on the designed system. A better approach is to consider neural network architectures that allow some level of built-in invariance due to structure; and of course, this can usually still be augmented with some handcrafted invariance achieved through data manipulation schemes." (Oyedotun et al 2015:3)
"Such features include:  distributed representation of knowledge at each hidden layer.  distinct features are extracted by units or neurons in each hidden layer.  several units can be active concurrently." (Oyedotun et al 2015:3)
"Generally, it is conceived that in deep networks, the first hidden layer extracts some primary features about the input, then these features are combined in the second layer to more defined features, and these features are further combined into well more defined features in the following layers, and so on. This can be somewhat seen as a hierarchical representation of knowledge;" (Oyedotun et al 2015:3)
"Deep learning depicts neural network architectures of more than a single hidden layer (multilayer networks); in contrast to networks of single hidden layer which are commonly referred to as shallow networks." (Oyedotun et al 2015:3)
"Generative Architectures This class of deep networks is not required to be deterministic of the class patterns that the inputs belong, but is used to sample joint statistical distribution of data; moreover this class of networks relies on unsupervised learning." (Oyedotun et al 2015:4)
"Discriminative Architectures Discriminative deep networks actually are required to be deterministic of the correlation of input data to the classes of patterns therein. Moreover, this category of networks relies on supervised learning." (Oyedotun et al 2015:4)
"Hybrid Architectures Networks that belong to this class rely on the combination of generative and discriminative approach in their architectures. Generally, such networks are generatively pre-trained and then discriminately finetuned for deterministic purposes." (Oyedotun et al 2015:4)
"The application of auto encoders and therefore generative architectures leverage on the unavailability of labelled data or the required logistics and cost that may be necessary in labeling available data. It therefore follows that generative learning suffices in situations where we have large unlabelled data and small labelled data" (Oyedotun et al 2015:4)
"The auto encoder can be seen as an encoder-decoder system, where the encoder (input-hidden layer pair) receives the input, extracting essential features for reconstruction; while the decoder (hidden-output layer pair) part receives the features extracted from the hidden layer, performing reconstruction at its best." (Oyedotun et al 2015:4)
"The training approach that is used in achieving learning in generative network architectures is known as „greedy layer-wise pre-training‟." (Oyedotun et al 2015:5)
"A DBN is a deep network, which is graphical and probabilistic in nature; it is essentially a generative model too. A belief net is a directed acyclic graph composed of stochastic variables [19]." (Oyedotun et al 2015:5)
"Such a training scheme is aimed at maximizing the likelihood of the input vector at a layer below given a configuration of a hidden layer that is directly on top of it." (Oyedotun et al 2015:6)
"A restricted Boltzmann machine has only two layers (fig.8); the input (visible) and the hidden layer. The connections between the two layers are undirected, and there are no interconnections between units of the same layer as in the general Boltzmann machine. We can therefore say that from the restriction in interconnections of units in layers, units are conditionally independent. The RBM can be seen as a Markov network, where the visible layer consists of either Bernoulli (binary) or Gaussian (real values usually between from 0 to 1) stochastic units, and the hidden layer of stochastic Bernoulli units [22]." (Oyedotun et al 2015:6)
"The simulation results on the considered invariances for the different trained networks are shown in table 3. It can be seen that the SDAE has the lowest error rate on translation, while the DBN outperformed other networks on rotational and scale invariances." (Oyedotun et al 2015:8)
"hence we can conjure that these networks were able to explore a more complex space of solutions while learning to the deep nature; since hierarchical learning allows more distributed knowledge representation." (Oyedotun et al 2015:8)
"It will be seen that the deep belief networks on the average, performed best compared to the other networks on variances like translation, rotation and scale mismatch; while its tolerance to noise decreased noticeably as the level of noise was increased as shown in table 4, table 5, and fig.15." (Oyedotun et al 2015:9)
"These variances are common constraints that occur in real life recognition systems for handwritten characters, and some of the solutions have been constraining the users (writers) to some particular possible domains of writing spaces or earmarked pattern of writing in order for low error rates to be achieved." (Oyedotun et al 2015:9)
"It has been shown that another flavour of neural networks, "convolutional networks" and its deep variant give very motivating performance on some of these constraints [25], however the complexity of these networks is somewhat obvious." (Oyedotun et al 2015:9)},
	file = {Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:/Users/johannesreichle/Zotero/storage/IWBWRJCE/Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:application/pdf},
}

@inproceedings{zhao_improving_2020,
	title = {Improving {Deep} {Learning} based {Optical} {Character} {Recognition} via {Neural} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185798},
	abstract = {Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment. We compared it with different methods, and the experimental results proved the effectiveness of our method.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Zhao, Zhenyao and Jiang, Min and Guo, Shihui and Wang, Zhenzhong and Chao, Fei and Tan, Kay Chen},
	month = jul,
	year = {2020},
	keywords = {Optical character recognition software, Text recognition, Computer architecture, Feature extraction, Training, Object detection, Task analysis},
	pages = {1--7},
	annote = {Extracted Annotations (17/10/2021, 23:42:47)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)},
	annote = {Extracted Annotations (22/10/2021, 12:48:20)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)
"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)
"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)
"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)
"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)
"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)
"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)
"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	annote = {Extracted Annotations (30/11/2021, 12:04:05)"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	file = {Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:/Users/johannesreichle/Zotero/storage/SHJA8U7A/Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/G9CV8S98/9185798.html:text/html},
}

@misc{beom_crnn_2021-1,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-10-19},
	author = {Beom},
	month = oct,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
}

@article{liao_textboxes_2018,
	title = {{TextBoxes}++: {A} {Single}-{Shot} {Oriented} {Scene} {Text} {Detector}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{TextBoxes}++},
	url = {http://arxiv.org/abs/1801.02765},
	doi = {10.1109/TIP.2018.2825107},
	abstract = {Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes\_plusplus},
	number = {8},
	urldate = {2021-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.02765},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3676--3690},
	annote = {Comment: 15 pages},
	file = {Liao et al_2018_TextBoxes++.pdf:/Users/johannesreichle/Zotero/storage/BYY77N9D/Liao et al_2018_TextBoxes++.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NPE7JJW5/1801.html:text/html},
}

@article{srivastava_comparative_2021,
	title = {Comparative analysis of deep learning image detection algorithms},
	volume = {8},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00434-w},
	doi = {10.1186/s40537-021-00434-w},
	abstract = {A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Journal of Big Data},
	author = {Srivastava, Shrey and Divekar, Amit Vishvas and Anilkumar, Chandu and Naik, Ishika and Kulkarni, Ved and Pattabiraman, V.},
	month = dec,
	year = {2021},
	pages = {66},
	annote = {Extracted Annotations (16/11/2021, 15:54:27)"From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms." (Srivastava et al 2021:66)"Fast R-CNN model as a method of object detection [3]. It makes use of the CNN method in the target detection field. The novelty of the method proposed by Girshick has proposed a window extraction algorithm instead of a conventional sliding window extraction procedure in the R-CNN model, there is separate training for the deep convolution network for feature isolation and the support vector machines for categorization [4]. In the fast RCNN method they have combined feature extraction with classification into a classification framework" (Srivastava et al 2021:67)"Whereas in the faster R-CNN method the proposal isolation region and bit of Fast R-CNN are put into a network template referred to as region proposal network (RPN). The accuracy of Fast R-CNN and Faster R-CNN is the same." (Srivastava et al 2021:67)"You Only Look Once (YOLO)—A one-time convolutional neural network for the prediction of the frame position and classification of multiple candidates is offered by YOLO. Endto-end target detection can be achieved this way. It uses a regression problem to solve object detection. A single end-to-end system completes the process of putting the output obtained from the original image to the category and position" (Srivastava et al 2021:67)"advanced YOLO v1 network model which optimizes the loss of function in YOLO v1, it has a new inception model structure, has a specialized pooling pyramid layer, and has better performance." (Srivastava et al 2021:68)"According to the team, SSD is a simple method and requires an object proposal as it is based on the complete elimination of the process that generates a proposal. It also eliminates the subsequent pixel and resampling stages. So, it combines everything into a single step. SSD is also very easy to train and is very straightforward when it comes to integrating it into the system. This makes detection easier. The primary feature of SSD is using multiscale convolutional bounding box outputs that are attached to several feature maps" (Srivastava et al 2021:68)"The best feature of Tiny SSD is its size of 2.3 MB which is even smaller than Tiny YOLO." (Srivastava et al 2021:68)"The paper also accesses some deep learning techniques for object detection systems. The current paper states that deep CNNs work on the principle of weight sharing. It gives us information about some crucial points in CNN. These features of CNN depicted in this paper are: [1] a. CNN is integration and involves the multiplication of two overlapping functions. b. Features maps are abstracted to reduce their complexity in terms of space c. Repetition of the process is done to produce the feature maps using filters. d. CNN utilizes different types of pooling layers." (Srivastava et al 2021:68)"In this work the multi-layered system they introduced the Squeeze-and-Excitation model as an additional layer to the SSD model. The improved model employed self-learning that further enhanced the accuracy of the system for small scale pedestrian detection." (Srivastava et al 2021:69)"Architectural Innovations (2014-2020): The well-known and widely used VGG architecture was developed in 2014 [22]. RCNN, based on VGG like many others, introduced the idea that objects are located in certain regions of the image; hence the name: region-based CNN [23]. Improved versions of RCNN—Fast RCNN [24] and Faster RCNN [3] came out in the subsequent years. Both of these reduced computation time, while maintaining the accuracy that RCNN is known for. Single Shot Multibox Detector (SSD), also based on VGG was developed around 2016 [8]. Another algorithm, You Only Look Once (YOLO), based on an architecture called DarkNet was first published in 2016 [6]. It is in active development; its third version was released in 2018 [25]." (Srivastava et al 2021:70)"SSD does not resample pixels or features for bounding box hypotheses and is as accurate as models that do. In addition to this, it is quite straightforward compared to methods that require object proposals because it completely eradicates feature resampling stages or pixel and proposal generation, by encompassing all computation in a single network. Therefore, SSD is very simple to train and can be easily integrated into systems that perform detection as one of their functions [8]. It's architecture heavily depends on the generation of bounding boxes and the extraction of feature maps, which are also known as default bounding boxes. Loss is calculated by the network, using comparisons of the offsets of the predicted classes and the default bounding boxes with the training samples' ground truth values, using different filters for every iteration. Using the back-propagation algorithm and the calculated loss value, all the parameters are updated." (Srivastava et al 2021:71)"SSD is built on a feed-forward complex network that builds a collection of standardsize bounding boxes and for each occurrence of an object in those boxes, a respective score. After score generation, non-maximum suppression is used to generate the final detection results. The preliminary network layers are built on a standard architecture utilized for high quality image classification (and truncated before any classification layers), which is a VGG-16 network. An auxiliary structure is added to the truncated base network such as convo6 to produce detections." (Srivastava et al 2021:71)"The reason for using auxiliary layers is because they allow us to extract the required features at multiple scales as well as reduce the size of our input with each layer that is traversed through [8]. For each cell in the image, the layer makes a certain number of predications. Each prediction consists of a boundary box and the box generates scores for all the classes it detects in this box including a score for no object at all." (Srivastava et al 2021:71)"Convolutional predictors for object detection: Every feature layer produces a fixed number of predictions by utilising convolutional filters. For every feature layer of size x × y having n channels, the rudimentary component for generating prediction variables of a potential detection result is a 3 × 3 × x small kernel that creates a confidence score for every class," (Srivastava et al 2021:71)non-maximum suppression: algorithm to find best proposed bounding box with IOU (note on p.71)"This computation results in a total of (s + 4) b filters that are applicable to every location in the feature map, resulting in (s + 4) × b × x × y outputs for a x × y feature ma" (Srivastava et al 2021:72)"All SSD predictions are divided into two types; negative matches or positive matches. Positive matches are only used by SSD to calculate the localization cost which is the misalignment of the boundary box with the default box." (Srivastava et al 2021:72)"However, SSD is not as efficient at detection for smaller objects, which can be solved by having a more efficient feature extractor backbone (e.g., ResNet101), with the addition of deconvolution layers along with skip connections to create additional large-scale context, and design a better network structure" (Srivastava et al 2021:73)"The algorithm of the original R-CNN technique is as follows: [29] 1. Using a Selective Search Algorithm, several candidate region proposals are extracted from the input image. In this algorithm, numerous candidate regions are generated in initial sub-segmentation. Then, regions which are similar are combined to form bigger regions using a greedy algorithm. These regions make up the final region proposals. 2. The CNN component warps the proposals and extracts distinct features as a vector output. 3. The features which are extracted are fed into an SVM (Support Vector Machine) for recognizing objects of interest in the proposal." (Srivastava et al 2021:75)"Fast R-CNN is an algorithm for object detection that solves some of the drawbacks of R-CNN. It uses an approach similar to that of its predecessor, but as opposed to using region proposals, the CNN utilizes the image itself for creating a convolutional feature map, following which region proposals are determined and warped from it. An RoI (Region of Interest) pooling layer is employed for reshaping the warped squares according to a predefined size for a fully connected layer to accept them. The region class is then predicted from the RoI vector with the help of a SoftMax laye" (Srivastava et al 2021:75)"YOLOv3 In modern times YOLO (You Only Look Once) is one of the most precise and accurate object detection algorithms available. It has been made on the basis of a newly altered and customized architecture named Darknet [25]" (Srivastava et al 2021:77)"YOLOv3 makes use of the latest darknet features like 53 layers and it has undergone training with one of the most reliable datasets called ImageNet. The layers used are from an architecture Darnnet-53 which is convolutional in nature. For detection, the aforementioned 53 layers were supplemented instead of the pre-existing 19 and this enhanced architecture was trained and instructed with PASCAL VOC." (Srivastava et al 2021:77)"It was found that Yolo-v3 is the fastest with SSD following closely and Faster RCNN coming in the last place. However, it can be said that the use case influences which algorithm is picked; if you are dealing with a relatively small dataset and don't need real-time results, it is best to go with Faster RCNN. Yolo-v3 is the one to pick if you need to analyse a live video feed. Meanwhile, SSD provides a good balance between speed and accuracy." (Srivastava et al 2021:90)},
	file = {Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:/Users/johannesreichle/Zotero/storage/2EQVX9L2/Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:application/pdf},
}

@article{chen_generative_2021,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	year = {2021},
	pages = {12},
	file = {Chen et al. - Generative Pretraining from Pixels.pdf:/Users/johannesreichle/Zotero/storage/A235GR5L/Chen et al. - Generative Pretraining from Pixels.pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Wolf et al_2020_Transformers.pdf:/Users/johannesreichle/Zotero/storage/WU9PCJ4B/Wolf et al_2020_Transformers.pdf:application/pdf},
}

@inproceedings{vicol_unbiased_2021,
	title = {Unbiased {Gradient} {Estimation} in {Unrolled} {Computation} {Graphs} with {Persistent} {Evolution} {Strategies}},
	url = {https://proceedings.mlr.press/v139/vicol21a.html},
	abstract = {Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.},
	language = {en},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10553--10563},
	file = {Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:/Users/johannesreichle/Zotero/storage/9NR7ES72/Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:application/pdf;Supplementary PDF:/Users/johannesreichle/Zotero/storage/6FXHTMRX/Vicol et al. - 2021 - Unbiased Gradient Estimation in Unrolled Computati.pdf:application/pdf},
}

@article{yang_learning_2021,
	title = {Learning {High}-{Precision} {Bounding} {Box} for {Rotated} {Object} {Detection} via {Kullback}-{Leibler} {Divergence}},
	url = {http://arxiv.org/abs/2106.01883},
	abstract = {Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios. Taking the perspective that horizontal detection is a special case for rotated object detection, in this paper, we are motivated to change the design of rotation regression loss from induction paradigm to deduction methodology, in terms of the relation between rotation and horizontal detection. We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio. This mechanism can be vital for high-precision detection as a slight angle error would cause a serious accuracy drop for large aspect ratios objects. More importantly, we have proved that KLD is scale invariant. We further show that the KLD loss can be degenerated into the popular \$l\_\{n\}\$-norm loss for horizontal detection. Experimental results on seven datasets using different detectors show its consistent superiority, and codes are available at https://github.com/yangxue0827/RotationDetection.},
	urldate = {2021-11-02},
	journal = {arXiv:2106.01883 [cs]},
	author = {Yang, Xue and Yang, Xiaojiang and Yang, Jirui and Ming, Qi and Wang, Wentao and Tian, Qi and Yan, Junchi},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01883},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 16 pages, 5 figures, 8 tables, accepted by NeurIPS21, codes are available at https://github.com/yangxue0827/RotationDetection},
	annote = {Extracted Annotations (04/11/2021, 18:11:57)
"Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios." (Yang et al 2021:1)
"We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio." (Yang et al 2021:1)
"In this paper, we take a step back, and aim to develop (from a deductive perspective) a unified regression framework for rotation detection and its special case: horizontal detection. In fact, our new framework enjoys a coherent property that it can be degenerated into the current commonly used regression loss (e.g. ln -norm) in special cases (horizontal detection), as shown in Figure 1(b)." (Yang et al 2021:2)
"The mainstream classical object detection algorithms can be roughly divided according to the following standards: Two- [7, 8, 9, 11] or Single-stage [10, 18, 19] object detection, Anchor-free [20, 21, 22] or Anchor-based [8, 9, 10] object detection and CNN [8, 10, 20] or Transformer-based [23, 24] object detection. Although the pipelines may vary, the mainstream regression loss often uses the popular ln -norm loss (such as smooth L1 loss) or IoU-based loss" (Yang et al 2021:3)
"However, horizontal detectors do not provide accurate orientation and scale information." (Yang et al 2021:3)
"The overall regression loss for rotation detection is: Lreg = ln -norm (tx ; ty ; tw ; th ; t )" (Yang et al 2021:4)
"It can be seen that parameters are optimized independently, making the loss (or detection accuracy) sensitive to the under-fitting of any of the parameters. This mechanism is fatal to high-precision detection." (Yang et al 2021:4)
text has long aspect ratios -{\textgreater} angle parameter very important -{\textgreater} KLD approach is favored (note on p.4)
 
"Although GWD scheme has played a preliminary exploration of the deductive paradigm, it does not focus on achieving high-precision detection and scale invariance. In the following, we will propose our new approach based on the Kullback-Leibler divergence (KLD)" (Yang et al 2021:5)
Gaussians are constructed like with GWD scheme (note on p.5)
 
"ICDAR2015, MLT and MSRA-TD500 are commonly used for oriented scene text detection and spotting. ICDAR2015 includes 1,000 training images and 500 testing images. ICDAR2017 MLT is a multi-lingual text dataset, which includes 7,200 training images, 1,800 validation images and 9,000 testing images. MSRA-TD500 dataset consists of 300 training images and 200 testing images." (Yang et al 2021:7)
"Limitations. Despite the theoretical grounds and the promising experimental justifications, our method has an obvious limitation that it cannot be directly applied to quadrilateral detection [33, 44]." (Yang et al 2021:10)},
	file = {Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:/Users/johannesreichle/Zotero/storage/JF2GVMIQ/Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/H7Q9M3ZH/2106.html:text/html},
}

@article{xu_dp-ssl_2021,
	title = {{DP}-{SSL}: {Towards} {Robust} {Semi}-supervised {Learning} with {A} {Few} {Labeled} {Samples}},
	shorttitle = {{DP}-{SSL}},
	url = {http://arxiv.org/abs/2110.13740},
	abstract = {The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning{\textasciitilde}(MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82\% annotation accuracy on unlabeled data and 93.46\% classification accuracy on test data, which are higher than the SOTA results.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.13740 [cs]},
	author = {Xu, Yi and Ding, Jiandong and Zhang, Lu and Zhou, Shuigeng},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13740},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021; 16 pages with appendix},
	file = {Xu et al_2021_DP-SSL.pdf:/Users/johannesreichle/Zotero/storage/7RVZR9N7/Xu et al_2021_DP-SSL.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XLYQEFMP/2110.html:text/html},
}

@article{lu_soft_2021,
	title = {{SOFT}: {Softmax}-free {Transformer} with {Linear} {Complexity}},
	shorttitle = {{SOFT}},
	url = {http://arxiv.org/abs/2110.11945},
	abstract = {Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.11945 [cs]},
	author = {Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2021 Spotlight. Project page at https://fudan-zvg.github.io/SOFT/},
	file = {Lu et al_2021_SOFT.pdf:/Users/johannesreichle/Zotero/storage/W6LD7EJZ/Lu et al_2021_SOFT.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/ZXCCUE46/2110.html:text/html},
}

@inproceedings{raisi_transformer-based_2021,
	address = {Nashville, TN, USA},
	title = {Transformer-based {Text} {Detection} in the {Wild}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522851/},
	doi = {10.1109/CVPRW53098.2021.00353},
	abstract = {A major limitation to most state-of-the-art visual localization methods is their ineptitude to make use of ubiquitous signs and directions that are typically intuitive to humans. Localization methods can greatly beneﬁt from a system capable of reasoning about a variety of cues beyond low-level features, such as street signs, store names, building directories, room numbers, etc.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Raisi, Zobeir and Naiel, Mohamed A. and Younes, Georges and Wardell, Steven and Zelek, John S.},
	month = jun,
	year = {2021},
	pages = {3156--3165},
	file = {Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:/Users/johannesreichle/Zotero/storage/Y6FVVXIN/Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:application/pdf},
}

@article{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2021-11-05},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Carion et al_2020_End-to-End Object Detection with Transformers.pdf:/Users/johannesreichle/Zotero/storage/3RYU4T68/Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/GBXATMTN/2005.html:text/html},
}

@misc{noauthor_accepted_nodate,
	title = {Accepted {Papers}},
	url = {https://nips.cc/Conferences/2021/AcceptedPapersInitial},
	urldate = {2021-11-05},
	file = {Accepted Papers:/Users/johannesreichle/Zotero/storage/BQ5SUEPT/AcceptedPapersInitial.html:text/html},
}

@article{zhang_fast_2021,
	title = {Fast {Multi}-{Resolution} {Transformer} {Fine}-tuning for {Extreme} {Multi}-label {Text} {Classification}},
	url = {http://arxiv.org/abs/2110.00685},
	abstract = {Extreme multi-label text classification (XMC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown significant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the fine-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively fine-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes significantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51\% to 54\%.},
	urldate = {2021-11-06},
	journal = {arXiv:2110.00685 [cs, stat]},
	author = {Zhang, Jiong and Chang, Wei-cheng and Yu, Hsiang-fu and Dhillon, Inderjit S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.00685},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:/Users/johannesreichle/Zotero/storage/FXMXFQ9J/Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XNYYFCCC/2110.html:text/html},
}

@article{sheng_centripetaltext_2021,
	title = {{CentripetalText}: {An} {Efficient} {Text} {Instance} {Representation} for {Scene} {Text} {Detection}},
	shorttitle = {{CentripetalText}},
	url = {http://arxiv.org/abs/2107.05945},
	abstract = {Scene text detection remains a grand challenge due to the variation in text curvatures, orientations, and aspect ratios. One of the hardest problems in this task is how to represent text instances of arbitrary shapes. Although many methods have been proposed to model irregular texts in a flexible manner, most of them lose simplicity and robustness. Their complicated post-processings and the regression under Dirac delta distribution undermine the detection performance and the generalization ability. In this paper, we propose an efficient text instance representation named CentripetalText (CT), which decomposes text instances into the combination of text kernels and centripetal shifts. Specifically, we utilize the centripetal shifts to implement pixel aggregation, guiding the external text pixels to the internal text kernels. The relaxation operation is integrated into the dense regression for centripetal shifts, allowing the correct prediction in a range instead of a specific value. The convenient reconstruction of text contours and the tolerance of prediction errors in our method guarantee the high detection accuracy and the fast inference speed, respectively. Besides, we shrink our text detector into a proposal generation module, namely CentripetalText Proposal Network, replacing Segmentation Proposal Network in Mask TextSpotter v3 and producing more accurate proposals. To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets. For the task of scene text detection, our approach achieves superior or competitive performance compared to other existing methods, e.g., F-measure of 86.3\% at 40.0 FPS on Total-Text, F-measure of 86.1\% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text recognition, our method outperforms Mask TextSpotter v3 by 1.1\% on Total-Text.},
	urldate = {2021-11-06},
	journal = {arXiv:2107.05945 [cs]},
	author = {Sheng, Tao and Chen, Jie and Lian, Zhouhui},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.05945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {Sheng et al_2021_CentripetalText.pdf:/Users/johannesreichle/Zotero/storage/GAFQN94L/Sheng et al_2021_CentripetalText.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HZFSY2GG/2107.html:text/html},
}

@article{shen_you_2021,
	title = {You {Never} {Cluster} {Alone}},
	url = {http://arxiv.org/abs/2106.01908},
	abstract = {Recent advances in self-supervised learning with instance-level contrastive objectives facilitate unsupervised clustering. However, a standalone datum is not perceiving the context of the holistic cluster, and may undergo sub-optimal assignment. In this paper, we extend the mainstream contrastive learning paradigm to a cluster-level scheme, where all the data subjected to the same cluster contribute to a unified representation that encodes the context of each data group. Contrastive learning with this representation then rewards the assignment of each datum. To implement this vision, we propose twin-contrast clustering (TCC). We define a set of categorical variables as clustering assignment confidence, which links the instance-level learning track with the cluster-level one. On one hand, with the corresponding assignment variables being the weight, a weighted aggregation along the data points implements the set representation of a cluster. We further propose heuristic cluster augmentation equivalents to enable cluster-level contrastive learning. On the other hand, we derive the evidence lower-bound of the instance-level contrastive objective with the assignments. By reparametrizing the assignment variables, TCC is trained end-to-end, requiring no alternating steps. Extensive experiments show that TCC outperforms the state-of-the-art on challenging benchmarks.},
	urldate = {2021-11-06},
	journal = {arXiv:2106.01908 [cs]},
	author = {Shen, Yuming and Shen, Ziyi and Wang, Menghan and Qin, Jie and Torr, Philip H. S. and Shao, Ling},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01908},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021},
	file = {Shen et al_2021_You Never Cluster Alone.pdf:/Users/johannesreichle/Zotero/storage/SJ7NZPJZ/Shen et al_2021_You Never Cluster Alone.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/3JBHRY6C/2106.html:text/html},
}

@article{ma_arbitrary-oriented_2018,
	title = {Arbitrary-{Oriented} {Scene} {Text} {Detection} via {Rotation} {Proposals}},
	volume = {20},
	issn = {1941-0077},
	doi = {10.1109/TMM.2018.2818020},
	abstract = {This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks, which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.},
	number = {11},
	journal = {IEEE Transactions on Multimedia},
	author = {Ma, Jianqi and Shao, Weiyuan and Ye, Hao and Wang, Li and Wang, Hong and Zheng, Yingbin and Xue, Xiangyang},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Pipelines, Computer architecture, Task analysis, arbitrary oriented, Image edge detection, Microsoft Windows, Proposals, Robustness, rotation proposals, Scene text detection},
	pages = {3111--3122},
	file = {Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:/Users/johannesreichle/Zotero/storage/MUTNDE2W/Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/N8FNCBBL/8323240.html:text/html},
}

@misc{goswami_deeper_2018,
	title = {A deeper look at how {Faster}-{RCNN} works},
	url = {https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd},
	abstract = {Faster-RCNN is one of the most well known object detection neural networks [1,2]. It is also the basis for many derived networks for…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = jul,
	year = {2018},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/FCB2UPLV/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-11-06},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick_2015_Fast R-CNN.pdf:/Users/johannesreichle/Zotero/storage/WHTYMPGV/Girshick_2015_Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/X3L8V8HR/1504.html:text/html},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-11-06},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {Ren et al_2016_Faster R-CNN.pdf:/Users/johannesreichle/Zotero/storage/G6DDTA5M/Ren et al_2016_Faster R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/DRXQUEDA/1506.html:text/html},
}

@misc{goswami_comparison_2020,
	title = {Comparison of {Faster}-{RCNN} and {Detection} {Transformer} ({DETR})},
	url = {https://whatdhack.medium.com/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04},
	abstract = {Faster-RCNN is a well known network, arguably the gold standard, in object detection and segmentation. Detection Transformer ( DETR) on…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/RAFM6M6L/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04.html:text/html},
}

@misc{hwalsuklee_awesome-deep-text-detection-recognition_2021,
	title = {awesome-deep-text-detection-recognition},
	copyright = {Apache-2.0},
	url = {https://github.com/hwalsuklee/awesome-deep-text-detection-recognition},
	abstract = {A curated list of resources for text detection/recognition (optical character recognition ) with deep learning methods.},
	urldate = {2021-11-11},
	author = {hwalsuklee},
	month = nov,
	year = {2021},
	note = {original-date: 2017-11-30T05:43:19Z},
	keywords = {awesome-list, awesome-lists, deep-learning, ocr, ocr-detection, ocr-paper, ocr-paper-list, ocr-papers, ocr-recognition, text-detection, text-detection-recognition, text-recognition},
}

@article{mittal_deep_2020,
	title = {Deep learning-based object detection in low-altitude {UAV} datasets: {A} survey},
	volume = {104},
	shorttitle = {Deep learning-based object detection in low-altitude {UAV} datasets},
	doi = {10.1016/j.imavis.2020.104046},
	abstract = {Deep learning-based object detection solutions emerged from computer vision has captivated full attention in recent years. The growing UAV market trends and interest in potential applications such as surveillance, visual navigation, object detection, and sensors-based obstacle avoidance planning have been holding good promises in the area of deep learning. Object detection algorithms implemented in deep learning framework have rapidly became a method for processing of moving images captured from drones. The primary objective of the paper is to provide a comprehensive review of the state of the art deep learning based object detection algorithms and analyze recent contributions of these algorithms to low altitude UAV datasets. The core focus of the studies is low-altitude UAV datasets because relatively less contribution was seen in the literature when compared with standard or remote-sensing based datasets. The paper discusses the following algorithms: Faster RCNN, Cascade RCNN, R-FCN etc. into two-stage, YOLO and its variants, SSD, RetinaNet into one-stage and CornerNet, Objects as Point etc. under advanced stages in deep learning based detectors. Further, one-two and advanced stages of detectors are studied in detail focusing on low-altitude UAV datasets. The paper provides a broad summary of low altitude datasets along with their respective literature in detection algorithms for the potential use of researchers. Various research gaps and challenges for object detection and classification in UAV datasets that need to deal with for improving the performance are also listed.},
	journal = {Image and Vision Computing},
	author = {Mittal, Payal and Singh, Raman and Sharma, Akashdeep},
	month = dec,
	year = {2020},
	pages = {104046},
	file = {Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:/Users/johannesreichle/Zotero/storage/RHDJAA7Z/Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:application/pdf},
}

@misc{noauthor_one-stage_nodate,
	title = {One-stage object detection},
	url = {https://machinethink.net/blog/object-detection/},
	urldate = {2021-11-16},
	file = {One-stage object detection:/Users/johannesreichle/Zotero/storage/U2Q8RYF5/object-detection.html:text/html},
}

@article{saunders_layers_2012,
	title = {The {Layers} of {Research} {Design}},
	url = {https://www.academia.edu/4107831/The_Layers_of_Research_Design},
	abstract = {Within this article we use the metaphor of the “Research Onion” (Saunders et al., 2012: 128) to illustrate how these final elements (the core of the research onion) need to be considered in relation to other design elements (the outer layers of the},
	language = {en},
	urldate = {2021-11-17},
	journal = {Rapport},
	author = {Saunders, Mark N. K.},
	year = {2012},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/MKLS6E4Z/The_Layers_of_Research_Design.html:text/html},
}

@article{qiao_text_2021,
	title = {Text {Perceptron}: {Towards} {End}-to-{End} {Arbitrary}-{Shaped} {Text} {Spotting}},
	shorttitle = {Text {Perceptron}},
	url = {http://arxiv.org/abs/2002.06820},
	abstract = {Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text.},
	urldate = {2021-11-17},
	journal = {arXiv:2002.06820 [cs]},
	author = {Qiao, Liang and Tang, Sanli and Cheng, Zhanzhan and Xu, Yunlu and Niu, Yi and Pu, Shiliang and Wu, Fei},
	month = oct,
	year = {2021},
	note = {arXiv: 2002.06820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI2020. Code is available at https://davar-lab.github.io/publication.html or https://github.com/hikopensource/DAVAR-Lab-OCR},
	file = {Qiao et al_2021_Text Perceptron.pdf:/Users/johannesreichle/Zotero/storage/GE89LJ5N/Qiao et al_2021_Text Perceptron.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/SA69IXXA/2002.html:text/html},
}

@article{torraco_writing_2005,
	title = {Writing {Integrative} {Literature} {Reviews}: {Guidelines} and {Examples}},
	volume = {4},
	issn = {1534-4843},
	shorttitle = {Writing {Integrative} {Literature} {Reviews}},
	url = {https://doi.org/10.1177/1534484305278283},
	doi = {10.1177/1534484305278283},
	abstract = {The integrative literature review is a distinctive form of research that generates new knowledge about the topic reviewed. Little guidance is available on how to write an integrative literature review. This article discusses how to organize and write an integrative literature review and cites examples of published integrative literature reviews that illustrate how this type of research has made substantive contributions to the knowledge base of human resource development.},
	language = {en},
	number = {3},
	urldate = {2021-11-18},
	journal = {Human Resource Development Review},
	author = {Torraco, Richard J.},
	month = sep,
	year = {2005},
	note = {Publisher: SAGE Publications},
	keywords = {integrative literature review, integrative research review: synthesis, literature review},
	pages = {356--367},
	annote = {Extracted Annotations (12/12/2021, 13:44:15)
"TABLE 2: A Checklist for Writing an Integrative Literature Review" (Torraco 2005:365)},
	file = {Torraco_2005_Writing Integrative Literature Reviews.pdf:/Users/johannesreichle/Zotero/storage/49PU84KP/Torraco_2005_Writing Integrative Literature Reviews.pdf:application/pdf},
}

@inproceedings{ye_textfusenet_2020,
	address = {Yokohama, Japan},
	title = {{TextFuseNet}: {Scene} {Text} {Detection} with {Richer} {Fused} {Features}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {{TextFuseNet}},
	url = {https://www.ijcai.org/proceedings/2020/72},
	doi = {10.24963/ijcai.2020/72},
	abstract = {Arbitrary shape text detection in natural scenes is an extremely challenging task. Unlike existing text detection approaches that only perceive texts based on limited feature representations, we propose a novel framework, namely TextFuseNet, to exploit the use of richer features fused for text detection. More speciﬁcally, we propose to perceive texts from three levels of feature representations, i.e., character-, word- and global-level, and then introduce a novel text representation fusion technique to help achieve robust arbitrary text detection. The multi-level feature representation can adequately describe texts by dissecting them into individual characters while still maintaining their general semantics. TextFuseNet then collects and merges the texts’ features from different levels using a multi-path fusion architecture which can effectively align and fuse different representations. In practice, our proposed TextFuseNet can learn a more adequate description of arbitrary shapes texts, suppressing false positives and producing more accurate detection results. Our proposed framework can also be trained with weak supervision for those datasets that lack character-level annotations. Experiments on several datasets show that the proposed TextFuseNet achieves state-of-the-art performance. Speciﬁcally, we achieve an F-measure of 94.3\% on ICDAR2013, 92.1\% on ICDAR2015, 87.1\% on Total-Text and 86.6\% on CTW-1500, respectively.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ye, Jian and Chen, Zhe and Liu, Juhua and Du, Bo},
	month = jul,
	year = {2020},
	pages = {516--522},
	file = {Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:/Users/johannesreichle/Zotero/storage/DDEV4B5A/Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:application/pdf},
}

@inproceedings{chen_tinynet_2019,
	address = {Beijing, China},
	title = {{TinyNet}: {A} {Lightweight}, {Modular}, and {Unified} {Network} {Architecture} for {The} {Internet} of {Things}},
	isbn = {978-1-4503-6886-5},
	shorttitle = {{TinyNet}},
	url = {http://dl.acm.org/citation.cfm?doid=3342280.3342290},
	doi = {10.1145/3342280.3342290},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2019 {Conference} {Posters} and {Demos} on   - {SIGCOMM} {Posters} and {Demos} '19},
	publisher = {ACM Press},
	author = {Chen, Gonglong and Wang, Yihui and Li, Huikang and Dong, Wei},
	year = {2019},
	pages = {9--11},
	file = {Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:/Users/johannesreichle/Zotero/storage/93RHUC63/Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:application/pdf},
}

@article{wang_pyramid_2021,
	title = {Pyramid {Vision} {Transformer}: {A} {Versatile} {Backbone} for {Dense} {Prediction} without {Convolutions}},
	shorttitle = {Pyramid {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2102.12122},
	abstract = {Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer{\textasciitilde}(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.},
	urldate = {2021-11-23},
	journal = {arXiv:2102.12122 [cs]},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	month = aug,
	year = {2021},
	note = {arXiv: 2102.12122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICCV 2021},
	file = {Wang et al_2021_Pyramid Vision Transformer.pdf:/Users/johannesreichle/Zotero/storage/BV7XAMYM/Wang et al_2021_Pyramid Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8AR4JD76/2102.html:text/html},
}

@inproceedings{benali_amjoud_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Neural} {Networks} {Backbones} for {Object} {Detection}},
	isbn = {978-3-030-51935-3},
	doi = {10.1007/978-3-030-51935-3_30},
	abstract = {Detecting objects in images is an extremely important step in many image and video analysis applications. Object detection is considered as one of the main challenges in the field of computer vision, which focuses on identifying and locating objects of different classes in an image. In this paper, we aim to highlight the important role of deep learning and convolutional neural networks in particular in the object detection task. We analyze and focus on the various state-of-the-art convolutional neural networks serving as a backbone in object detection models. We test and evaluate them in the common datasets and benchmarks up-to-date. We Also outline the main features of each architecture. We demonstrate that the application of some convolutional neural network architectures has yielded very promising state-of-the-art results in image classification in the first place and then in the object detection task. The results have surpassed all the traditional methods, and in some cases, outperformed the human being’s performance.},
	language = {en},
	booktitle = {Image and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Benali Amjoud, Ayoub and Amrouch, Mustapha},
	editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
	year = {2020},
	keywords = {Object detection, Convolutional neural networks, Review},
	pages = {282--289},
	file = {Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:/Users/johannesreichle/Zotero/storage/EHNQ56PL/Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:application/pdf},
}

@misc{noauthor_new_nodate,
	title = {New mobile neural network architectures},
	url = {https://machinethink.net/blog/mobile-architectures/},
	urldate = {2021-11-23},
	file = {New mobile neural network architectures:/Users/johannesreichle/Zotero/storage/832WZH9H/mobile-architectures.html:text/html},
}

@inproceedings{vogelsang_requirements_2019,
	title = {Requirements {Engineering} for {Machine} {Learning}: {Perspectives} from {Data} {Scientists}},
	shorttitle = {Requirements {Engineering} for {Machine} {Learning}},
	doi = {10.1109/REW.2019.00050},
	abstract = {Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Vogelsang, Andreas and Borg, Markus},
	month = sep,
	year = {2019},
	keywords = {machine learning, Adaptation models, Analytical models, Business, data science, Encoding, interview study, Interviews, requirements engineering, Requirements engineering, Synthetic aperture sonar},
	pages = {245--251},
	annote = {Extracted Annotations (15/12/2021, 16:57:40)"characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems." (Vogelsang and Borg 2019:245)"We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process." (Vogelsang and Borg 2019:245)"In addition, a recent survey suggests that Requirements Engineering (RE) is the most difficult activity for the development of ML-based systems [2]." (Vogelsang and Borg 2019:245)"In a recent survey, Ishikawa and Yoshioka [2] reported that RE was listed as the most difficult activity for the development of ML-based systems: "The dominant concerns [. . . ] pertain to decision making with the customers. In the conventional setting, this activity involved requirements analysis and specification in the initial phase and an acceptance inspection in the final phase. This activity flow is not possible when working with ML-based systems due to the impossibility of prior estimation or assurance of achievable accuracy."" (Vogelsang and Borg 2019:246)"She argues that there is no unified collection or consideration of many NFRs for ML, including a consideration of ML-specific quality trade-off data." (Vogelsang and Borg 2019:246)"stressed the importance of quality targets for ML models: "For a successful project, the requirements must be clear. Especially the evaluation metric must be specified". In ML systems, the quality of the resulting predictions can be considered a functional requirement (P3: "I consider predictive power as functional requirement."" (Vogelsang and Borg 2019:247)"Quantification of quality targets is certainly also a challenge for conventional software [22], but training an ML model to go beyond a certain utility breakpoint turns into a functional requirement in practice." (Vogelsang and Borg 2019:247)""Explainability is twofold: On the one side, there is a need to explain the model (what has been learned). On the other side, there is a need to explain single predictions of the model."" (Vogelsang and Borg 2019:248)""If there is a combination or transformation of features that is smaller but has a similar performance, we prefer that. [. . . ] We try to minimize the number of features to make the model more explainable."" (Vogelsang and Borg 2019:248)"ML systems are designed to discriminate. ML algorithms identify recurring patterns in data (i.e., stereotypes) and apply these patterns to judge about unseen data." (Vogelsang and Borg 2019:248)"An example is how the General Data Protection Regulation (GDPR) constrains that personal data can only be used in ways specified by an explicit consent." (Vogelsang and Borg 2019:248)"Training data is an integral part of any ML system. We envision that requirements for (training) data play a larger role for specifying ML systems than for conventional systems. We may even have data requirements as a new class of requirements." (Vogelsang and Borg 2019:248)"Based on our interviews, we would add "training data needs specified and validated requirements like code"." (Vogelsang and Borg 2019:249)""You could try, but it won't help" - the data is what it is, and it is up to the data scientist to make the most of it." (Vogelsang and Borg 2019:249)"That means, requirements on data quantity should not be specified on the number of examples but rather on the diversity of the examples" (Vogelsang and Borg 2019:249)"In some regulated domains, there are constraints on the amount of data necessary to tackle some problems. P4: "If we work on models to predict the likelihood of loan losses, we are forced to consider data from at least 5 years."" (Vogelsang and Borg 2019:249)"The higher the quality of the data, the better the application will work. That's why the process before the training is important: how I clean and augment the data."" (Vogelsang and Borg 2019:249)""There are many dimensions of data quality. [. . . ] For me, the most important ones are completeness, consistency, and correctness". Completeness refers to the sparsity of data within each characteristic (i.e., does the data cover the whole range of possible values). Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being true. Correctness is strongly influenced by the way how the data was collected." (Vogelsang and Borg 2019:249)},
	annote = {Extracted Annotations (23/11/2021, 16:07:51)"characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems." (Vogelsang and Borg 2019:245)"We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process." (Vogelsang and Borg 2019:245)"In addition, a recent survey suggests that Requirements Engineering (RE) is the most difficult activity for the development of ML-based systems [2]." (Vogelsang and Borg 2019:245)"In a recent survey, Ishikawa and Yoshioka [2] reported that RE was listed as the most difficult activity for the development of ML-based systems: "The dominant concerns [. . . ] pertain to decision making with the customers. In the conventional setting, this activity involved requirements analysis and specification in the initial phase and an acceptance inspection in the final phase. This activity flow is not possible when working with ML-based systems due to the impossibility of prior estimation or assurance of achievable accuracy."" (Vogelsang and Borg 2019:246)"She argues that there is no unified collection or consideration of many NFRs for ML, including a consideration of ML-specific quality trade-off data." (Vogelsang and Borg 2019:246)"In ML systems, the quality of the resulting predictions can be considered a functional requirement" (Vogelsang and Borg 2019:247)"Quantification of quality targets is certainly also a challenge for conventional software [22], but training an ML model to go beyond a certain utility breakpoint turns into a functional requirement in practice." (Vogelsang and Borg 2019:247)""Explainability is twofold: On the one side, there is a need to explain the model (what has been learned). On the other side, there is a need to explain single predictions of the model."" (Vogelsang and Borg 2019:248)""If there is a combination or transformation of features that is smaller but has a similar performance, we prefer that. [. . . ] We try to minimize the number of features to make the model more explainable."" (Vogelsang and Borg 2019:248)"Training data is an integral part of any ML system. We envision that requirements for (training) data play a larger role for specifying ML systems than for conventional systems." (Vogelsang and Borg 2019:248)"Based on our interviews, we would add "training data needs specified and validated requirements like code"." (Vogelsang and Borg 2019:249)""You could try, but it won't help" - the data is what it is, and it is up to the data scientist to make the most of it." (Vogelsang and Borg 2019:249)"In some regulated domains, there are constraints on the amount of data necessary to tackle some problems. P4: "If we work on models to predict the likelihood of loan losses, we are forced to consider data from at least 5 years."" (Vogelsang and Borg 2019:249)"The higher the quality of the data, the better the application will work. That's why the process before the training is important: how I clean and augment the data."" (Vogelsang and Borg 2019:249)""There are many dimensions of data quality. [. . . ] For me, the most important ones are completeness, consistency, and correctness". Completeness refers to the sparsity of data within each characteristic (i.e., does the data cover the whole range of possible values). Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being true. Correctness is strongly influenced by the way how the data was collected." (Vogelsang and Borg 2019:249)},
	file = {Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:/Users/johannesreichle/Zotero/storage/GSZJZHNW/Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/QT2UUEQG/8933800.html:text/html},
}

@inproceedings{arpteg_software_2018,
	title = {Software {Engineering} {Challenges} of {Deep} {Learning}},
	doi = {10.1109/SEAA.2018.00018},
	abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
	booktitle = {2018 44th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Arpteg, Anders and Brinne, Björn and Crnkovic-Friis, Luka and Bosch, Jan},
	month = aug,
	year = {2018},
	keywords = {Big Data, deep learning, machine learning, Machine learning, artificial intelligence, Buildings, Companies, Google, Oils, Software engineering, software engineering challenges},
	pages = {50--59},
	annote = {Extracted Annotations (23/11/2021, 14:37:19)
"A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges." (Arpteg et al 2018:50)
"Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of highquality systems." (Arpteg et al 2018:50)
"One of the main differences from traditional machine learning (ML) methods is that DL automatically learns how to represent data using multiple layers of abstraction [5], [6]. In traditional ML, a significant amount of work has to be spent on "feature engineering" to build this representation manually, but this process can now be automated to a higher degree. Having an automated and data-driven method for learning how to represent data improves both the performance of the model and reduces requirements for manual feature engineering work [7], [8]." (Arpteg et al 2018:50)
"A key difference between ML systems and non-ML systems is that data partly replaces code in a ML system, and a learning algorithm is used to automatically identify patterns in the data instead of writing hard coded rules." (Arpteg et al 2018:50)
"ML systems not only experience code-level debts but also dependencies related to changes to the external world. Data dependencies have been found to build similar debt as code dependencies." (Arpteg et al 2018:51)
"Deep Learning also makes it possible to compose complex models from a set of sub models and potentially reuse pretrained parameters with so called "transfer learning" techniques." (Arpteg et al 2018:51)
"It is not uncommon for pipelines to change, add and remove fields, or become deprecated. Keeping a deployed production-ready ML system up to date with all these changes require a significant amount of work, and requires supporting monitoring and logging systems to be able to detect when these changes occur." (Arpteg et al 2018:51)
"This section presents a list of concisely described challenges in the intersection between ML and SE. They have been grouped into three categories: development, production, and organizational challenges." (Arpteg et al 2018:53)
"With the addition of data dependencies and a high degree of configuration parameters, it can be very challenging to properly maintain ML systems in the long run. Also, it is not uncommon to perform hyperparameter tuning of models, potentially by making use of automated meta-optimization methods that generate hundreds of versions of the same data and model but with different configuration parameters [35]. Deep learning can also add the requirement of specific hardware." (Arpteg et al 2018:53)
"The great advances that have been made in fields such as computer vision and speech recognition, have been accomplished by replacing a modular processing pipeline with large neural networks that are trained end-to-end [37]. In essence, transparency is traded for accuracy. This is an unavoidable reality." (Arpteg et al 2018:53)
"A major challenge in developing DL systems is the difficulties in estimating the results before a system has been trained and tested." (Arpteg et al 2018:53)
"1) Experiment Management: During the development of ML models, a large number of experiments are usually performed to identify the optimal model. Each experiment can differ from other experiments in a number of ways and it is important to ensure reproducible results for these experiments. To have reproducible results, it may be necessary to know the exact version of components such as: 1) Hardware (e.g. GPU models primarily) 2) Platform (e.g. operating system and installed packages) 3) Source code (e.g. model training and pre-processing) 4) Configuration (e.g. model configuration and preprocessing settings) 5) Training data (e.g. input signals and target values) 6) Model state (e.g. versions of trained models)." (Arpteg et al 2018:53)
"Resource Limitations: Working with data that require distributed system adds another magnitude of complexity compared to single machine solutions. It is not only the volume of the data that may require a distributed solution, but also computational needs for extracting and transforming data, training and evaluating the model, and/or serving the model in production." (Arpteg et al 2018:54)
"It is challenging to provide a sample that includes all the edge cases that may exist in the full dataset. Also, as the external world is dynamic and changes over time, new edge cases will continue to appear later in time." (Arpteg et al 2018:54)
"There can be many reasons for the need of special techniques to handle resource limitations such as a lack of memory (CPU or GPU), long training time, or low-latency serving needs." (Arpteg et al 2018:56)},
	file = {Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/79S64LYS/Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/WKSGQ2B7/8498185.html:text/html},
}

@article{okoli_guide_2010,
	title = {A guide to conducting a systematic literature review of information systems research},
	author = {Okoli, Chitu and Schabram, Kira},
	year = {2010},
	file = {Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:/Users/johannesreichle/Zotero/storage/VHAVNWH2/Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:application/pdf},
}

@article{snyder_literature_2019,
	title = {Literature review as a research methodology: {An} overview and guidelines},
	volume = {104},
	issn = {0148-2963},
	shorttitle = {Literature review as a research methodology},
	url = {http://www.sciencedirect.com/science/article/pii/S0148296319304564},
	doi = {10.1016/j.jbusres.2019.07.039},
	abstract = {Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.},
	language = {en},
	urldate = {2020-04-06},
	journal = {Journal of Business Research},
	author = {Snyder, Hannah},
	month = nov,
	year = {2019},
	keywords = {Integrative review, Literature review, Research methodology, Synthesis, Systematic review},
	pages = {333--339},
	annote = {Extracted Annotations (23/11/2021, 16:18:00)
"A literature review can broadly be described as a more or less systematic way of collecting and synthesizing previous research (Baumeister \& Leary, 1997; Tranfield, Denyer, \& Smart, 2003)." (Snyder 2019:333)
"By integratingfindings and perspectives from many empiricalfindings, a literature review can address research questions with a power that no single study has." (Snyder 2019:333)
"In addition, a literature review is an excellent way of synthesizing researchfindings to show evidence on a meta-level and to uncover areas in which more research is needed, which is a critical component of creating theoretical frameworks and building conceptual models" (Snyder 2019:333)
"he paper has several contributions. First, this paper separates between different types of review methodologies; systematic," (Snyder 2019:333)
"semi-systematic and integrative approaches and argues that depending on purpose and the quality of execution, each type of approach can be very effective. While systematic reviews have strict requirements for search strategy and selecting articles for inclusion in the review, they are effective in synthesizing what the collection of studies are showing in a particular question and can provide evidence of effect that can inform policy and practice." (Snyder 2019:334)
"Instead, a semi-systematic review approach could be a good strategy for example map theoretical approaches or themes as well as identifying knowledge gaps within the literature. In some cases, a research question requires a more creative collection of data, in these cases; an integrative review approach can be useful when the purpose of the review is not to cover all articles ever published on the topic but rather to combine perspectives to create new theoretical models." (Snyder 2019:334)
"mature topics, the purpose of using an integrative review method is to overview the knowledge base, to critically review and potentially reconceptualize, and to expand on the theoretical foundation of the specific topic as it develops." (Snyder 2019:336)
"This includes selecting search terms and appropriate databases and deciding on inclusion and exclusion criteria." (Snyder 2019:337)
"does it make a substantial, practical, or theoretical contribution?early stated and motivated?" (Snyder 2019:338)},
	file = {Snyder_2019_Literature review as a research methodology.pdf:/Users/johannesreichle/Zotero/storage/HJIZLCQU/Snyder_2019_Literature review as a research methodology.pdf:application/pdf;ScienceDirect Snapshot:/Users/johannesreichle/Zotero/storage/YBMI7WLU/S0148296319304564.html:text/html},
}

@article{webster_analyzing_2002,
	title = {Analyzing the past to prepare for the future: {Writing} a literature review},
	shorttitle = {Analyzing the past to prepare for the future},
	journal = {MIS quarterly},
	author = {Webster, Jane and Watson, Richard T.},
	year = {2002},
	note = {Publisher: JSTOR},
	pages = {xiii--xxiii},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/T9JG4FKL/4132319.html:text/html;Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:/Users/johannesreichle/Zotero/storage/76JDULLA/Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:application/pdf},
}

@book{zowghi_requirements_2014,
	address = {Berlin, Heidelberg},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Requirements {Engineering}},
	volume = {432},
	isbn = {978-3-662-43609-7 978-3-662-43610-3},
	url = {http://link.springer.com/10.1007/978-3-662-43610-3},
	language = {en},
	urldate = {2021-11-23},
	publisher = {Springer Berlin Heidelberg},
	editor = {Zowghi, Didar and Jin, Zhi and Junqueira Barbosa, Simone Diniz and Chen, Phoebe and Cuzzocrea, Alfredo and Du, Xiaoyong and Filipe, Joaquim and Kara, Orhun and Kotenko, Igor and Sivalingam, Krishna M. and Ślęzak, Dominik and Washio, Takashi and Yang, Xiaokang},
	year = {2014},
	doi = {10.1007/978-3-662-43610-3},
	annote = {Extracted Annotations (24/11/2021, 10:50:14)
"The primary focus of requirements engineering has been functional requirements, which specify functions that a system or system component must deliver to users [3]. Increasingly, both researchers and practitioners realized that there exist many other requirements that play important role in shaping the target system, defining the development process, and managing the development project [4]. Non-Functional Requirements (NFR), as an umbrella term, then was coined to name these requirements [5]." (Zowghi and Jin 2014:10)},
	file = {Zowghi and Jin - 2014 - Requirements Engineering.pdf:/Users/johannesreichle/Zotero/storage/RTD5ZNET/Zowghi and Jin - 2014 - Requirements Engineering.pdf:application/pdf},
}

@article{noauthor_ieee_1998,
	title = {{IEEE} {Standard} for a {Software} {Quality} {Metrics} {Methodology}},
	doi = {10.1109/IEEESTD.1998.243394},
	abstract = {A methodology for establishing quality requirements and identifying, implementing, analyzing and validating the process and product software quality metrics is defined. The methodology spans the entire software life-cycle.},
	journal = {IEEE Std 1061-1998},
	month = dec,
	year = {1998},
	note = {Conference Name: IEEE Std 1061-1998},
	keywords = {IEEE standards},
	pages = {i--},
	file = {1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:/Users/johannesreichle/Zotero/storage/NU3DCQRT/1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/7Z7IEYIP/749159.html:text/html},
}

@book{kotonya_requirements_1998,
	edition = {1st},
	title = {Requirements {Engineering}: {Processes} and {Techniques}},
	isbn = {978-0-471-97208-2},
	shorttitle = {Requirements {Engineering}},
	abstract = {Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws},
	publisher = {Wiley Publishing},
	author = {Kotonya, Gerald and Sommerville, Ian},
	year = {1998},
}

@incollection{chung_non-functional_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Non}-{Functional} {Requirements} in {Software} {Engineering}},
	isbn = {978-3-642-02463-4},
	url = {https://doi.org/10.1007/978-3-642-02463-4_19},
	abstract = {Essentially a software system’s utility is determined by both its functionality and its non-functional characteristics, such as usability, flexibility, performance, interoperability and security. Nonetheless, there has been a lop-sided emphasis in the functionality of the software, even though the functionality is not useful or usable without the necessary non-functional characteristics. In this chapter, we review the state of the art on the treatment of non-functional requirements (hereafter, NFRs), while providing some prospects for future directions.},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Conceptual {Modeling}: {Foundations} and {Applications}: {Essays} in {Honor} of {John} {Mylopoulos}},
	publisher = {Springer},
	author = {Chung, Lawrence and do Prado Leite, Julio Cesar Sampaio},
	editor = {Borgida, Alexander T. and Chaudhri, Vinay K. and Giorgini, Paolo and Yu, Eric S.},
	year = {2009},
	doi = {10.1007/978-3-642-02463-4_19},
	keywords = {requirements engineering, alternatives, goal-oriented requirements engineering, NFRs, Non-functional requirements, satisficing, selection criteria, softgoals},
	pages = {363--379},
	file = {Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:/Users/johannesreichle/Zotero/storage/GB3VDQR4/Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:application/pdf},
}

@article{niu_26ms_2019,
	title = {26ms {Inference} {Time} for {ResNet}-50: {Towards} {Real}-{Time} {Execution} of all {DNNs} on {Smartphone}},
	shorttitle = {26ms {Inference} {Time} for {ResNet}-50},
	url = {http://arxiv.org/abs/1905.00571},
	abstract = {With the rapid emergence of a spectrum of high-end mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed. This work presents CADNN, a programming framework to efficiently execute DNN on mobile devices with the help of advanced model compression (sparsity) and a set of thorough architecture-aware optimization. The evaluation result demonstrates that CADNN outperforms all the state-of-the-art dense DNN execution frameworks like TensorFlow Lite and TVM.},
	urldate = {2021-11-24},
	journal = {arXiv:1905.00571 [cs, stat]},
	author = {Niu, Wei and Ma, Xiaolong and Wang, Yanzhi and Ren, Bin},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00571},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Extracted Annotations (15/12/2021, 15:00:55)"With the rapid emergence of a spectrum of highend mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed." (Niu et al 2019:1)},
	file = {Niu et al_2019_26ms Inference Time for ResNet-50.pdf:/Users/johannesreichle/Zotero/storage/F7T9RZPR/Niu et al_2019_26ms Inference Time for ResNet-50.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/B2A2APAU/1905.html:text/html},
}

@article{nowruzi_how_2019,
	title = {How much real data do we actually need: {Analyzing} object detection performance using synthetic and real data},
	shorttitle = {How much real data do we actually need},
	url = {http://arxiv.org/abs/1907.07061},
	abstract = {In recent years, deep learning models have resulted in a huge amount of progress in various areas, including computer vision. By nature, the supervised training of deep models requires a large amount of data to be available. This ideal case is usually not tractable as the data annotation is a tremendously exhausting and costly task to perform. An alternative is to use synthetic data. In this paper, we take a comprehensive look into the effects of replacing real data with synthetic data. We further analyze the effects of having a limited amount of real data. We use multiple synthetic and real datasets along with a simulation tool to create large amounts of cheaply annotated synthetic data. We analyze the domain similarity of each of these datasets. We provide insights about designing a methodological procedure for training deep networks using these datasets.},
	urldate = {2021-11-25},
	journal = {arXiv:1907.07061 [cs]},
	author = {Nowruzi, Farzan Erlik and Kapoor, Prince and Kolhatkar, Dhanvin and Hassanat, Fahed Al and Laganiere, Robert and Rebut, Julien},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07061},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in International Conference on Machine Learning (ICML 2019) Workshop on AI for Autonomous Driving},
	annote = {Extracted Annotations (25/11/2021, 16:13:50)
"This is also ultimately valuable as the huge training session can be achieved independently from the smaller fine-tuning counterpart. It is shown that the photo-realism is not as important as the diversity of the data." (Nowruzi et al 2019:8)},
	annote = {Extracted Annotations (26/11/2021, 14:43:05)
"In recent years, deep learning models have resulted in a huge amount of progress in various areas, including computer vision. By nature, the supervised training of deep models requires a large amount of data to be available. This ideal case is usually not tractable as the data annotation is a tremendously exhausting and costly task to perform." (Nowruzi et al 2019:1)
"The necessity of large amounts of annotated data is a bottleneck in computer vision tasks." (Nowruzi et al 2019:2)
"This is also ultimately valuable as the huge training session can be achieved independently from the smaller fine-tuning counterpart. It is shown that the photo-realism is not as important as the diversity of the data." (Nowruzi et al 2019:8)},
	file = {Nowruzi et al_2019_How much real data do we actually need.pdf:/Users/johannesreichle/Zotero/storage/42P4ENVD/Nowruzi et al_2019_How much real data do we actually need.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/KKTNYECV/1907.html:text/html},
}

@inproceedings{ouyang_factors_2016,
	title = {Factors in {Finetuning} {Deep} {Model} for {Object} {Detection} {With} {Long}-{Tail} {Distribution}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.html},
	urldate = {2021-11-25},
	author = {Ouyang, Wanli and Wang, Xiaogang and Zhang, Cong and Yang, Xiaokang},
	year = {2016},
	pages = {864--873},
	annote = {Extracted Annotations (25/11/2021, 16:13:45)"We find that it is better to have the number of samples uniform across different classes for feature learning." (Ouyang et al 2016:871)},
	annote = {Extracted Annotations (26/11/2021, 14:30:56)
"Finetuning from a pretrained deep model is found to yield state-of-the-art performance for many vision tasks." (Ouyang et al 2016:864)
"Analysis and experimental investigation on the factors that influence the effectiveness of finetuning. The investigated factors include the influence of the pretraining and finetuning on different layers of the deep model, the influence of the long tail, the influence of the training sample number, the effectiveness of different subsets of object classes, and the influence from the subset of training data." (Ouyang et al 2016:865)
"We find that it is better to have the number of samples uniform across different classes for feature learning." (Ouyang et al 2016:871)},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/LBVJEWSZ/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.html:text/html;Ouyang et al_2016_Factors in Finetuning Deep Model for Object Detection With Long-Tail.pdf:/Users/johannesreichle/Zotero/storage/9PFLFFAE/Ouyang et al_2016_Factors in Finetuning Deep Model for Object Detection With Long-Tail.pdf:application/pdf},
}

@inproceedings{namysl_efficient_2019,
	title = {Efficient, {Lexicon}-{Free} {OCR} using {Deep} {Learning}},
	doi = {10.1109/ICDAR.2019.00055},
	abstract = {Contrary to popular belief, Optical Character Recognition (OCR) remains a challenging problem when text occurs in unconstrained environments, like natural scenes, due to geometrical distortions, complex backgrounds, and diverse fonts. In this paper, we present a segmentation-free OCR system that combines deep learning methods, synthetic training data generation, and data augmentation techniques. We render synthetic training data using large text corpora and over 2000 fonts. To simulate text occurring in complex natural scenes, we augment extracted samples with geometric distortions and with a proposed data augmentation technique - alpha-compositing with background textures. Our models employ a convolutional neural network encoder to extract features from text images. Inspired by the recent progress in neural machine translation and language modeling, we examine the capabilities of both recurrent and convolutional neural networks in modeling the interactions between input elements. The proposed OCR system surpasses the accuracy of leading commercial and open-source engines on distorted text samples.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Namysl, Marcin and Konya, Iuliu},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Optical character recognition software, Text recognition, CNN, Feature extraction, Training, Character recognition, CTC, Hidden Markov models, Image segmentation, LSTM, OCR, synthetic data},
	pages = {295--301},
	file = {Namysl_Konya_2019_Efficient, Lexicon-Free OCR using Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/MMR42C2C/Namysl_Konya_2019_Efficient, Lexicon-Free OCR using Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/WE6PW565/8977969.html:text/html},
}

@article{el_bahi_text_2019,
	title = {Text recognition in document images obtained by a smartphone based on deep convolutional and recurrent neural network},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-07855-z},
	doi = {10.1007/s11042-019-07855-z},
	abstract = {Automatic text recognition in document images is an important task in many real-world applications. Several systems have been proposed to accomplish this task. However, a little attention has been given to document images obtained by mobile phones. To meet this need, we propose a new system that integrates preprocessing, features extraction and classification in order to recognize text contained in the document images acquired by a smartphone. The preprocessing phase is applied to locate the text region, and then segment that region into text line images. In the second phase, a sliding window divides the text-line image into a sequence of frames; afterwards a deep convolutional neural network (CNN) model is used to extract features from each frame. Finally, an architecture that combines the bidirectional recurrent neural network (RNN), the gated recurrent units (GRU) block and the connectionist temporal classification (CTC) layer is explored to ensure the classification phase. The proposed system has been tested on the ICDAR2015 Smartphone document OCR dataset and the experimental results show that the proposed system is capable to achieve promising recognition rates.},
	language = {en},
	number = {18},
	urldate = {2021-11-26},
	journal = {Multimedia Tools and Applications},
	author = {El Bahi, Hassan and Zatni, Abdelkarim},
	month = sep,
	year = {2019},
	pages = {26453--26481},
	file = {El Bahi_Zatni_2019_Text recognition in document images obtained by a smartphone based on deep.pdf:/Users/johannesreichle/Zotero/storage/LH4DNH6K/El Bahi_Zatni_2019_Text recognition in document images obtained by a smartphone based on deep.pdf:application/pdf},
}

@inproceedings{sourvanos_challenges_2018,
	title = {Challenges in {Input} {Preprocessing} for {Mobile} {OCR} {Applications}: {A} {Realistic} {Testing} {Scenario}},
	shorttitle = {Challenges in {Input} {Preprocessing} for {Mobile} {OCR} {Applications}},
	doi = {10.1109/IISA.2018.8633688},
	abstract = {Applications of OCR range from automatic text input from images and videos and text-to-speech to automatic translation of textual information in real world scenes. Such wide applicability of OCR has promoted its use on mobile devices, such as smartphones. However, there are intrinsic complications when designing an end-to-end OCR system for smartphones, as there is a number of steps and challenges, inherent to OCR itself, which may prove arduous and inconvenient for a conventional mobile device to handle. For starters, before actual OCR, input needs to be preprocessed, in order to enhance the existing textual information and minimize noise and outliers. In this study, we attempt to document the challenges faced by a smartphone-enabled OCR application, experiment on the preprocessing steps using a realistic testing scenario and pose some open issues for future investigation.},
	booktitle = {2018 9th {International} {Conference} on {Information}, {Intelligence}, {Systems} and {Applications} ({IISA})},
	author = {Sourvanos, Nikolaos and Tsatiris, Georgios},
	month = jul,
	year = {2018},
	keywords = {Optical character recognition software, Testing, Text recognition, Image segmentation, Data preprocessing, Smart phones},
	pages = {1--5},
	file = {Sourvanos_Tsatiris_2018_Challenges in Input Preprocessing for Mobile OCR Applications.pdf:/Users/johannesreichle/Zotero/storage/SJUBEAAA/Sourvanos_Tsatiris_2018_Challenges in Input Preprocessing for Mobile OCR Applications.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/XT8CXNXF/8633688.html:text/html},
}

@inproceedings{ghosh_visual_2017,
	title = {Visual {Attention} {Models} for {Scene} {Text} {Recognition}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.158},
	abstract = {In this paper we propose an approach to lexicon-free recognition of text in scene images. Our approach relies on a LSTM-based soft visual attention model learned from convolutional features. A set of feature vectors are derived from an intermediate convolutional layer corresponding to different areas of the image. This permits encoding of spatial information into the image representation. In this way, the framework is able to learn how to selectively focus on different parts of the image. At every time step the recognizer emits one character using a weighted combination of the convolutional feature vectors according to the learned attention model. Training can be done end-to-end using only word level annotations. In addition, we show that modifying the beam search algorithm by integrating an explicit language model leads to significantly better recognition results. We validate the performance of our approach on standard SVT and ICDAR'03 scene text datasets, showing state-of-the-art performance in unconstrained text recognition.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Ghosh, Suman K. and Valveny, Ernest and Bagdanov, Andrew D.},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Text recognition, Computational modeling, Adaptation models, Character recognition, Decoding, Image recognition, Visualization},
	pages = {943--948},
	annote = {Extracted Annotations (26/11/2021, 13:20:38)
"However, robust reading of text in uncontrolled environments is very different from text recognition in document images and much more challenging due to multiple factors such as difficult acquisition conditions, low resolution, font variability, complex backgrounds, different lighting conditions, blur, etc." (Ghosh et al 2017:943)
"The problem of end-to-end scene text recognition is usually divided in two different tasks: word detection and word recognition." (Ghosh et al 2017:943)
"Existing word recognition methods can be broadly divided into dictionay-based methods, using some kind of predefined lexicon to guide the recognition, and unconstrained methods, able to recognize any word." (Ghosh et al 2017:943)
"Biassco et al.in [3] rely on sequential character classifiers. They use a massive number of annotated character bounding boxes to learn character classifiers. Binarization and sliding window methods are used to generate character proposals followed by a text/background classifier. Finally, character probabilities given by character classifiers are used in a beam search to recognize words. They also integrate a static character n-gram language model in every step of the beam search to incorporate an underlying language model." (Ghosh et al 2017:943)
"In contrast to the above strategies our approach neither recognizes individual characters in the word image nor uses any holistic representation to recognize the word. It rather uses a LSTM-based visual attention model on top of CNN features (based on [19]) to focus attention on relevant parts of the image at every step and infer a character present in the image (see figure 1)" (Ghosh et al 2017:944)
"The visual attention model can be trained using only word bounding boxes and does not need explicit character bounding boxes at training time." (Ghosh et al 2017:944)},
	file = {Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:/Users/johannesreichle/Zotero/storage/W7DQDRHK/Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/H5S99QNM/8270089.html:text/html},
}

@article{hu_gtc_2020,
	title = {{GTC}: {Guided} {Training} of {CTC} {Towards} {Efficient} and {Accurate} {Scene} {Text} {Recognition}},
	shorttitle = {{GTC}},
	url = {http://arxiv.org/abs/2002.01276},
	abstract = {Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance. With the benefit of guided training, CTC model achieves robust and accurate prediction for both regular and irregular scene text while maintaining a fast inference speed. Moreover, to further leverage the potential of CTC decoder, a graph convolutional network (GCN) is proposed to learn the local correlations of extracted features. Extensive experiments on standard benchmarks demonstrate that our end-to-end model achieves a new state-of-the-art for regular and irregular scene text recognition and needs 6 times shorter inference time than attentionbased methods.},
	urldate = {2021-11-30},
	journal = {arXiv:2002.01276 [cs, eess]},
	author = {Hu, Wenyang and Cai, Xiaocong and Hou, Jun and Yi, Shuai and Lin, Zhiping},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.01276},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	annote = {Comment: Accepted by AAAI 2020},
	annote = {Extracted Annotations (30/11/2021, 12:41:11)
"Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance" (Hu et al 2020:1)
"However, due to different sizes, fonts, colors and character placements of scene texts, scene text recognition is still a challenging task." (Hu et al 2020:1)},
	file = {Hu et al_2020_GTC.pdf:/Users/johannesreichle/Zotero/storage/NT5A62AM/Hu et al_2020_GTC.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/Y58C3PJB/2002.html:text/html},
}

@article{chen_text_2021,
	title = {Text {Recognition} in the {Wild}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Text {Recognition} in the {Wild}},
	url = {https://dl.acm.org/doi/10.1145/3440756},
	doi = {10.1145/3440756},
	abstract = {The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition.},
	language = {en},
	number = {2},
	urldate = {2021-11-30},
	journal = {ACM Computing Surveys},
	author = {Chen, Xiaoxue and Jin, Lianwen and Zhu, Yuanzhi and Luo, Canjie and Wang, Tianwei},
	month = apr,
	year = {2021},
	pages = {1--35},
	annote = {Extracted Annotations (06/01/2022, 15:59:08)"Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios." (Chen et al 2021:1)"In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency." (Chen et al 2021:1)"Specifically, rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios," (Chen et al 2021:1)"Recognizing text in natural scenes, also known as scene text recognition (STR), is usually considered as a special form of optical character recognition (OCR), that is, camera-based OCR. Although OCR in scanned documents is well developed [137, 241], STR remains challenging because of many factors, including complex backgrounds, various fonts, and imperfect imaging conditions. Figure 1 compares the following characteristics of STR and OCR in scanned documents. • Background. Unlike OCR in scanned documents, text in natural scenes can appear on anything (e.g., signboards, walls, or product packagings). Therefore, scene text images may contain very complex backgrounds. Moreover, the texture of the background can be visually similar to text, which causes additional challenges for recognition. • Form. Text in scanned documents is usually printed in a single color with regular font, consistent size, and uniform arrangement. Meanwhile, in natural scenes, text appears in multiple colors with irregular fonts, different sizes, and diverse orientations. The diversity of text thus makes STR more difficult than OCR in scanned documents. • Noise. Text in natural scenes is usually distorted by noise interference, such as nonuniform illumination, low resolution, and motion blurring. Imperfect imaging conditions cause failures in the STR. • Access. Scanned text is usually frontal and occupies the main part of an image. However, scene text is captured randomly, resulting in irregular deformations (such as perspective distortion). Furthermore, the various shapes of text increase the difficulty of recognizing characters and predicting text strings." (Chen et al 2021:2)"As illustrated in Figure 3, various fundamental problems have been defined at various stages of this task in the literature: text localization, text verification, text detection, text segmentation, text recognition, and end-to-end systems" (Chen et al 2021:4)"Text localization. The objective of text localization [102] is to localize text components precisely and group them into candidate text regions with as little background as possible" (Chen et al 2021:4)"Text verification. Text verification [89] aims at verifying text candidate regions as text or non-text." (Chen et al 2021:4)"Recent works [81, 196] have used a convolution neural network (CNN) to improve text/non-text discrimination." (Chen et al 2021:5)"Text detection. The function of text detection [201, 219] is to determine whether text is present using localization and verification procedures [222]." (Chen et al 2021:5)"Text segmentation. Text segmentation has been identified as one of the most challenging problems [184]. It includes text line segmentation [168, 223] and character segmentation [144, 167]. The former refers to splitting a region of multiple text lines into multiple subregions of single text lines. The latter refers to separating a text instance into multiple regions of single characters." (Chen et al 2021:5)"Compared with traditional methods, deep learning methods have the following advantages: (i) Automation: automatic feature representation learning can free researchers from empirically designing handcrafted features. (ii) Effectiveness: excellent recognition performance far exceeds traditional algorithms. (iii) Generalization: algorithms can be easily applied to similar vision-based problems." (Chen et al 2021:6)"The objective of STR is to translate a cropped text instance image into a target string sequence. There are two types of scene text in nature: regular and irregular." (Chen et al 2021:6)"Text Image Super-resolution (TextSR). Scene text is usually distorted by various noise interferences, such as low resolution." (Chen et al 2021:9)"Rectification. The function of rectification is to normalize the input text instance image, remove the distortion, and reduce the difficulty of irregular text recognition. Specifically, irregular text [217] refers to text with perspective distortion or an arbitrary curving shape, which usually causes additional challenges in recognition." (Chen et al 2021:9)"Image preprocessing includes but is not limited to the aforementioned types. It can significantly reduce the difficulties of recognition by improving image quality." (Chen et al 2021:10)"Feature Representation Stage Feature representation maps the input text instance image to a representation that reflects the attributes relevant for character recognition, while suppressing irrelevant features such as font, color, size, and background." (Chen et al 2021:10)"A deeper and more advanced feature extractor usually results in better representation power, which is suitable for improving STR with complex backgrounds. However, performance improvement costs increased memory and computation consumption" (Chen et al 2021:10)"Contextual cues are beneficial for image-based sequence recognition. Although recurrent neural network (RNN)-based [72] structures, such as BiLSTM or LSTM, can model character sequences, there are some inherent limitations. In contrast, CNNs or transformers [182] can not only effectively deal with long sequences but also be parallelized efficiently. Modeling language sequences using CNNs or transformer structures may be a new trend for sequence modeling because of its intrinsic superiority." (Chen et al 2021:11)"Given a text image with a complex background as input, an end-to-end system aims to directly convert all text regions into string sequences. Typically, this includes text detection, text recognition, and postprocessing." (Chen et al 2021:14)"Several factors promote the emergence of end-to-end systems: (i) Errors can accumulate in a cascade way of text detection and recognition, which may lead to a large fraction of garbage predictions, while an end-to-end system can prevent errors from being accumulated during the training. (ii) In an end-to-end system, text detection and recognition can share information and be jointly optimized to improve the overall performance. (iii) An end-to-end system is easier to maintain and adapt to new domains; whereas, maintaining a cascaded pipeline with data and model dependencies requires substantial engineering efforts. (iv) End-to-end systems exhibit competitive performance with faster inference and smaller storage requirements." (Chen et al 2021:14)"Although the current end-to-end systems work fairly well in many real-world scenarios, they have limitations. The following difficulties should be considered: (i) How can we efficiently bridge and share information between text detection and recognition? (ii) How can the significant differences in learning difficulty and convergence speed be balanced between text detection and recognition? (iii) How can joint optimization be improved? Moreover, a simple, compact, and powerful end-to-end system is yet to be developed." (Chen et al 2021:15)"In this section, we summarize the evaluation protocols for Latin and multilingual texts. 4.2.1 Evaluation Protocols for Latin Text. Recognition Protocols. The word recognition accuracy (WRA) and word error rate (WER) are two widely used recognition evaluation protocols for Latin text. • WRA. W RA is defined by W RA= Wr, (7) W whereW is the total number of words, andWr represents the number of correctly recognized words. • WER. W ER is defined by W ER = 1−W RA= 1− Wr. (8) W" (Chen et al 2021:21)"Most competitions [27, 67, 114] measured the algorithm recognition performance using a traditional evaluation metric, the normalized edit distance (NED):" (Chen et al 2021:22)"where D (.) stands for the Levenshtein distance. si and ˆi denote the predicted text and the corresponding ground truth, respectively. Furthermore, li and ˆi are their text lengths. N is the total number of text lines. The NED protocol measures mismatching between the predicted text and the corresponding ground truth. Therefore, the recognition score is usually calculated as 1-NED. End-to-end Protocols. Two main evaluation protocols for end-to-end systems have been used during recent competitions:" (Chen et al 2021:22)},
	file = {Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:/Users/johannesreichle/Zotero/storage/TVGZBTI2/Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:application/pdf},
}

@article{long_scene_2021,
	title = {Scene {Text} {Detection} and {Recognition}: {The} {Deep} {Learning} {Era}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Scene {Text} {Detection} and {Recognition}},
	url = {https://link.springer.com/10.1007/s11263-020-01369-0},
	doi = {10.1007/s11263-020-01369-0},
	abstract = {With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inevitably inﬂuenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, methodology and performance. This survey is aimed at summarizing and analyzing the major changes and signiﬁcant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Speciﬁcally, we will emphasize the dramatic differences brought by deep learning and remaining grand challenges. We expect that this review paper would serve as a reference book for researchers in this ﬁeld. Related resources are also collected in our Github repository (https://github.com/Jyouhou/SceneTextPapers).},
	language = {en},
	number = {1},
	urldate = {2021-11-30},
	journal = {International Journal of Computer Vision},
	author = {Long, Shangbang and He, Xin and Yao, Cong},
	month = jan,
	year = {2021},
	pages = {161--184},
	annote = {Extracted Annotations (27/12/2021, 16:06:39)
"computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution," (Long et al 2021:161)
"on the other hand, the rich and precise high-level semantics embodied in text could be beneficial for understanding the world around us." (Long et al 2021:161)
"Diversity and Variability of Text in Natural Scenes Distinctive from scripts in documents, text in natural scene exhibit much higher diversity and variability" (Long et al 2021:161)
"Complexity and Interference of Backgrounds The backgrounds of natural scenes are virtually unpredictable." (Long et al 2021:161)
"Imperfect Imaging Conditions In" (Long et al 2021:162)
"1) Most methods utilize deeplearning based models; (2) Most researchers are approaching the problem from a diversity of perspectives, trying to solve different challenges. Methods driven by deep learning enjoy the advantage that automatic feature learning can save us from designing and testing a large amount of potential handcrafted features." (Long et al 2021:163)
"c, d are simplified pipeline. In c, detectors and recognizers are separate. In d, the detectors pass cropped Illustrations of tion system pipelines. feature maps to recognizers, which allows end-to-end training" (Long et al 2021:164)
"(1) text detection that detects and localizes text in natural images; (2) recognition system that transcribes and converts the content of the detected text regions into linguistic symbols; (3) end-to-end system that performs both text detection and recognition in one unified pipeline; (4) auxiliary methods that aim to support the main task of text detection and recognition, e.g. synthetic data generation." (Long et al 2021:164)
"Overall, in this stage, scene text detection algorithms still havelongandslowpipelines,thoughtheyhavereplacedsome hand-crafted features with learning-based ones. The design methodology is bottom-up and based on key components, such as single characters and text center lines." (Long et al 2021:165)
"with curved, oriented, or long text for one-staged methods due to the limitation of the receptive field, and the efficiency is limited for two-staged methods." (Long et al 2021:166)
"Character-level representation is yet another effective way. Baek et al. (2019b) propose to learn a segmentation map for character centers and links between them. Both components and links are predicted in the form of a Gaussian heat map. However, this method requires iterative weak supervision as real-world datasets are rarely equipped with character-level labels." (Long et al 2021:167)},
	annote = {Extracted Annotations (27/12/2021, 16:20:58)
"computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution," (Long et al 2021:161)
"on the other hand, the rich and precise high-level semantics embodied in text could be beneficial for understanding the world around us." (Long et al 2021:161)
"Diversity and Variability of Text in Natural Scenes Distinctive from scripts in documents, text in natural scene exhibit much higher diversity and variability" (Long et al 2021:161)
"Complexity and Interference of Backgrounds The backgrounds of natural scenes are virtually unpredictable." (Long et al 2021:161)
"Imperfect Imaging Conditions In" (Long et al 2021:162)
"1) Most methods utilize deeplearning based models; (2) Most researchers are approaching the problem from a diversity of perspectives, trying to solve different challenges. Methods driven by deep learning enjoy the advantage that automatic feature learning can save us from designing and testing a large amount of potential handcrafted features." (Long et al 2021:163)
"c, d are simplified pipeline. In c, detectors and recognizers are separate. In d, the detectors pass cropped Illustrations of tion system pipelines. feature maps to recognizers, which allows end-to-end training" (Long et al 2021:164)
"(1) text detection that detects and localizes text in natural images; (2) recognition system that transcribes and converts the content of the detected text regions into linguistic symbols; (3) end-to-end system that performs both text detection and recognition in one unified pipeline; (4) auxiliary methods that aim to support the main task of text detection and recognition, e.g. synthetic data generation." (Long et al 2021:164)
"Overall, in this stage, scene text detection algorithms still havelongandslowpipelines,thoughtheyhavereplacedsome hand-crafted features with learning-based ones. The design methodology is bottom-up and based on key components, such as single characters and text center lines." (Long et al 2021:165)
"with curved, oriented, or long text for one-staged methods due to the limitation of the receptive field, and the efficiency is limited for two-staged methods." (Long et al 2021:166)
"Character-level representation is yet another effective way. Baek et al. (2019b) propose to learn a segmentation map for character centers and links between them. Both components and links are predicted in the form of a Gaussian heat map. However, this method requires iterative weak supervision as real-world datasets are rarely equipped with character-level labels." (Long et al 2021:167)
"Character level annotations are more accurate and better. However, most existing datasets do not provide characterlevel annotating. Since characters are smaller and close to each other, character-level annotation is more costly and inconvenient." (Long et al 2021:171)
"enchmark Datasets and Evaluation Protocol" (Long et al 2021:172)},
	file = {Long et al. - 2021 - Scene Text Detection and Recognition The Deep Lea.pdf:/Users/johannesreichle/Zotero/storage/HCWGWF4P/Long et al. - 2021 - Scene Text Detection and Recognition The Deep Lea.pdf:application/pdf},
}

@article{watanabe_preliminary_2019,
	title = {Preliminary {Systematic} {Literature} {Review} of {Machine} {Learning} {System} {Development} {Process}},
	url = {http://arxiv.org/abs/1910.05528},
	abstract = {Previous machine learning (ML) system development research suggests that emerging software quality attributes are a concern due to the probabilistic behavior of ML systems. Assuming that detailed development processes depend on individual developers and are not discussed in detail. To help developers to standardize their ML system development processes, we conduct a preliminary systematic literature review on ML system development processes. A search query of 2358 papers identified 7 papers as well as two other papers determined in an ad-hoc review. Our findings include emphasized phases in ML system developments, frequently described practices and tailored traditional software development practices.},
	urldate = {2021-12-01},
	journal = {arXiv:1910.05528 [cs]},
	author = {Watanabe, Yasuhiro and Washizaki, Hironori and Sakamoto, Kazunori and Saito, Daisuke and Honda, Kiyoshi and Tsuda, Naohiko and Fukazawa, Yoshiaki and Yoshioka, Nobukazu},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.05528},
	keywords = {Computer Science - Machine Learning},
	annote = {Good example for literature review},
	file = {Watanabe et al_2019_Preliminary Systematic Literature Review of Machine Learning System Development.pdf:/Users/johannesreichle/Zotero/storage/NEUNMXHG/Watanabe et al_2019_Preliminary Systematic Literature Review of Machine Learning System Development.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/4GVXIP5X/1910.html:text/html},
}

@inproceedings{hu_towards_2020,
	title = {Towards {Requirements} {Specification} for {Machine}-learned {Perception} {Based} on {Human} {Performance}},
	doi = {10.1109/AIRE51212.2020.00014},
	abstract = {The application of machine learning (ML) based perception algorithms in safety-critical systems such as autonomous vehicles have raised major safety concerns due to the apparent risks to human lives. Yet assuring the safety of such systems is a challenging task, in a large part because ML components (MLCs) rarely have clearly specified requirements. Instead, they learn their intended tasks from the training data. One of the most well-studied properties that ensure the safety of MLCs is the robustness against small changes in images. But the range of changes considered small has not been systematically defined. In this paper, we propose an approach for specifying and testing requirements for robustness based on human perception. With this approach, the MLCs are required to be robust to changes that fall within the range defined based on human perception performance studies. We demonstrate the approach on a state-of-the-art object detector.},
	booktitle = {2020 {IEEE} {Seventh} {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Hu, Boyue Caroline and Salay, Rick and Czarnecki, Krzysztof and Rahimi, Mona and Selim, Gehan and Chechik, Marsha},
	month = sep,
	year = {2020},
	keywords = {Safety, Testing, Task analysis, Robustness, Visualization, Gaussian noise, Measurement, n/a},
	pages = {48--51},
	annote = {Extracted Annotations (02/12/2021, 11:48:21)"In addition, it is often difficult to rigorously specify the tasks that MLCs are expected to perform." (Hu et al 2020:48)"Ashmore et al. identified a list of desired properties of MLCs that should be considered as requirements: perfor-" (Hu et al 2020:48)"mance, robustness, reusability and interpretability" (Hu et al 2020:48)"Therefore, requirements that ensure robustness are crucial to assure that decisions made by ML can be trusted in safety-critical contexts" (Hu et al 2020:48)"Position and contributions: Specifying full requirements of the expected behaviour for MLCs may not be feasible." (Hu et al 2020:48)"Human performance is used to bound the amount of changes that the MLCs are required to be robust to. We present a systematic method of generating such requirements and a method for testing whether the requirements have been satisfied. Our requirements can be used for verification and safety guarantees for MLCs in safety-critical systems." (Hu et al 2020:49)"For our purposes, we only consider modifications that can be formally defined as transformations. Investigating modifications in images that cannot be formally expressed as transformations, e.g., changing the clothes of a pedestrian, is left as future work. Some examples of transformations are: Affine transformations [10] such as scaling and rotating. Transformations modifying different aspects of the perceptual context [22]: - Light sources, e.g., changing the color or brightness of the light. - Medium, e.g., adding weather conditions like rain or fog. - Objects, e.g., changing position of the object. - Observer (camera), e.g., different viewpoint and exposure of the camera, different amount of visual noise." (Hu et al 2020:49)"As shown in Fig. 1, we further refine requirements for robustness as invariant and equivariant requirements. An MLC can have multiple outputs and different outputs may be required to be invariant or equivariant with respect to given a transformation of the input. For example, an object detector produces a class label and a bounding box position and extent for each object it detects in the input image. With respect to a translation transformation that moves objects, we require that bounding box position is equivariant and moves a corresponding amount, while the class and bounding box extent is invariant." (Hu et al 2020:49)},
	file = {Hu et al_2020_Towards Requirements Specification for Machine-learned Perception Based on.pdf:/Users/johannesreichle/Zotero/storage/CE5E9RB3/Hu et al_2020_Towards Requirements Specification for Machine-learned Perception Based on.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/67IEQCH4/9233007.html:text/html},
}

@article{siebert_construction_2021,
	title = {Construction of a quality model for machine learning systems},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/10.1007/s11219-021-09557-y},
	doi = {10.1007/s11219-021-09557-y},
	abstract = {Nowadays, systems containing components based on machine learning (ML) methods are becoming more widespread. In order to ensure the intended behavior of a software system, there are standards that define necessary qualities of the system and its components (such as ISO/IEC 25010). Due to the different nature of ML, we have to re-interpret existing qualities for ML systems or add new ones (such as trustworthiness). We have to be very precise about which quality property is relevant for which entity of interest (such as completeness of training data or correctness of trained model), and how to objectively evaluate adherence to quality requirements. In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively. In addition to the overall construction process described, the main outcomes include a meta-model for specifying quality models for ML systems, reference elements regarding relevant views, entities, quality properties, and measures for ML systems based on existing research, an example instantiation of a quality model for a concrete industrial use case, and lessons learned from applying the construction process. We found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice. In the future, we want to learn how the term quality differs between different types of ML systems and come up with reference quality models for evaluating qualities of ML systems.},
	language = {en},
	urldate = {2021-12-01},
	journal = {Software Quality Journal},
	author = {Siebert, Julien and Joeckel, Lisa and Heidrich, Jens and Trendowicz, Adam and Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio},
	month = jun,
	year = {2021},
	annote = {Extracted Annotations (02/12/2021, 13:52:05)
"In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively." (Siebert et al 2021:1)
"e found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice." (Siebert et al 2021:1)
"A data-driven software component is a piece of software that" (Siebert et al 2021:1)
"solves a given task (e.g., image segmentation, sentiment analysis, classification, etc.), using methods from data science, such as machine learning (ML), data mining, natural language processing, signal processing, statistics, etc. The functionality of data-driven software components (or at least part of it) is not entirely defined by the programmer in the classical way (by programming it directly), but is derived (i.e., learned) from data. At the core of a data-driven software component lies the notion of a model (or several models, coupled together in a pipeline)." (Siebert et al 2021:2)
"In short, the behavior of such components is first and foremost fundamentally different from traditional (i.e., not data-driven) software: the relationship between the input and the outcome of the software component is usually non-linear." (Siebert et al 2021:2)
"This input-output relationship is also only defined for a subset of the data, which leads to uncertainty in outcomes for previously unseen data" (Siebert et al 2021:2)
"Second, common development principles from software engineering, such as encapsulation and modularity, have to be rethought. For example, changing the application context of a data-driven component (its intended scope), the type of model used, or the internal parameters, usually implies retraining the component. This is also referred to as the CACE principle: changing anything changes everything (Sculley et al., 2015)." (Siebert et al 2021:2)
"For instance, ISO/ IEC 25010 (ISO/IEC, 2011) defines quality models for software and systems, i.e., a hierarchy of quality properties of interest and how to quantify and assess them. Due to the different nature of data-driven software components, these quality models cannot be applied directly as they are." (Siebert et al 2021:2)
"To develop meaningful quality models, it is necessary to understand the application context of the use case and what kind of data-driven method is used." (Siebert et al 2021:2)
"To build a quality model, it is first necessary to define the usage scenarios (i.e., How will the system be used? By whom? What are the expectations in terms of quality? etc.). This naturally goes hand in hand with the definition of the relevant quality properties and the entities to be measured. It is also necessary to define how to measure these properties and, finally, to decide on the basis of these measurements what to do to improve certain properties of quality." (Siebert et al 2021:3)
"Technical robustness, reliability, dependability (e.g., correctness of output, estimation of model uncertainty, robustness against harmful inputs, errors, or unexpected situations)" (Siebert et al 2021:4)
"A model is by definition a re-presentation (i.e., a simplification) of some part of reality (i.e., the system under study)." (Siebert et al 2021:4)
"ML components usually consist of several sub-components organized in pipelines: e.g., data preparation (e.g., resizing or cropping images, cleaning text), features engineering, training and evaluating the models." (Siebert et al 2021:5)
"In (Nakajima,  2018), the authors distinguish between three main qualities, namely service quality, product quality, and platform quality. They also describe different views/entities of the system: the training dataset, the neural network, the hyper parameters," (Siebert et al 2021:5)
"As a last example, the authors in Hamada et al. (2020) provide five main qualities related to views/entities, namely data integrity, model robustness, system quality, process agility, and customer expectation, including a total of 49 quality sub-properties." (Siebert et al 2021:5)
"For example, for classification tasks, the goodness of fit can be measured by accuracy, precision, recall, f-score, etc" (Siebert et al 2021:6)
"This section describes the process we followed to construct a quality model for ML systems based on our previous work in the field (Goeb et al., 2015). It consists of six steps, which we will describe sequentially, but which are performed iteratively in practice. An overview of all steps and illustrations of the major outcomes for each step are presented in Fig. 2." (Siebert et al 2021:6)
"The most basic and common one is an inconsistent understanding of quality by developers and users of ML systems. Additionally, quality criteria considered by system developers differ from the quality requirements of a system's users. Furthermore, data scientists tend to evaluate the fulfillment of quality criteria and their mutual dependencies rather implicitly, which makes it difficult to comprehend the decisions they made regarding quality." (Siebert et al 2021:8)
"Most of all, it creates a basis for considering the quality of an ML system systematically." (Siebert et al 2021:8)
"The concept of a property is general and can be used on different levels of abstraction. Entities and their properties may be abstract or specific. The basic difference between the two is that in contrast to specific properties of entities, generic ones are rather difficult to quantify and evaluate." (Siebert et al 2021:8)
"The CRISP-DM model (short for CRoss Industry Standard Process for Data Mining) (Shearer, 2000) is an open standard describing the different phases encountered in data analysis projects. This model proposes six phases, namely, business understanding, data understanding, data preparation, modeling, evaluation, deployment. It is currently thought to be the de-facto standard for projects developing ML components," (Siebert et al 2021:10)
"Data understanding The data understanding phase can be seen as a requirement engineering phase specifically directed towards the data (it usually goes hand in hand with the business understanding phase). Indeed, the type of analysis method and the corresponding evaluation measures that can be used depend on several data-related factors (besides the analysis objective): the type of data available (e.g., unstructured data like text or images vs. structured data like tabular data), whether some ground truth is available (see the discussion in the previous section), the quality of the data (e.g., its resolution, its representativity; whether noise, outliers, or missing values are present, etc.), and how the data is gathered." (Siebert et al 2021:11)
"Modeling The modeling phase is probably the tip of the iceberg when it comes to developing ML components. This is where methods such as ML are applied to form and evaluate the artifacts that make up the component. As previously mentioned, this phase is strongly linked to the data preparation phase. In general, an ML component is composed of several sub-components from these two phases. The quality of the ML model is impacted by several aspects: the type of task to be solved (e.g., classification, clustering, regression, anomaly detection, dimensionality reduction, etc.), the type of model (neural network, decision tree, etc.), the data used for building (i.e., training), and evaluating the developed artifacts, as well as the manner in which the data is separated for training and validation, together with requirements on runtime complexity or safety constraints. Since the way the component is created is experimental, the way these experiments are managed (using hyperparameter search, cross-validation, independent train-test split) also plays a role in terms of quality. The modeling phase also contains an evaluation part that aims at evaluating the trained component with regard to the available data. It does so by measuring performance" (Siebert et al 2021:11)
"measures (such as precision, recall, etc. for classification tasks), performing sensitivity analysis, or testing against adversarial examples." (Siebert et al 2021:12)
"An ML component usually consists of several subcomponents organized in a directed acyclic graph (also called a pipeline)" (Siebert et al 2021:13)
"Again, the goal of this distinction is to separate quality properties related to a specific entity instance from those related to the entity type. For example, the appropriateness of a given model applies to a model type (like the family of decision trees), whereas the goodness of fit applies to a specific trained instance." (Siebert et al 2021:13)
"In this step, we created a table of reference elements to be used in an ML quality model. We used the quality requirements and the views defined in the previous sections to select pertinent entities for the use case. From that point on, we identified quality properties of interest for all entities." (Siebert et al 2021:15)
"On the higher levels of the hierarchy, we are interested in the general quality (property) of each step of the pipeline (entity). F" (Siebert et al 2021:19)
"The processing pipeline is modeled as a hierarchy of entities (steps and sub-steps)." (Siebert et al 2021:19)
"On the lower levels of the quality model, all relevant entities and properties can be found for each step of the pipeline, such as "trained model × stability". Each property of an entity has a set of measures assigned to it and an evaluation rule describing how to evaluate the measures" (Siebert et al 2021:19)},
	annote = {Extracted Annotations (07/12/2021, 17:43:41)"In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively." (Siebert et al 2021:1)"e found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice." (Siebert et al 2021:1)"A data-driven software component is a piece of software that" (Siebert et al 2021:1)"solves a given task (e.g., image segmentation, sentiment analysis, classification, etc.), using methods from data science, such as machine learning (ML), data mining, natural language processing, signal processing, statistics, etc. The functionality of data-driven software components (or at least part of it) is not entirely defined by the programmer in the classical way (by programming it directly), but is derived (i.e., learned) from data. At the core of a data-driven software component lies the notion of a model (or several models, coupled together in a pipeline)." (Siebert et al 2021:2)"In short, the behavior of such components is first and foremost fundamentally different from traditional (i.e., not data-driven) software: the relationship between the input and the outcome of the software component is usually non-linear." (Siebert et al 2021:2)"This input-output relationship is also only defined for a subset of the data, which leads to uncertainty in outcomes for previously unseen data" (Siebert et al 2021:2)"Second, common development principles from software engineering, such as encapsulation and modularity, have to be rethought. For example, changing the application context of a data-driven component (its intended scope), the type of model used, or the internal parameters, usually implies retraining the component. This is also referred to as the CACE principle: changing anything changes everything (Sculley et al., 2015). Third, the development and integration of datadriven software components are a multi-disciplinary approach: it requires knowledge about the application domain, knowledge about how to construct models, and finally, knowledge about software engineering. Fourth, quality assurance, and specifically testing, works differently than in traditional software. This is because data-driven methods (such as ML, for instance) target problems where the expected solution is inherently difficult to formalize, and where test oracles are not directly available (Belani et al., 2019; Bosch et al., 2018; Horkoff, 2019; Zhang et al., 2020)." (Siebert et al 2021:2)"For instance, ISO/ IEC 25010 (ISO/IEC, 2011) defines quality models for software and systems, i.e., a hierarchy of quality properties of interest and how to quantify and assess them. Due to the different nature of data-driven software components, these quality models cannot be applied directly as they are." (Siebert et al 2021:2)"To develop meaningful quality models, it is necessary to understand the application context of the use case and what kind of data-driven method is used." (Siebert et al 2021:2)"In Sect. 7, we define different views one can take on an ML system and relevant entities, which will have to be evaluated for a specific use case and application context." (Siebert et al 2021:3)"To build a quality model, it is first necessary to define the usage scenarios (i.e., How will the system be used? By whom? What are the expectations in terms of quality? etc.). This naturally goes hand in hand with the definition of the relevant quality properties and the entities to be measured. It is also necessary to define how to measure these properties and, finally, to decide on the basis of these measurements what to do to improve certain properties of quality." (Siebert et al 2021:3)"Technical robustness, reliability, dependability (e.g., correctness of output, estimation of model uncertainty, robustness against harmful inputs, errors, or unexpected situations)" (Siebert et al 2021:4)"A model is by definition a re-presentation (i.e., a simplification) of some part of reality (i.e., the system under study)." (Siebert et al 2021:4)"To define usage scenarios and elicit requirements, it is necessary to understand the intended purpose of the model." (Siebert et al 2021:4)"ML components usually consist of several sub-components organized in pipelines: e.g., data preparation (e.g., resizing or cropping images, cleaning text), features engineering, training and evaluating the models." (Siebert et al 2021:5)"In (Nakajima,  2018), the authors distinguish between three main qualities, namely service quality, product quality, and platform quality. They also describe different views/entities of the system: the training dataset, the neural network, the hyper parameters," (Siebert et al 2021:5)"As a last example, the authors in Hamada et al. (2020) provide five main qualities related to views/entities, namely data integrity, model robustness, system quality, process agility, and customer expectation, including a total of 49 quality sub-properties." (Siebert et al 2021:5)"We also see that, because the field of data science is large, the importance of certain quality properties and measures for quantifying them depends on the concrete context and use case, and they have to be addressed in different tasks of the process model used." (Siebert et al 2021:5)"For example, for classification tasks, the goodness of fit can be measured by accuracy, precision, recall, f-score, etc" (Siebert et al 2021:6)"This section describes the process we followed to construct a quality model for ML systems based on our previous work in the field (Goeb et al., 2015). It consists of six steps, which we will describe sequentially, but which are performed iteratively in practice. An overview of all steps and illustrations of the major outcomes for each step are presented in Fig. 2." (Siebert et al 2021:6)"Define quality meta-model: First of all, we described the features of our ML quality model; that is, the basic structure we want to use for documenting all quality properties of interest and the measures/metrics for quantifying those properties" (Siebert et al 2021:6)"Define use case and application context: Previous research in the field of quality modeling concluded that the concept of "quality" highly depends on the application context and concrete use case." (Siebert et al 2021:6)"Identify relevant ML quality requirements: Developing an ML model can be split into different stages related to understanding the problem to solve, gathering the required data, and building the ML model itself" (Siebert et al 2021:7)"Identity relevant entities of an ML system: In an ML system, the ML model itself is only one entity of many. To build a comprehensive quality model for ML systems, it is important to analyze all relevant entities that could come into play, such as the data, the model itself, the infrastructure on which the model is executed," (Siebert et al 2021:7)"Identify reference elements of an ML quality model: Based on the identified quality requirements from the ML development process and the relevant entities of an ML system, we can create a table of reference elements to be used in an ML quality model." (Siebert et al 2021:7)"Instantiate quality model for use case:" (Siebert et al 2021:7)"The most basic and common one is an inconsistent understanding of quality by developers and users of ML systems. Additionally, quality criteria considered by system developers differ from the quality requirements of a system's users. Furthermore, data scientists tend to evaluate the fulfillment of quality criteria and their mutual dependencies rather implicitly, which makes it difficult to comprehend the decisions they made regarding quality." (Siebert et al 2021:8)"Most of all, it creates a basis for considering the quality of an ML system systematically." (Siebert et al 2021:8)"The concept of a property is general and can be used on different levels of abstraction. Entities and their properties may be abstract or specific. The basic difference between the two is that in contrast to specific properties of entities, generic ones are rather difficult to quantify and evaluate." (Siebert et al 2021:8)"Quality evaluation comprises four basic elements: measurement, evaluation, aggregation, and interpretation. Measurement consists of the collection of measurement data for the factors specified at the lowest level of the quality model's hierarchy according to the measures defined in the quality model. Evaluation involves assessing the fulfillment of quality preferences associated with the factor. Aggregation comprises the synthesis of assessments obtained on individual child factors in a bottom-up manner throughout the quality model hierarchy into an overall assessment of a system under assessment. Finally, interpretation is the translation of the potentially abstract quality assessments into evaluations that are understandable (intuitive) for human decision makers." (Siebert et al 2021:9)"The CRISP-DM model (short for CRoss Industry Standard Process for Data Mining) (Shearer, 2000) is an open standard describing the different phases encountered in data analysis projects. This model proposes six phases, namely, business understanding, data understanding, data preparation, modeling, evaluation, deployment. It is currently thought to be the de-facto standard for projects developing ML components," (Siebert et al 2021:10)"Data understanding The data understanding phase can be seen as a requirement engineering phase specifically directed towards the data (it usually goes hand in hand with the business understanding phase). Indeed, the type of analysis method and the corresponding evaluation measures that can be used depend on several data-related factors (besides the analysis objective): the type of data available (e.g., unstructured data like text or images vs. structured data like tabular data), whether some ground truth is available (see the discussion in the previous section), the quality of the data (e.g., its resolution, its representativity; whether noise, outliers, or missing values are present, etc.), and how the data is gathered." (Siebert et al 2021:11)"Modeling The modeling phase is probably the tip of the iceberg when it comes to developing ML components. This is where methods such as ML are applied to form and evaluate the artifacts that make up the component. As previously mentioned, this phase is strongly linked to the data preparation phase. In general, an ML component is composed of several sub-components from these two phases. The quality of the ML model is impacted by several aspects: the type of task to be solved (e.g., classification, clustering, regression, anomaly detection, dimensionality reduction, etc.), the type of model (neural network, decision tree, etc.), the data used for building (i.e., training), and evaluating the developed artifacts, as well as the manner in which the data is separated for training and validation, together with requirements on runtime complexity or safety constraints. Since the way the component is created is experimental, the way these experiments are managed (using hyperparameter search, cross-validation, independent train-test split) also plays a role in terms of quality. The modeling phase also contains an evaluation part that aims at evaluating the trained component with regard to the available data. It does so by measuring performance" (Siebert et al 2021:11)"measures (such as precision, recall, etc. for classification tasks), performing sensitivity analysis, or testing against adversarial examples." (Siebert et al 2021:12)"An ML component usually consists of several subcomponents organized in a directed acyclic graph (also called a pipeline)" (Siebert et al 2021:13)"Again, the goal of this distinction is to separate quality properties related to a specific entity instance from those related to the entity type. For example, the appropriateness of a given model applies to a model type (like the family of decision trees), whereas the goodness of fit applies to a specific trained instance." (Siebert et al 2021:13)"A given pipeline may be trained several times with different model types, training algorithms, or datasets in order to find the best combinations (also known as the Combined Model Selection and Hyperparameter optimization (CASH) problem" (Siebert et al 2021:14)"Since a decision outputted by an ML component is always subject to uncertainty, and since wrong decisions might impact the system's overall quality, considering the flow of information from the system input through all components to the system output is important for understanding the impact of a given ML component's quality on the overall system behavior" (Siebert et al 2021:14)"What we call the infrastructure view is closely related to the system view. However, here the view is more focused on the quality properties related to how the system is concretely implemented (e.g., hardware, training libraries)." (Siebert et al 2021:15)"The environment consists of elements that (1) are external to the system under consideration and (2) interact either directly or indirectly with the system" (Siebert et al 2021:15)"In this step, we created a table of reference elements to be used in an ML quality model. We used the quality requirements and the views defined in the previous sections to select pertinent entities for the use case. From that point on, we identified quality properties of interest for all entities." (Siebert et al 2021:15)"On the higher levels of the hierarchy, we are interested in the general quality (property) of each step of the pipeline (entity). F" (Siebert et al 2021:19)"The processing pipeline is modeled as a hierarchy of entities (steps and sub-steps)." (Siebert et al 2021:19)"On the lower levels of the quality model, all relevant entities and properties can be found for each step of the pipeline, such as "trained model × stability". Each property of an entity has a set of measures assigned to it and an evaluation rule describing how to evaluate the measures" (Siebert et al 2021:19)},
	file = {Siebert et al. - 2021 - Construction of a quality model for machine learni.pdf:/Users/johannesreichle/Zotero/storage/8NR2ADA9/Siebert et al. - 2021 - Construction of a quality model for machine learni.pdf:application/pdf},
}

@inproceedings{nakamichi_requirements-driven_2020,
	title = {Requirements-{Driven} {Method} to {Determine} {Quality} {Characteristics} and {Measurements} for {Machine} {Learning} {Software} and {Its} {Evaluation}},
	doi = {10.1109/RE48521.2020.00036},
	abstract = {As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio and Joeckel, Lisa and Siebert, Julien and Heidrich, Jens},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	keywords = {machine learning, Machine learning, Software systems, Data models, quality assurance, Quality assurance, quality characteristics, quality measures, quality requirements, Software measurement, software quality model},
	pages = {260--270},
	annote = {Extracted Annotations (03/12/2021, 11:05:59)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (07/12/2021, 17:43:39)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (08/12/2021, 11:37:27)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"It has been noted that MLS has essentially different characteristics from those of conventional software systems [12] [21]. Sculley et al. pointed out that machine learning component is only a small portion of the entire MLS. Therefore, it is necessary to consider the entire MLS instead of machine learning component [21]. This raises new problems in software engineering [1] [15]." (Nakamichi et al 2020:261)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"mportance of measurement of machine learning performance, explainability, and specific legal requirements as the difference from the conventional requirements" (Nakamichi et al 2020:261)
"Although the quality requirements of MLS are attracting attention, there have been few conventional works on the quality requirements of MLS in the context of enterprise systems." (Nakamichi et al 2020:261)
"The requirements analyst extracts important issues from the list of items to be considered during MLS development according to the customer's needs. On the other hand, MLS quality characteristics that are an extension of the ISO/IEC 25010 quality characteristics, measures, and measurement method are defined as MLS quality evaluation models. In addition, a mapping between the above issues and the related MLS quality characteristics is provided in advance. As a result, it is possible to derive the quality and measurement items to be guaranteed by the MLS based on the customer's needs." (Nakamichi et al 2020:262)
"The context of MLS discussed in this paper consists of environment/user, system/infrastructure, model, and data as shown in Fig. 2 [7]." (Nakamichi et al 2020:262)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (25/12/2021, 11:56:47)"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)"It has been noted that MLS has essentially different characteristics from those of conventional software systems [12] [21]. Sculley et al. pointed out that machine learning component is only a small portion of the entire MLS. Therefore, it is necessary to consider the entire MLS instead of machine learning component [21]. This raises new problems in software engineering [1] [15]." (Nakamichi et al 2020:261)"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)"mportance of measurement of machine learning performance, explainability, and specific legal requirements as the difference from the conventional requirements" (Nakamichi et al 2020:261)"Although the quality requirements of MLS are attracting attention, there have been few conventional works on the quality requirements of MLS in the context of enterprise systems." (Nakamichi et al 2020:261)"The requirements analyst extracts important issues from the list of items to be considered during MLS development according to the customer's needs. On the other hand, MLS quality characteristics that are an extension of the ISO/IEC 25010 quality characteristics, measures, and measurement method are defined as MLS quality evaluation models. In addition, a mapping between the above issues and the related MLS quality characteristics is provided in advance. As a result, it is possible to derive the quality and measurement items to be guaranteed by the MLS based on the customer's needs." (Nakamichi et al 2020:262)"The context of MLS discussed in this paper consists of environment/user, system/infrastructure, model, and data as shown in Fig. 2 [7]." (Nakamichi et al 2020:262)"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)"We assume that the correctness of a function of MLS corresponds to the correctness of the output data. The output data, test data, and trained models of MLS were specified as entities related to the MLS output. We defined the properties of these entities to be evaluated in terms of functional correctness. The properties are listed below: (1) Representativeness: A property of the data; the degree to which the statistical characteristics of the actual data set are consistent with the statistics assumed for the data set. (2) Independence: A property of test data; the extent to which test data are independent from training data." (Nakamichi et al 2020:263)"(3) Fitness: A quality of the output data of the MLS and trained models, and the degree of correctness." (Nakamichi et al 2020:264)"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	file = {Nakamichi et al_2020_Requirements-Driven Method to Determine Quality Characteristics and.pdf:/Users/johannesreichle/Zotero/storage/GZJKLEGC/Nakamichi et al_2020_Requirements-Driven Method to Determine Quality Characteristics and.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/AWR84EVA/9218162.html:text/html},
}

@article{ashmore_assuring_2021,
	title = {Assuring the {Machine} {Learning} {Lifecycle}: {Desiderata}, {Methods}, and {Challenges}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Assuring the {Machine} {Learning} {Lifecycle}},
	url = {https://dl.acm.org/doi/10.1145/3453444},
	doi = {10.1145/3453444},
	abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the
              assurance of ML
              , i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the
              machine learning lifecycle
              , i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
	language = {en},
	number = {5},
	urldate = {2021-12-01},
	journal = {ACM Computing Surveys},
	author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
	month = jun,
	year = {2021},
	pages = {1--39},
	annote = {Extracted Annotations (01/12/2021, 16:22:29)
"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)
"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)
"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)
"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)
"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)
"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	annote = {Extracted Annotations (02/12/2021, 11:48:08)
"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)
"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)
"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)
"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)
"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)
"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	annote = {Extracted Annotations (15/12/2021, 15:19:31)"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)"An ML model is performant if it operates as expected according to a measure (or set of measures) that captures relevant characteristics of the model output." (Ashmore et al 2021:14)"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	file = {Ashmore et al. - 2021 - Assuring the Machine Learning Lifecycle Desiderat.pdf:/Users/johannesreichle/Zotero/storage/RJRLZU8A/Ashmore et al. - 2021 - Assuring the Machine Learning Lifecycle Desiderat.pdf:application/pdf},
}

@inproceedings{seshia_formal_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Formal {Specification} for {Deep} {Neural} {Networks}},
	isbn = {978-3-030-01090-4},
	doi = {10.1007/978-3-030-01090-4_2},
	abstract = {The increasing use of deep neural networks in a variety of applications, including some safety-critical ones, has brought renewed interest in the topic of verification of neural networks. However, verification is most meaningful when performed with high-quality formal specifications. In this paper, we survey the landscape of formal specification for deep neural networks, and discuss the opportunities and challenges for formal methods for this domain.},
	language = {en},
	booktitle = {Automated {Technology} for {Verification} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Seshia, Sanjit A. and Desai, Ankush and Dreossi, Tommaso and Fremont, Daniel J. and Ghosh, Shromona and Kim, Edward and Shivakumar, Sumukh and Vazquez-Chanlatte, Marcell and Yue, Xiangyu},
	editor = {Lahiri, Shuvendu K. and Wang, Chao},
	year = {2018},
	pages = {20--34},
	annote = {Extracted Annotations (02/12/2021, 11:48:27)
"Consider a sample space Z of the form X × Y , and an ordered training set S = ((xi, yi))m=1, where xi ∈ X is the data and yi ∈ Y is the corresponding label. Let H be a hypothesis space (e.g., a particular neural network architecture parameterized by a weight vectorw). If the network computes a function from X to Y , we will denote it by fw; i.e., fw(x) = y. There is a loss (or risk) function : H × Z →R so that given a hypothesisw ∈ H and a sample (x, y) ∈ Z, we" (Seshia et al 2018:21)
"obtain a loss (w, (x, y)). We consider the case where we want to minimize the average loss over the training set S, 1 m LS(w) = m In the equation given above, λ {\textgreater} 0 and the term R(w) is called the regularizer; the latter seeks to enforce a notion of "simplicity" inw. Since S is fixed, we sometimes denote i(w) = (w, (xi, yi)) as a function only ofw. The training problem is to find aw that minimizes Ls(w); i.e., we wish to solve the following optimization problem: min LS(w) w∈H" (Seshia et al 2018:22)
"In recent years, a significant amount of work has addressed the robustness (or lack thereof) of neural networks to so-called "adversarial perturbations" of their inputs (for example, [5, 9, 27, 38, 42, 43, 54, 58]). Techniques used to demonstrate a lack of robustness are often referred to as "adversarial analysis."" (Seshia et al 2018:24)},
	file = {Seshia et al_2018_Formal Specification for Deep Neural Networks.pdf:/Users/johannesreichle/Zotero/storage/I2VTXLR2/Seshia et al_2018_Formal Specification for Deep Neural Networks.pdf:application/pdf},
}

@article{zhang_machine_2020,
	title = {Machine {Learning} {Testing}: {Survey}, {Landscapes} and {Horizons}},
	issn = {1939-3520},
	shorttitle = {Machine {Learning} {Testing}},
	doi = {10.1109/TSE.2019.2962027},
	abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 138 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in machine learning testing.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {machine learning, Machine learning, Robustness, Software engineering, Data models, deep neural network, software testing, Software testing, Training data},
	pages = {1--1},
	file = {Zhang et al_2020_Machine Learning Testing.pdf:/Users/johannesreichle/Zotero/storage/SYKPYQ9U/Zhang et al_2020_Machine Learning Testing.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/YJDAU47Q/9000651.html:text/html},
}

@inproceedings{xing_convolutional_2019,
	address = {Seoul, Korea (South)},
	title = {Convolutional {Character} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010699/},
	doi = {10.1109/ICCV.2019.00922},
	abstract = {Recent progress has been made on developing a uniﬁed framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage frameworks by involving ROI pooling, which can degrade the performance on recognition tasks. In this work, we propose convolutional character networks (”CharNet”), which is a one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize a character as basic element, allowing us to overcome the main difﬁculty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful onestage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-theart approaches [26, 25] by a large margin, e.g., with improvements of 65.33\%→71.08\% (with generic lexicon) on ICDAR 2015, and 54.0\%→69.23\% on Total-Text, on endto-end text recognition. Code is available at: https:// github.com/MalongTech/research-charnet.},
	language = {en},
	urldate = {2021-12-10},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Xing, Linjie and Tian, Zhi and Huang, Weilin and Scott, Matthew},
	month = oct,
	year = {2019},
	pages = {9125--9135},
	file = {Xing et al. - 2019 - Convolutional Character Networks.pdf:/Users/johannesreichle/Zotero/storage/P5B8NDTU/Xing et al. - 2019 - Convolutional Character Networks.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
	annote = {Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models},
	file = {Goodfellow et al_2016_Deep learning.pdf:/Users/johannesreichle/Zotero/storage/675ZE798/Goodfellow et al_2016_Deep learning.pdf:application/pdf},
}

@book{abramowicz_business_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Business {Information} {Systems}: 22nd {International} {Conference}, {BIS} 2019, {Seville}, {Spain}, {June} 26–28, 2019, {Proceedings}, {Part} {II}},
	volume = {354},
	isbn = {978-3-030-20481-5 978-3-030-20482-2},
	shorttitle = {Business {Information} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-030-20482-2},
	language = {en},
	urldate = {2021-12-14},
	publisher = {Springer International Publishing},
	editor = {Abramowicz, Witold and Corchuelo, Rafael},
	year = {2019},
	doi = {10.1007/978-3-030-20482-2},
	file = {Abramowicz and Corchuelo - 2019 - Business Information Systems 22nd International C.pdf:/Users/johannesreichle/Zotero/storage/5RZWASEF/Abramowicz and Corchuelo - 2019 - Business Information Systems 22nd International C.pdf:application/pdf},
}

@misc{noauthor_yangxue0827rotationdetection_nodate,
	title = {yangxue0827/{RotationDetection}: {This} is a tensorflow-based rotation detection benchmark, also called {AlphaRotate}.},
	url = {https://github.com/yangxue0827/RotationDetection},
	urldate = {2021-12-15},
	file = {yangxue0827/RotationDetection\: This is a tensorflow-based rotation detection benchmark, also called AlphaRotate.:/Users/johannesreichle/Zotero/storage/UD6Y49BQ/RotationDetection.html:text/html},
}

@inproceedings{ignatov_ai_2019,
	title = {{AI} {Benchmark}: {All} {About} {Deep} {Learning} on {Smartphones} in 2019},
	shorttitle = {{AI} {Benchmark}},
	doi = {10.1109/ICCVW.2019.00447},
	abstract = {The performance of mobile AI accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of SoCs. The current 4th generation of mobile NPUs is already approaching the results of CUDA-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep AI models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware acceleration for AI inference. We also discuss the recent changes in the Android ML pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Machine learning, Machine Learning, Acceleration, AI Benchmark, Android, Androids, Artificial Intelligence, Benchmark, Computer Vision, Deep Learning, Humanoid robots, Mobile, Mobile handsets, Performance evaluation, Smartphones, SoCs, TensorFlow},
	pages = {3617--3635},
	file = {Ignatov et al_2019_AI Benchmark.pdf:/Users/johannesreichle/Zotero/storage/YM4YKD6J/Ignatov et al_2019_AI Benchmark.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/MGC3F8N2/9022101.html:text/html},
}

@article{chng_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Arbitrary}-{Shaped} {Text} ({RRC}-{ArT})},
	url = {http://arxiv.org/abs/1909.07145},
	abstract = {This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT) that consists of three major challenges: i) scene text detection, ii) scene text recognition, and iii) scene text spotting. A total of 78 submissions from 46 unique teams/individuals were received for this competition. The top performing score of each challenge is as follows: i) T1 - 82.65\%, ii) T2.1 - 74.3\%, iii) T2.2 - 85.32\%, iv) T3.1 - 53.86\%, and v) T3.2 - 54.91\%. Apart from the results, this paper also details the ArT dataset, tasks description, evaluation metrics and participants methods. The dataset, the evaluation kit as well as the results are publicly available at https://rrc.cvc.uab.es/?ch=14},
	urldate = {2021-12-21},
	journal = {arXiv:1909.07145 [cs]},
	author = {Chng, Chee-Kheng and Liu, Yuliang and Sun, Yipeng and Ng, Chun Chet and Luo, Canjie and Ni, Zihan and Fang, ChuanMing and Zhang, Shuaitao and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and Chan, Chee Seng and Jin, Lianwen},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.07145
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Technical report of ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT) Competition},
	file = {Chng et al_2019_ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT).pdf:/Users/johannesreichle/Zotero/storage/QYHGFJ2X/Chng et al_2019_ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT).pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/MV9GJITH/1909.html:text/html},
}

@inproceedings{baek_what_2019,
	address = {Seoul, Korea (South)},
	title = {What {Is} {Wrong} {With} {Scene} {Text} {Recognition} {Model} {Comparisons}? {Dataset} and {Model} {Analysis}},
	isbn = {978-1-72814-803-8},
	shorttitle = {What {Is} {Wrong} {With} {Scene} {Text} {Recognition} {Model} {Comparisons}?},
	url = {https://ieeexplore.ieee.org/document/9010273/},
	doi = {10.1109/ICCV.2019.00481},
	abstract = {Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the ﬁeld due to the inconsistent choices of training and evaluation datasets. This paper addresses this difﬁculty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a uniﬁed four-stage STR framework that most existing STR models ﬁt into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available1.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Baek, Jeonghun and Kim, Geewook and Lee, Junyeop and Park, Sungrae and Han, Dongyoon and Yun, Sangdoo and Oh, Seong Joon and Lee, Hwalsuk},
	month = oct,
	year = {2019},
	pages = {4714--4722},
	annote = {Extracted Annotations (26/12/2021, 10:22:48)"First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into." (Baek et al 2019:4714)"The maturity of Optical Character Recognition (OCR) systems has led to its successful application on cleaned documents, but most traditional OCR methods have failed to be as effective on STR tasks due to the diverse text appearances that occur in the real world and the imperfect conditions in which these scenes are captured." (Baek et al 2019:4714)"Finally, we study the module-wise contributions in terms of accuracy, speed, and memory demand, under a unified experimental setting" (Baek et al 2019:4715)"Due to the resemblence of STR to computer vision tasks (e.g. object detection) and sequence prediction tasks, STR has benefited from high-performance convolutional neural networks (CNNs) and recurrent neural networks (RNNs)." (Baek et al 2019:4717)"The four stages derived from existing STR models are as follows: 1. Transformation (Trans.) normalizes the input text image using the Spatial Transformer Network (STN [11]) to ease downstream stages. 2. Feature extraction (Feat.) maps the input image to a representation that focuses on the attributes relevant for character recognition, while suppressing irrelevant features such as font, color, size, and background. 3. Sequence modeling (Seq.) captures the contextual information within a sequence of characters for the next stage to predict each character more robustly, rather than doing it independently. 4. Prediction (Pred.) estimates the output character sequence from the identified features of an image." (Baek et al 2019:4717)"In this stage, from the input H, a module predict a sequence of characters, (i.e., Y = y1 , y2 , . . . ). By summing up previous works, we have two options for prediction: (1) Connectionist temporal classification (CTC) [6] and (2) attention-based sequence prediction (Attn) [24, 4]." (Baek et al 2019:4718)"Vertical texts: most of current STR models assumes horizontal text images, and thus structurally could not deal with vertical texts. Some STR models [28, 5] exploit vertical information also, however, vertical texts are not clearly covered yet. Further research would be needed to cover vertical texts." (Baek et al 2019:4721)},
	file = {Baek et al. - 2019 - What Is Wrong With Scene Text Recognition Model Co.pdf:/Users/johannesreichle/Zotero/storage/82G8BT9K/Baek et al. - 2019 - What Is Wrong With Scene Text Recognition Model Co.pdf:application/pdf},
}

@article{atienza_vision_2021,
	title = {Vision {Transformer} for {Fast} and {Efficient} {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/2105.08582},
	abstract = {Scene text recognition (STR) enables computers to read text in natural scenes such as object labels, road signs and instructions. STR helps machines perform informed decisions such as what object to pick, which direction to go, and what is the next step of action. In the body of work on STR, the focus has always been on recognition accuracy. There is little emphasis placed on speed and computational efficiency which are equally important especially for energy-constrained mobile machines. In this paper we propose ViTSTR, an STR with a simple single stage model architecture built on a compute and parameter efficient vision transformer (ViT). On a comparable strong baseline method such as TRBA with accuracy of 84.3\%, our small ViTSTR achieves a competitive accuracy of 82.6\% (84.2\% with data augmentation) at 2.4x speed up, using only 43.4\% of the number of parameters and 42.2\% FLOPS. The tiny version of ViTSTR achieves 80.3\% accuracy (82.1\% with data augmentation), at 2.5x the speed, requiring only 10.9\% of the number of parameters and 11.9\% FLOPS. With data augmentation, our base ViTSTR outperforms TRBA at 85.2\% accuracy (83.7\% without augmentation) at 2.3x the speed but requires 73.2\% more parameters and 61.5\% more FLOPS. In terms of trade-offs, nearly all ViTSTR configurations are at or near the frontiers to maximize accuracy, speed and computational efficiency all at the same time.},
	urldate = {2021-12-21},
	journal = {arXiv:2105.08582 [cs]},
	author = {Atienza, Rowel},
	month = may,
	year = {2021},
	note = {arXiv: 2105.08582},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear at ICDAR2021 Springer Lecture Notes in Computer Science series},
	file = {Atienza_2021_Vision Transformer for Fast and Efficient Scene Text Recognition.pdf:/Users/johannesreichle/Zotero/storage/8HCJS8TP/Atienza_2021_Vision Transformer for Fast and Efficient Scene Text Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/QVWX5KYD/2105.html:text/html},
}

@article{kumar_scene_2016,
	title = {Scene {Text} {Recognition} using {Artificial} {Neural} {Network}: {A} {Survey}},
	volume = {137},
	shorttitle = {Scene {Text} {Recognition} using {Artificial} {Neural} {Network}},
	doi = {10.5120/ijca2016908804},
	abstract = {Nowadays, scene text recognition has become an important emerging area of research in the field of image processing. In image processing, character recognition boosts the complexity in the area of Artificial Intelligence. Character recognition is not easy for computer programs in comparison to humans. In the broad spectrum of things, it may consider that recognizing patterns is the only thing which humans can do well and computers cannot. There are many reasons including various sources of variability, hypothesis and absence of hard-and-fast rules that define the appearance of a visual character. Hence; there is an unavoidable requirement for heuristic deduction of rules from different samples. This review highlights the superiority of artificial neural networks, a popular area of Artificial Intelligence, over various other available methods like fuzzy logic and genetic algorithm. In this paper, two methods are listed for character recognition – offline and online. The ―Offline‖ methods include Feature Extraction, Clustering, and Pattern Matching. Artificial neural networks use the static image properties. The online methods are divided into two methods, k-NN classifier and direction based algorithm. Thus, the scale of techniques available for scene text recognition deserves an admiration. This review gives a detail survey of use of artificial neural network in scene text recognition.},
	journal = {International Journal of Computer Applications},
	author = {Kumar, Sunil and Kumar, Krishan and Mishra, Rahul},
	month = apr,
	year = {2016},
	pages = {975--8887},
	file = {Kumar et al_2016_Scene Text Recognition using Artificial Neural Network.pdf:/Users/johannesreichle/Zotero/storage/8SIPHDJT/Kumar et al_2016_Scene Text Recognition using Artificial Neural Network.pdf:application/pdf},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Machine learning, Computer algorithms},
}

@inproceedings{chauhan_review_2018,
	title = {A {Review} on {Conventional} {Machine} {Learning} vs {Deep} {Learning}},
	doi = {10.1109/GUCON.2018.8675097},
	abstract = {In now days, deep learning has become a prominent and emerging research area in computer vision applications. Deep learning permits the multiple layers models for computation to learn representations of data by processing in their original form while it is not possible in conventional machine learning. These methods surprisingly improved the accuracy of various image processing domains such as speech recognition, face recognition, object detection and in biomedical applications. Deep neural networks (DNN) such as convolutional neural network (CNN) provide tremendous results in processing of images and videos, while another approach of deep network i.e. recurrent neural network (RNN) gives better performance with sequential data such as text and speech.},
	booktitle = {2018 {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Chauhan, Nitin Kumar and Singh, Krishna},
	month = sep,
	year = {2018},
	keywords = {Deep learning, Neural networks, CNN, ANN, Classification algorithms, DNN, DT, Fully connected layers, LDA, Machine learning algorithms, Neurons, PCA, Pooling layers, QDA, RBM, RNN, Support vector machines, SVM},
	pages = {347--352},
	file = {Chauhan_Singh_2018_A Review on Conventional Machine Learning vs Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/J833HDNX/Chauhan_Singh_2018_A Review on Conventional Machine Learning vs Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/VAR38ZMB/8675097.html:text/html},
}

@book{richter_statistisches_2019,
	address = {Berlin, Heidelberg},
	title = {Statistisches und maschinelles {Lernen}: {Gängige} {Verfahren} im Überblick},
	isbn = {978-3-662-59353-0 978-3-662-59354-7},
	shorttitle = {Statistisches und maschinelles {Lernen}},
	url = {http://link.springer.com/10.1007/978-3-662-59354-7},
	language = {de},
	urldate = {2022-01-09},
	publisher = {Springer Berlin Heidelberg},
	author = {Richter, Stefan},
	year = {2019},
	doi = {10.1007/978-3-662-59354-7},
	file = {Richter - 2019 - Statistisches und maschinelles Lernen Gängige Ver.pdf:/Users/johannesreichle/Zotero/storage/8Y32ACBZ/Richter - 2019 - Statistisches und maschinelles Lernen Gängige Ver.pdf:application/pdf},
}

@book{geron_hands-machine_2017,
	address = {Beijing ; Boston},
	edition = {First edition},
	title = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4919-6229-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}},
	abstract = {"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" --},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	year = {2017},
	note = {OCLC: ocn953432302},
	keywords = {Artificial intelligence, Machine learning, Automatische Klassifikation, COMPUTERS / Computer Vision \& Pattern Recognition, COMPUTERS / Data Processing, COMPUTERS / Intelligence (AI) \& Semantics, COMPUTERS / Natural Language Processing, COMPUTERS / Neural Networks, Künstliche Intelligenz, Maschinelles Lernen, Nonfiction, Python 3.0},
	annote = {Includes index},
	file = {Géron - Hands-On Machine Learning with Scikit-Learn and Te.pdf:/Users/johannesreichle/Zotero/storage/CXQSL6VU/Géron - Hands-On Machine Learning with Scikit-Learn and Te.pdf:application/pdf},
}

@article{ye_least_nodate,
	title = {Least {Squares} {Linear} {Discriminant} {Analysis}},
	abstract = {Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classiﬁcation. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for binary-class classiﬁcations can be formulated as a least squares problem. Previous studies have shown certain relationship between multivariate linear regression and LDA for the multi-class case. Many of these studies show that multivariate linear regression with a speciﬁc class indicator matrix as the output can be applied as a preprocessing step for LDA. However, directly casting LDA as a least squares problem is challenging for the multi-class case. In this paper, a novel formulation for multivariate linear regression is proposed. The equivalence relationship between the proposed least squares formulation and LDA for multi-class classiﬁcations is rigorously established under a mild condition, which is shown empirically to hold in many applications involving high-dimensional data. Several LDA extensions based on the equivalence relationship are discussed.},
	language = {en},
	author = {Ye, Jieping},
	pages = {8},
	file = {Ye - Least Squares Linear Discriminant Analysis.pdf:/Users/johannesreichle/Zotero/storage/ARDF7AQJ/Ye - Least Squares Linear Discriminant Analysis.pdf:application/pdf},
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	annote = {Includes index},
}

@article{ho_real-world-weight_2020,
	title = {The {Real}-{World}-{Weight} {Cross}-{Entropy} {Loss} {Function}: {Modeling} the {Costs} of {Mislabeling}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {The {Real}-{World}-{Weight} {Cross}-{Entropy} {Loss} {Function}},
	doi = {10.1109/ACCESS.2019.2962617},
	abstract = {In this paper, we propose a new metric to measure goodness-of-fit for classifiers: the Real World Cost function. This metric factors in information about a real world problem, such as financial impact, that other measures like accuracy or F1 do not. This metric is also more directly interpretable for users. To optimize for this metric, we introduce the Real-World-Weight Cross-Entropy loss function, in both binary classification and single-label multiclass classification variants. Both variants allow direct input of real world costs as weights. For single-label, multiclass classification, our loss function also allows direct penalization of probabilistic false positives, weighted by label, during the training of a machine learning model. We compare the design of our loss function to the binary cross-entropy and categorical cross-entropy functions, as well as their weighted variants, to discuss the potential for improvement in handling a variety of known shortcomings of machine learning, ranging from imbalanced classes to medical diagnostic error to reinforcement of social bias. We create scenarios that emulate those issues using the MNIST data set and demonstrate empirical results of our new loss function. Finally, we discuss our intuition about why this approach works and sketch a proof based on Maximum Likelihood Estimation.},
	journal = {IEEE Access},
	author = {Ho, Yaoshiang and Wookey, Samuel},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Neural networks, Machine learning, Training, Measurement, class imbalance, cross-entropy, ethnic stereotypes, maximum likelihood estimation, Maximum likelihood estimation, oversampling, Probabilistic logic, social bias, softmax, Standards, undersampling},
	pages = {4806--4813},
	file = {Ho_Wookey_2020_The Real-World-Weight Cross-Entropy Loss Function.pdf:/Users/johannesreichle/Zotero/storage/U6H99L69/Ho_Wookey_2020_The Real-World-Weight Cross-Entropy Loss Function.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TVP49BNC/8943952.html:text/html},
}

@article{wolpert_lack_1996,
	title = {The {Lack} of {A} {Priori} {Distinctions} {Between} {Learning} {Algorithms}},
	volume = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1996.8.7.1341},
	doi = {10.1162/neco.1996.8.7.1341},
	abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
	number = {7},
	urldate = {2022-01-13},
	journal = {Neural Computation},
	author = {Wolpert, David H.},
	month = oct,
	year = {1996},
	pages = {1341--1390},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/KB5LF5VV/The-Lack-of-A-Priori-Distinctions-Between-Learning.html:text/html;Wolpert_ARTICLE Communicated by Steven Nowlan The Lack of A Priori Distinctions Between.pdf:/Users/johannesreichle/Zotero/storage/Y48F3CJS/Wolpert_ARTICLE Communicated by Steven Nowlan The Lack of A Priori Distinctions Between.pdf:application/pdf},
}

@article{boue_deep_2018,
	title = {Deep learning for pedestrians: backpropagation in {CNNs}},
	shorttitle = {Deep learning for pedestrians},
	url = {http://arxiv.org/abs/1811.11987},
	abstract = {The goal of this document is to provide a pedagogical introduction to the main concepts underpinning the training of deep neural networks using gradient descent; a process known as backpropagation. Although we focus on a very influential class of architectures called "convolutional neural networks" (CNNs) the approach is generic and useful to the machine learning community as a whole. Motivated by the observation that derivations of backpropagation are often obscured by clumsy index-heavy narratives that appear somewhat mathemagical, we aim to offer a conceptually clear, vectorized description that articulates well the higher level logic. Following the principle of "writing is nature's way of letting you know how sloppy your thinking is", we try to make the calculations meticulous, self-contained and yet as intuitive as possible. Taking nothing for granted, ample illustrations serve as visual guides and an extensive bibliography is provided for further explorations. (For the sake of clarity, long mathematical derivations and visualizations have been broken up into short "summarized views" and longer "detailed views" encoded into the PDF as optional content groups. Some figures contain animations designed to illustrate important concepts in a more engaging style. For these reasons, we advise to download the document locally and open it using Adobe Acrobat Reader. Other viewers were not tested and may not render the detailed views, animations correctly.)},
	urldate = {2022-01-20},
	journal = {arXiv:1811.11987 [cs, stat]},
	author = {Boué, Laurent},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.11987},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Symbolic Computation},
	file = {Boué_2018_Deep learning for pedestrians.pdf:/Users/johannesreichle/Zotero/storage/QAQQ4UWJ/Boué_2018_Deep learning for pedestrians.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/LHFH7XKK/1811.html:text/html},
}

@inproceedings{davis_relationship_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {The relationship between {Precision}-{Recall} and {ROC} curves},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
	doi = {10.1145/1143844.1143874},
	abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an eﬃcient algorithm for computing this curve. Finally, we also note diﬀerences in the two types of curves are signiﬁcant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
	language = {en},
	urldate = {2022-01-23},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	author = {Davis, Jesse and Goadrich, Mark},
	year = {2006},
	pages = {233--240},
	file = {Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:/Users/johannesreichle/Zotero/storage/ZMM8UG98/Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:application/pdf},
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	isbn = {978-3-319-46448-0},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300300×300300 {\textbackslash}times 300 input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512×512512×512512 {\textbackslash}times 512 input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional neural network, Real-time object detection},
	pages = {21--37},
	file = {Liu et al_2016_SSD.pdf:/Users/johannesreichle/Zotero/storage/JWLQEVVX/Liu et al_2016_SSD.pdf:application/pdf},
}

@article{qin_towards_nodate,
	title = {Towards {Unconstrained} {End}-to-{End} {Text} {Spotting}},
	abstract = {We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectiﬁcation. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to signiﬁcant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6\%, and on the Total-Text (curved) benchmark by more than 16\%.},
	language = {en},
	author = {Qin, Siyang and Bissacco, Alessandro and Raptis, Michalis and Fujii, Yasuhisa and Xiao, Ying},
	pages = {11},
	file = {Qin et al. - Towards Unconstrained End-to-End Text Spotting.pdf:/Users/johannesreichle/Zotero/storage/FSW8B4J8/Qin et al. - Towards Unconstrained End-to-End Text Spotting.pdf:application/pdf},
}

@inproceedings{su_relationship_2015,
	address = {Northampton Massachusetts USA},
	title = {A {Relationship} between the {Average} {Precision} and the {Area} {Under} the {ROC} {Curve}},
	isbn = {978-1-4503-3833-2},
	url = {https://dl.acm.org/doi/10.1145/2808194.2809481},
	doi = {10.1145/2808194.2809481},
	abstract = {For similar evaluation tasks, the area under the receiver operating characteristic curve (AUC) is often used by researchers in machine learning, whereas the average precision (AP) is used more often by the information retrieval community. We establish some results to explain why this is the case. Speciﬁcally, we show that, when both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system.},
	language = {en},
	urldate = {2022-01-23},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {The} {Theory} of {Information} {Retrieval}},
	publisher = {ACM},
	author = {Su, Wanhua and Yuan, Yan and Zhu, Mu},
	month = sep,
	year = {2015},
	pages = {349--352},
	file = {Su et al. - 2015 - A Relationship between the Average Precision and t.pdf:/Users/johannesreichle/Zotero/storage/CAT4EF9T/Su et al. - 2015 - A Relationship between the Average Precision and t.pdf:application/pdf},
}
