
@misc{beom_text_2021,
	title = {Text {Detector} for {OCR}},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/Text_Detector},
	abstract = {Text detection model that combines Retinanet with textboxes++ for OCR},
	urldate = {2021-09-18},
	author = {Beom},
	month = aug,
	year = {2021},
	note = {original-date: 2019-03-12T05:11:06Z},
}

@misc{beom_crnn_2021,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-09-18},
	author = {Beom},
	month = sep,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
	annote = {CRNN implementation (Keras)},
}

@misc{bhat_rajesh-bhatspark-ai-summit-2020-text-extraction_2021,
	title = {rajesh-bhat/spark-ai-summit-2020-text-extraction},
	copyright = {MIT},
	url = {https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/43eeb1f1a27e6ae84dcb0ef4cc11494dcc61cafb/CRNN_CTC_wandb.ipynb},
	urldate = {2021-09-22},
	author = {Bhat, Rajesh Shreedhar},
	month = aug,
	year = {2021},
	note = {original-date: 2020-06-02T14:21:10Z},
}

@inproceedings{chen_improvement_2018,
	address = {Shanghai, China},
	title = {Improvement {Research} and {Application} of {Text} {Recognition} {Algorithm} {Based} on {CRNN}},
	isbn = {978-1-4503-6605-2},
	url = {http://dl.acm.org/citation.cfm?doid=3297067.3297073},
	doi = {10.1145/3297067.3297073},
	abstract = {This paper is based on CRNN model to recognize the text in the images of football matches scene, and two improvements are proposed. Considering the edge feature of text is strong, this paper adds MFM layers into CRNN model aiming to enhance the contrast. In order to solve the problem of losing details of image static features in the process of getting contextual features, this paper fuses up these two kinds of features. The training and testing experiments carried out on public dataset and manual dataset respectively verify the validity of the improvements, and the recognition accurate rate is higher than original model.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Signal} {Processing} and {Machine} {Learning} - {SPML} '18},
	publisher = {ACM Press},
	author = {Chen, Lei and Li, Shaobin},
	year = {2018},
	pages = {166--170},
	file = {Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:/Users/johannesreichle/Zotero/storage/4JUQTVGL/Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:application/pdf},
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2021-09-24},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/M7XHZX79/backpropagation-in-convolutional-neural-networks.html:text/html},
}

@misc{shperber_gentle_2021,
	title = {A gentle introduction to {OCR}},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa},
	abstract = {How and why to apply deep learning to Optical Character Recognition},
	language = {en},
	urldate = {2021-09-18},
	journal = {Medium},
	author = {Shperber, Gidi},
	month = feb,
	year = {2021},
	annote = {Has a lot of approaches that can be checked},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/JSRM93VX/a-gentle-introduction-to-ocr-ee1469a201aa.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/PFLR88V5/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TSDHL78C/4376991.html:text/html},
}

@article{zhou_east_2017,
	title = {{EAST}: {An} {Efficient} and {Accurate} {Scene} {Text} {Detector}},
	shorttitle = {{EAST}},
	url = {http://arxiv.org/abs/1704.03155},
	abstract = {Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.},
	urldate = {2021-09-18},
	journal = {arXiv:1704.03155 [cs]},
	author = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
	month = jul,
	year = {2017},
	note = {arXiv: 1704.03155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2017, fix equation (3)},
	file = {arXiv Fulltext PDF:/Users/johannesreichle/Zotero/storage/GXDSSU9P/Zhou et al. - 2017 - EAST An Efficient and Accurate Scene Text Detecto.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/2VF6VH7T/1704.html:text/html},
}

@misc{bhat_text_nodate,
	title = {Text {Recognition} {With} {CRNN}-{CTC} {Network} - {Weights} \& {Biases}},
	url = {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2021-09-22},
	journal = {W\&B},
	author = {Bhat, Rajesh Shreedhar},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/9Q9S8C2K/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI.html:text/html},
}

@article{hochreiter_long_nodate,
	title = {Long {Short} {Term} {Memory}},
	abstract = {Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitmry time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-T ime Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
	language = {en},
	author = {Hochreiter, Sepp and Schmidhuber, Jiirgen},
	pages = {12},
	file = {Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:/Users/johannesreichle/Zotero/storage/VPLXUVMT/Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:application/pdf},
}

@inproceedings{kloss_learning_2016,
	address = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	language = {en},
	urldate = {2021-09-26},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	month = oct,
	year = {2016},
	pages = {5238--5245},
	file = {Kloss et al. - 2016 - Learning where to search using visual attention.pdf:/Users/johannesreichle/Zotero/storage/VJ2CKK4W/Kloss et al. - 2016 - Learning where to search using visual attention.pdf:application/pdf},
}

@misc{odegua_how_2020,
	title = {How to put machine learning models into production},
	url = {https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/},
	abstract = {The goal of building a machine learning model is to solve a problem, and a machine learning model can only do so when it is in production and actively in use by consumers. As such, model deployment is as important as model building.},
	language = {en-US},
	urldate = {2021-09-30},
	journal = {Stack Overflow Blog},
	author = {Odegua, Rising},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/ZG37NKW8/how-to-put-machine-learning-models-into-production.html:text/html},
}

@book{dumas_fundamentals_2013,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Business} {Process} {Management}},
	isbn = {978-3-642-33142-8 978-3-642-33143-5},
	url = {http://link.springer.com/10.1007/978-3-642-33143-5},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Dumas, Marlon and La Rosa, Marcello and Mendling, Jan and Reijers, Hajo A.},
	year = {2013},
	doi = {10.1007/978-3-642-33143-5},
	file = {Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:/Users/johannesreichle/Zotero/storage/XSJHQVLD/Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:application/pdf},
}

@article{frank_style-and-citation-guide_nodate,
	title = {style-and-citation-guide},
	language = {en},
	author = {Frank, Brigitte},
	pages = {5},
	file = {Frank - style-and-citation-guide.pdf:/Users/johannesreichle/Zotero/storage/8PYU73DA/Frank - style-and-citation-guide.pdf:application/pdf},
}

@book{johannesson_introduction_2021,
	address = {Cham},
	title = {An {Introduction} to {Design} {Science}},
	isbn = {978-3-030-78131-6 978-3-030-78132-3},
	url = {https://link.springer.com/10.1007/978-3-030-78132-3},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer International Publishing},
	author = {Johannesson, Paul and Perjons, Erik},
	year = {2021},
	doi = {10.1007/978-3-030-78132-3},
	file = {Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:/Users/johannesreichle/Zotero/storage/69A2NAR7/Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:application/pdf},
}

@book{cox_translating_2017,
	address = {Berkeley, CA},
	title = {Translating {Statistics} to {Make} {Decisions}: {A} {Guide} for the {Non}-{Statistician}},
	isbn = {978-1-4842-2255-3 978-1-4842-2256-0},
	shorttitle = {Translating {Statistics} to {Make} {Decisions}},
	url = {http://link.springer.com/10.1007/978-1-4842-2256-0},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Apress},
	author = {Cox, Victoria},
	year = {2017},
	doi = {10.1007/978-1-4842-2256-0},
	file = {Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:/Users/johannesreichle/Zotero/storage/S33M4ZEW/Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:application/pdf},
}

@inproceedings{ponti_everything_2017,
	title = {Everything {You} {Wanted} to {Know} about {Deep} {Learning} for {Computer} {Vision} but {Were} {Afraid} to {Ask}},
	doi = {10.1109/SIBGRAPI-T.2017.12},
	abstract = {Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.},
	booktitle = {2017 30th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ponti, Moacir Antonelli and Ribeiro, Leonardo Sampaio Ferraz and Nazare, Tiago Santana and Bui, Tu and Collomosse, John},
	month = oct,
	year = {2017},
	note = {ISSN: 2474-0705},
	keywords = {CNN, Computational modeling, computer vision, Computer vision, deep learning, Gallium nitride, image processing, Image processing, machine learning, Machine learning, Tensile stress},
	pages = {17--41},
	annote = {Extracted Annotations (14/10/2021, 10:29:11)
"Deep Learning methods are currently the stateof-the-art in many Computer Vision and Image Processing problems," (Ponti et al 2017:17)
"This is mainly due to two reasons: the availability of labelled image datasets with millions of images [1], [2], and computer hardware that allowed to speed-up computations." (Ponti et al 2017:17)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/5DFA5TFR/Ponti et al. - 2017 - Everything You Wanted to Know about Deep Learning .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/U9DQFTKT/8250222.html:text/html},
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, artificial intelligence, backpropagation, Computer architecture, convolution neural network, deep neural network architectures, Feature extraction, Feedforward neural networks, Machine learning algorithm, optimization, Recurrent neural networks, supervised and unsupervised learning, Training},
	pages = {53040--53065},
	annote = {Extracted Annotations (14/10/2021, 10:43:39)
"The painstakingly handcrafted feature extractors used in traditional learning, classication, and pattern recognition systems are not scalable for large-sized data sets." (Shrestha and Mahmood 2019:53040)
"Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures." (Shrestha and Mahmood 2019:53040)
"DNN is a type of neural network modeled as a multilayer perceptron (MLP) that is trained with algorithms to learn representations from data sets without any manual design of feature extractors. As the name Deep Learning suggests, it consists of higher or deeper number of processing layers, which contrasts with shallow learning model with fewer layers of units." (Shrestha and Mahmood 2019:53041)},
	annote = {Must read!
Good explanation of math behind CNN, LSTM but also optimization algoritghms (like SGD)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/A5UHX3PQ/Shrestha and Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@book{balas_handbook_2019,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Handbook of {Deep} {Learning} {Applications}},
	volume = {136},
	isbn = {978-3-030-11478-7 978-3-030-11479-4},
	url = {http://link.springer.com/10.1007/978-3-030-11479-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina Emilia and Roy, Sanjiban Sekhar and Sharma, Dharmendra and Samui, Pijush},
	year = {2019},
	doi = {10.1007/978-3-030-11479-4},
	annote = {Used in Introduction for Motivation part-{\textgreater} different Applications (see TOC)},
	file = {Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:/Users/johannesreichle/Zotero/storage/EHB78HRJ/Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:application/pdf},
}

@book{prince_computer_2012,
	title = {Computer {Vision}: {Models}, {Learning}, and {Inference}},
	isbn = {978-1-107-01179-3},
	shorttitle = {Computer {Vision}},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Prince, Simon J. D.},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: PmrICLzHutgC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics},
}

@book{das_machine_2021,
	address = {Cham, Switzerland},
	series = {Studies in computational intelligence},
	title = {Machine learning algorithms for industrial applications},
	isbn = {978-3-030-50640-7},
	language = {en},
	number = {volume 907},
	publisher = {Springer},
	editor = {Das, Santosh Kumar and Das, Shom Prasad and Dey, Nilanjan and Hassanien, Aboul Ella},
	year = {2021},
	file = {Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:/Users/johannesreichle/Zotero/storage/4ABV9L7G/Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:application/pdf},
}

@book{singh_computer_2021,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Computer {Vision} and {Image} {Processing}: 5th {International} {Conference}, {CVIP} 2020, {Prayagraj}, {India}, {December} 4-6, 2020, {Revised} {Selected} {Papers}, {Part} {I}},
	volume = {1376},
	isbn = {9789811610851 9789811610868},
	shorttitle = {Computer {Vision} and {Image} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-1086-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer Singapore},
	editor = {Singh, Satish Kumar and Roy, Partha and Raman, Balasubramanian and Nagabhushan, P.},
	year = {2021},
	doi = {10.1007/978-981-16-1086-8},
	file = {Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:/Users/johannesreichle/Zotero/storage/QEEQCRND/Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:application/pdf},
}

@misc{singh_optical_nodate,
	title = {Optical {Character} {Recognition} {Techniques}: {A} {Survey}},
	shorttitle = {Optical {Character} {Recognition} {Techniques}},
	abstract = {This paper presents a literature review on English OCR techniques. English OCR system is compulsory to convert numerous published books of English into editable computer text files. Latest research in this area has been able to grown some new methodologies to overcome the complexity of English writing style. Still these algorithms have not been tested for complete characters of English Alphabet. Hence, a system is required which can handle all classes of English text and identify characters among these classes.},
	author = {Singh, Sukhpreet},
	annote = {Extracted Annotations (17/10/2021, 21:45:46)
"Optical Character Recognition [1] - [5] is a process that can convert text, present in digital image, to editable text." (Singh :1)
"The next step is to apply an OCR (Optical Character Recognition) process, meaning that the scanned image of each document will be translated into machine process able text" (Singh :1)
"a post-processing step to correct these errors is a very important part of the post-processing chain." (Singh :1)},
	file = {Singh_Optical Character Recognition Techniques.pdf:/Users/johannesreichle/Zotero/storage/67Q5REKF/Singh_Optical Character Recognition Techniques.pdf:application/pdf;Citeseer - Snapshot:/Users/johannesreichle/Zotero/storage/98UBX8JM/download.html:text/html},
}

@article{oyedotun_deep_2015,
	title = {Deep {Learning} in {Character} {Recognition} {Considering} {Pattern} {Invariance} {Constraints}},
	volume = {7},
	issn = {2074904X, 20749058},
	url = {http://www.mecs-press.org/ijisa/ijisa-v7-n7/v7n7-1.html},
	doi = {10.5815/ijisa.2015.07.01},
	abstract = {Character recognition is a field of machine learning that has been under research for several decades. The particular success of neural networks in pattern recognition and therefore character recognition is laudable. Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex. This research work reviews some of the most successful pre-training approaches to initializing deep networks such as stacked auto encoders, and deep belief networks based on achieved error rates. More importantly, this research also parallels investigating the performance of deep networks on some common problems associated with pattern recognition systems such as translational invariance, rotational invariance, scale mismatch, and noise. To achieve this, Yoruba vowel characters databases have been used in this research.},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {International Journal of Intelligent Systems and Applications},
	author = {Oyedotun, Oyebade K. and Olaniyi, Ebenezer O. and Khashman, Adnan},
	month = jun,
	year = {2015},
	pages = {1--10},
	annote = {Extracted Annotations (22/10/2021, 16:59:10)
"Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex." (Oyedotun et al 2015:1)
"Neural networks, conversely, can learn the features of task on which they are designed and trained; they can also adapt to some moderate variations such as noise on the data they have been trained with, hence considered intelligent. The success of neural networks in contrast to other non-intelligent recognition approaches is striking, based on performance, and somewhat ease of design considering the capability of neural networks in approximation any mapping function of inputs to outputs while requiring „least‟ domain specific knowledge for its programming each time it is embedded in different applications. i.e. self-programming." (Oyedotun et al 2015:1)
"consideration on the amount of common pattern invariance achievable in learning has also changed; such common invariances include relatively moderate translation, rotation, scale mismatch, and noisy patterns." (Oyedotun et al 2015:2)
"Conversely, this paper takes an alternative approach, by presenting a work which focuses on invariance learning based on the neural network architectures, rather than complex training data manipulation schemes and painstaking mathematical foundations. It is noteworthy that this research did not employ any invariant feature extraction technique; hence, investigates how neural network structures and learning paradigms affect invariance learning. Also, to reinforce the application importance of this work, real life data, „handwritten characters‟, have been used to train and simulate the considered networks." (Oyedotun et al 2015:3)
"Unfortunately, the difficulty is to synthesize, and then to efficiently compute, the classification function that maps objects to categories, given that objects in a category can have widely varying input representations [7]; bearing in mind also that this approach increases computational load on the designed system. A better approach is to consider neural network architectures that allow some level of built-in invariance due to structure; and of course, this can usually still be augmented with some handcrafted invariance achieved through data manipulation schemes." (Oyedotun et al 2015:3)
"Such features include:  distributed representation of knowledge at each hidden layer.  distinct features are extracted by units or neurons in each hidden layer.  several units can be active concurrently." (Oyedotun et al 2015:3)
"Generally, it is conceived that in deep networks, the first hidden layer extracts some primary features about the input, then these features are combined in the second layer to more defined features, and these features are further combined into well more defined features in the following layers, and so on. This can be somewhat seen as a hierarchical representation of knowledge;" (Oyedotun et al 2015:3)
"Deep learning depicts neural network architectures of more than a single hidden layer (multilayer networks); in contrast to networks of single hidden layer which are commonly referred to as shallow networks." (Oyedotun et al 2015:3)
"Generative Architectures This class of deep networks is not required to be deterministic of the class patterns that the inputs belong, but is used to sample joint statistical distribution of data; moreover this class of networks relies on unsupervised learning." (Oyedotun et al 2015:4)
"Discriminative Architectures Discriminative deep networks actually are required to be deterministic of the correlation of input data to the classes of patterns therein. Moreover, this category of networks relies on supervised learning." (Oyedotun et al 2015:4)
"Hybrid Architectures Networks that belong to this class rely on the combination of generative and discriminative approach in their architectures. Generally, such networks are generatively pre-trained and then discriminately finetuned for deterministic purposes." (Oyedotun et al 2015:4)
"The application of auto encoders and therefore generative architectures leverage on the unavailability of labelled data or the required logistics and cost that may be necessary in labeling available data. It therefore follows that generative learning suffices in situations where we have large unlabelled data and small labelled data" (Oyedotun et al 2015:4)
"The auto encoder can be seen as an encoder-decoder system, where the encoder (input-hidden layer pair) receives the input, extracting essential features for reconstruction; while the decoder (hidden-output layer pair) part receives the features extracted from the hidden layer, performing reconstruction at its best." (Oyedotun et al 2015:4)
"The training approach that is used in achieving learning in generative network architectures is known as „greedy layer-wise pre-training‟." (Oyedotun et al 2015:5)
"A DBN is a deep network, which is graphical and probabilistic in nature; it is essentially a generative model too. A belief net is a directed acyclic graph composed of stochastic variables [19]." (Oyedotun et al 2015:5)
"Such a training scheme is aimed at maximizing the likelihood of the input vector at a layer below given a configuration of a hidden layer that is directly on top of it." (Oyedotun et al 2015:6)
"A restricted Boltzmann machine has only two layers (fig.8); the input (visible) and the hidden layer. The connections between the two layers are undirected, and there are no interconnections between units of the same layer as in the general Boltzmann machine. We can therefore say that from the restriction in interconnections of units in layers, units are conditionally independent. The RBM can be seen as a Markov network, where the visible layer consists of either Bernoulli (binary) or Gaussian (real values usually between from 0 to 1) stochastic units, and the hidden layer of stochastic Bernoulli units [22]." (Oyedotun et al 2015:6)
"The simulation results on the considered invariances for the different trained networks are shown in table 3. It can be seen that the SDAE has the lowest error rate on translation, while the DBN outperformed other networks on rotational and scale invariances." (Oyedotun et al 2015:8)
"hence we can conjure that these networks were able to explore a more complex space of solutions while learning to the deep nature; since hierarchical learning allows more distributed knowledge representation." (Oyedotun et al 2015:8)
"It will be seen that the deep belief networks on the average, performed best compared to the other networks on variances like translation, rotation and scale mismatch; while its tolerance to noise decreased noticeably as the level of noise was increased as shown in table 4, table 5, and fig.15." (Oyedotun et al 2015:9)
"These variances are common constraints that occur in real life recognition systems for handwritten characters, and some of the solutions have been constraining the users (writers) to some particular possible domains of writing spaces or earmarked pattern of writing in order for low error rates to be achieved." (Oyedotun et al 2015:9)
"It has been shown that another flavour of neural networks, "convolutional networks" and its deep variant give very motivating performance on some of these constraints [25], however the complexity of these networks is somewhat obvious." (Oyedotun et al 2015:9)},
	file = {Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:/Users/johannesreichle/Zotero/storage/IWBWRJCE/Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:application/pdf},
}

@inproceedings{zhao_improving_2020,
	title = {Improving {Deep} {Learning} based {Optical} {Character} {Recognition} via {Neural} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185798},
	abstract = {Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment. We compared it with different methods, and the experimental results proved the effectiveness of our method.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Zhao, Zhenyao and Jiang, Min and Guo, Shihui and Wang, Zhenzhong and Chao, Fei and Tan, Kay Chen},
	month = jul,
	year = {2020},
	keywords = {Computer architecture, Feature extraction, Object detection, Optical character recognition software, Task analysis, Text recognition, Training},
	pages = {1--7},
	annote = {Extracted Annotations (17/10/2021, 23:42:47)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)},
	annote = {Extracted Annotations (22/10/2021, 12:48:20)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)
"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)
"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)
"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)
"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)
"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)
"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)
"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	file = {Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:/Users/johannesreichle/Zotero/storage/SHJA8U7A/Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/G9CV8S98/9185798.html:text/html},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-10-19},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {RetinaNet},
	file = {Lin et al_2018_Focal Loss for Dense Object Detection.pdf:/Users/johannesreichle/Zotero/storage/3NLWH9LE/Lin et al_2018_Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8CJ43KHN/1708.html:text/html},
}

@misc{beom_crnn_2021-1,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-10-19},
	author = {Beom},
	month = oct,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
}

@article{liao_textboxes_2018,
	title = {{TextBoxes}++: {A} {Single}-{Shot} {Oriented} {Scene} {Text} {Detector}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{TextBoxes}++},
	url = {http://arxiv.org/abs/1801.02765},
	doi = {10.1109/TIP.2018.2825107},
	abstract = {Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes\_plusplus},
	number = {8},
	urldate = {2021-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.02765},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3676--3690},
	annote = {Comment: 15 pages},
	file = {Liao et al_2018_TextBoxes++.pdf:/Users/johannesreichle/Zotero/storage/BYY77N9D/Liao et al_2018_TextBoxes++.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NPE7JJW5/1801.html:text/html},
}

@article{shi_end--end_2015,
	title = {An {End}-to-{End} {Trainable} {Neural} {Network} for {Image}-based {Sequence} {Recognition} and {Its} {Application} to {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1507.05717},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	urldate = {2021-10-19},
	journal = {arXiv:1507.05717 [cs]},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.05717},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 figures},
	file = {Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:/Users/johannesreichle/Zotero/storage/8PJQ4H5U/Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NAI47ZEJ/1507.html:text/html},
}

@misc{noauthor_search_nodate,
	title = {Search · · argman/{EAST}},
	url = {https://github.com/argman/EAST},
	abstract = {A tensorflow implementation of EAST text detector. Contribute to argman/EAST development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {Tensorflow EAST implementation},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/DSDTHJ9A/EAST.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {SakuraRiven}/{EAST} at pythonrepo.com},
	url = {https://github.com/SakuraRiven/EAST},
	abstract = {PyTorch Re-Implementation of EAST: An Efficient and Accurate Scene Text Detector - GitHub - SakuraRiven/EAST at pythonrepo.com},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {EAST with PyTorch},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/2XJ788B8/EAST.html:text/html},
}

@misc{noauthor_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Arbitrary}-{Shaped} {Text} ({RRC}-{ArT})},
	url = {https://deepai.org/publication/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art},
	abstract = {09/16/19 - This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped
Text (RRC-ArT) that consists of three major challeng...},
	urldate = {2021-10-19},
	journal = {DeepAI},
	month = sep,
	year = {2019},
	annote = {Challenge that goes in the right direction -{\textgreater} read},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/R524XQNK/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art.html:text/html},
}

@inproceedings{jenckel_anyocr_2016,
	address = {Cancun},
	title = {{anyOCR}: {A} sequence learning based {OCR} system for unlabeled historical documents},
	isbn = {978-1-5090-4847-2},
	shorttitle = {{anyOCR}},
	url = {http://ieeexplore.ieee.org/document/7900265/},
	doi = {10.1109/ICPR.2016.7900265},
	abstract = {Institutes and libraries around the globe are preserving the literary heritage by digitizing historical documents. However, to make this data easily accessible the scanned documents need to be transformed into search-able text. State of the art OCR systems using Long-Short-Term-Memory networks (LSTM) have been applied successfully to recognize text in both printed and handwritten form. Besides the general challenges with historical documents, e.g. poor image quality, damaged characters, etc., especially unknown scripts and old fonds make it difficult to provide the large amount of transcribed training data required for these methods to perform well. Transcribing the documents manually is very costly in terms of manhours and require language specific expertise. The unknown fonds and requirement for meaningful context also make the use of synthetic data unfeasible.},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Jenckel, Martin and Bukhari, Syed Saqib and Dengel, Andreas},
	month = dec,
	year = {2016},
	pages = {4035--4040},
	file = {Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:/Users/johannesreichle/Zotero/storage/YDA4CDN8/Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:application/pdf},
}

@misc{noauthor_few-shot_2020,
	title = {Few-{Shot} {Learning} ({FSL}): {What} it is \& its {Applications}},
	shorttitle = {Few-{Shot} {Learning} ({FSL})},
	url = {https://research.aimultiple.com/few-shot-learning/},
	abstract = {Few-shot/low-shot learning definition, importance, use cases, approaches, implementations in Python \& differences between zero-shot learning},
	language = {en-US},
	urldate = {2021-10-27},
	month = nov,
	year = {2020},
	annote = {Great source for starting to learn about few shot learning -{\textgreater} basic stuff, overview, follow up sources!},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/P73M3WXD/few-shot-learning.html:text/html},
}

@misc{noauthor_cross-domain_nodate,
	title = {Cross-{Domain} {Few}-{Shot} {Learning} for {Mobile} {OCR} {\textbar} {Anyline}},
	url = {https://anyline.com/news/cross-domain-few-shot-learning-mobile-ocr},
	abstract = {Find out the latest progress from our research collaboration with AI Lab of JKU Linz, on the potential of the 'CHEF' Few-Shot learning method.},
	language = {en},
	urldate = {2021-10-29},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/LUSIBD9S/cross-domain-few-shot-learning-mobile-ocr.html:text/html},
}

@article{srivastava_comparative_2021,
	title = {Comparative analysis of deep learning image detection algorithms},
	volume = {8},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00434-w},
	doi = {10.1186/s40537-021-00434-w},
	abstract = {A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Journal of Big Data},
	author = {Srivastava, Shrey and Divekar, Amit Vishvas and Anilkumar, Chandu and Naik, Ishika and Kulkarni, Ved and Pattabiraman, V.},
	month = dec,
	year = {2021},
	pages = {66},
	file = {Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:/Users/johannesreichle/Zotero/storage/2EQVX9L2/Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:application/pdf},
}

@article{chen_generative_nodate,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	pages = {12},
	file = {Chen et al. - Generative Pretraining from Pixels.pdf:/Users/johannesreichle/Zotero/storage/A235GR5L/Chen et al. - Generative Pretraining from Pixels.pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Wolf et al_2020_Transformers.pdf:/Users/johannesreichle/Zotero/storage/WU9PCJ4B/Wolf et al_2020_Transformers.pdf:application/pdf},
}

@inproceedings{vicol_unbiased_2021,
	title = {Unbiased {Gradient} {Estimation} in {Unrolled} {Computation} {Graphs} with {Persistent} {Evolution} {Strategies}},
	url = {https://proceedings.mlr.press/v139/vicol21a.html},
	abstract = {Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.},
	language = {en},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10553--10563},
	file = {Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:/Users/johannesreichle/Zotero/storage/9NR7ES72/Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:application/pdf;Supplementary PDF:/Users/johannesreichle/Zotero/storage/6FXHTMRX/Vicol et al. - 2021 - Unbiased Gradient Estimation in Unrolled Computati.pdf:application/pdf},
}

@article{yang_learning_2021,
	title = {Learning {High}-{Precision} {Bounding} {Box} for {Rotated} {Object} {Detection} via {Kullback}-{Leibler} {Divergence}},
	url = {http://arxiv.org/abs/2106.01883},
	abstract = {Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios. Taking the perspective that horizontal detection is a special case for rotated object detection, in this paper, we are motivated to change the design of rotation regression loss from induction paradigm to deduction methodology, in terms of the relation between rotation and horizontal detection. We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio. This mechanism can be vital for high-precision detection as a slight angle error would cause a serious accuracy drop for large aspect ratios objects. More importantly, we have proved that KLD is scale invariant. We further show that the KLD loss can be degenerated into the popular \$l\_\{n\}\$-norm loss for horizontal detection. Experimental results on seven datasets using different detectors show its consistent superiority, and codes are available at https://github.com/yangxue0827/RotationDetection.},
	urldate = {2021-11-02},
	journal = {arXiv:2106.01883 [cs]},
	author = {Yang, Xue and Yang, Xiaojiang and Yang, Jirui and Ming, Qi and Wang, Wentao and Tian, Qi and Yan, Junchi},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01883},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 16 pages, 5 figures, 8 tables, accepted by NeurIPS21, codes are available at https://github.com/yangxue0827/RotationDetection},
	annote = {Extracted Annotations (04/11/2021, 18:11:57)
"Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios." (Yang et al 2021:1)
"We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio." (Yang et al 2021:1)
"In this paper, we take a step back, and aim to develop (from a deductive perspective) a unified regression framework for rotation detection and its special case: horizontal detection. In fact, our new framework enjoys a coherent property that it can be degenerated into the current commonly used regression loss (e.g. ln -norm) in special cases (horizontal detection), as shown in Figure 1(b)." (Yang et al 2021:2)
"The mainstream classical object detection algorithms can be roughly divided according to the following standards: Two- [7, 8, 9, 11] or Single-stage [10, 18, 19] object detection, Anchor-free [20, 21, 22] or Anchor-based [8, 9, 10] object detection and CNN [8, 10, 20] or Transformer-based [23, 24] object detection. Although the pipelines may vary, the mainstream regression loss often uses the popular ln -norm loss (such as smooth L1 loss) or IoU-based loss" (Yang et al 2021:3)
"However, horizontal detectors do not provide accurate orientation and scale information." (Yang et al 2021:3)
"The overall regression loss for rotation detection is: Lreg = ln -norm (tx ; ty ; tw ; th ; t )" (Yang et al 2021:4)
"It can be seen that parameters are optimized independently, making the loss (or detection accuracy) sensitive to the under-fitting of any of the parameters. This mechanism is fatal to high-precision detection." (Yang et al 2021:4)
text has long aspect ratios -{\textgreater} angle parameter very important -{\textgreater} KLD approach is favored (note on p.4)
 
"Although GWD scheme has played a preliminary exploration of the deductive paradigm, it does not focus on achieving high-precision detection and scale invariance. In the following, we will propose our new approach based on the Kullback-Leibler divergence (KLD)" (Yang et al 2021:5)
Gaussians are constructed like with GWD scheme (note on p.5)
 
"ICDAR2015, MLT and MSRA-TD500 are commonly used for oriented scene text detection and spotting. ICDAR2015 includes 1,000 training images and 500 testing images. ICDAR2017 MLT is a multi-lingual text dataset, which includes 7,200 training images, 1,800 validation images and 9,000 testing images. MSRA-TD500 dataset consists of 300 training images and 200 testing images." (Yang et al 2021:7)
"Limitations. Despite the theoretical grounds and the promising experimental justifications, our method has an obvious limitation that it cannot be directly applied to quadrilateral detection [33, 44]." (Yang et al 2021:10)},
	file = {Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:/Users/johannesreichle/Zotero/storage/JF2GVMIQ/Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/H7Q9M3ZH/2106.html:text/html},
}

@article{xu_dp-ssl_2021,
	title = {{DP}-{SSL}: {Towards} {Robust} {Semi}-supervised {Learning} with {A} {Few} {Labeled} {Samples}},
	shorttitle = {{DP}-{SSL}},
	url = {http://arxiv.org/abs/2110.13740},
	abstract = {The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning{\textasciitilde}(MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82\% annotation accuracy on unlabeled data and 93.46\% classification accuracy on test data, which are higher than the SOTA results.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.13740 [cs]},
	author = {Xu, Yi and Ding, Jiandong and Zhang, Lu and Zhou, Shuigeng},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13740},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021; 16 pages with appendix},
	file = {Xu et al_2021_DP-SSL.pdf:/Users/johannesreichle/Zotero/storage/7RVZR9N7/Xu et al_2021_DP-SSL.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XLYQEFMP/2110.html:text/html},
}

@article{lu_soft_2021,
	title = {{SOFT}: {Softmax}-free {Transformer} with {Linear} {Complexity}},
	shorttitle = {{SOFT}},
	url = {http://arxiv.org/abs/2110.11945},
	abstract = {Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.11945 [cs]},
	author = {Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11945},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021 Spotlight. Project page at https://fudan-zvg.github.io/SOFT/},
	file = {Lu et al_2021_SOFT.pdf:/Users/johannesreichle/Zotero/storage/W6LD7EJZ/Lu et al_2021_SOFT.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/ZXCCUE46/2110.html:text/html},
}

@inproceedings{raisi_transformer-based_2021,
	address = {Nashville, TN, USA},
	title = {Transformer-based {Text} {Detection} in the {Wild}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522851/},
	doi = {10.1109/CVPRW53098.2021.00353},
	abstract = {A major limitation to most state-of-the-art visual localization methods is their ineptitude to make use of ubiquitous signs and directions that are typically intuitive to humans. Localization methods can greatly beneﬁt from a system capable of reasoning about a variety of cues beyond low-level features, such as street signs, store names, building directories, room numbers, etc.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Raisi, Zobeir and Naiel, Mohamed A. and Younes, Georges and Wardell, Steven and Zelek, John S.},
	month = jun,
	year = {2021},
	pages = {3156--3165},
	file = {Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:/Users/johannesreichle/Zotero/storage/Y6FVVXIN/Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:application/pdf},
}

@article{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2021-11-05},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Carion et al_2020_End-to-End Object Detection with Transformers.pdf:/Users/johannesreichle/Zotero/storage/3RYU4T68/Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/GBXATMTN/2005.html:text/html},
}

@misc{noauthor_accepted_nodate,
	title = {Accepted {Papers}},
	url = {https://nips.cc/Conferences/2021/AcceptedPapersInitial},
	urldate = {2021-11-05},
	file = {Accepted Papers:/Users/johannesreichle/Zotero/storage/BQ5SUEPT/AcceptedPapersInitial.html:text/html},
}

@article{zhang_fast_2021,
	title = {Fast {Multi}-{Resolution} {Transformer} {Fine}-tuning for {Extreme} {Multi}-label {Text} {Classification}},
	url = {http://arxiv.org/abs/2110.00685},
	abstract = {Extreme multi-label text classification (XMC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown significant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the fine-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively fine-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes significantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51\% to 54\%.},
	urldate = {2021-11-06},
	journal = {arXiv:2110.00685 [cs, stat]},
	author = {Zhang, Jiong and Chang, Wei-cheng and Yu, Hsiang-fu and Dhillon, Inderjit S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.00685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:/Users/johannesreichle/Zotero/storage/FXMXFQ9J/Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XNYYFCCC/2110.html:text/html},
}

@article{sheng_centripetaltext_2021,
	title = {{CentripetalText}: {An} {Efficient} {Text} {Instance} {Representation} for {Scene} {Text} {Detection}},
	shorttitle = {{CentripetalText}},
	url = {http://arxiv.org/abs/2107.05945},
	abstract = {Scene text detection remains a grand challenge due to the variation in text curvatures, orientations, and aspect ratios. One of the hardest problems in this task is how to represent text instances of arbitrary shapes. Although many methods have been proposed to model irregular texts in a flexible manner, most of them lose simplicity and robustness. Their complicated post-processings and the regression under Dirac delta distribution undermine the detection performance and the generalization ability. In this paper, we propose an efficient text instance representation named CentripetalText (CT), which decomposes text instances into the combination of text kernels and centripetal shifts. Specifically, we utilize the centripetal shifts to implement pixel aggregation, guiding the external text pixels to the internal text kernels. The relaxation operation is integrated into the dense regression for centripetal shifts, allowing the correct prediction in a range instead of a specific value. The convenient reconstruction of text contours and the tolerance of prediction errors in our method guarantee the high detection accuracy and the fast inference speed, respectively. Besides, we shrink our text detector into a proposal generation module, namely CentripetalText Proposal Network, replacing Segmentation Proposal Network in Mask TextSpotter v3 and producing more accurate proposals. To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets. For the task of scene text detection, our approach achieves superior or competitive performance compared to other existing methods, e.g., F-measure of 86.3\% at 40.0 FPS on Total-Text, F-measure of 86.1\% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text recognition, our method outperforms Mask TextSpotter v3 by 1.1\% on Total-Text.},
	urldate = {2021-11-06},
	journal = {arXiv:2107.05945 [cs]},
	author = {Sheng, Tao and Chen, Jie and Lian, Zhouhui},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.05945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {Sheng et al_2021_CentripetalText.pdf:/Users/johannesreichle/Zotero/storage/GAFQN94L/Sheng et al_2021_CentripetalText.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HZFSY2GG/2107.html:text/html},
}

@article{shen_you_2021,
	title = {You {Never} {Cluster} {Alone}},
	url = {http://arxiv.org/abs/2106.01908},
	abstract = {Recent advances in self-supervised learning with instance-level contrastive objectives facilitate unsupervised clustering. However, a standalone datum is not perceiving the context of the holistic cluster, and may undergo sub-optimal assignment. In this paper, we extend the mainstream contrastive learning paradigm to a cluster-level scheme, where all the data subjected to the same cluster contribute to a unified representation that encodes the context of each data group. Contrastive learning with this representation then rewards the assignment of each datum. To implement this vision, we propose twin-contrast clustering (TCC). We define a set of categorical variables as clustering assignment confidence, which links the instance-level learning track with the cluster-level one. On one hand, with the corresponding assignment variables being the weight, a weighted aggregation along the data points implements the set representation of a cluster. We further propose heuristic cluster augmentation equivalents to enable cluster-level contrastive learning. On the other hand, we derive the evidence lower-bound of the instance-level contrastive objective with the assignments. By reparametrizing the assignment variables, TCC is trained end-to-end, requiring no alternating steps. Extensive experiments show that TCC outperforms the state-of-the-art on challenging benchmarks.},
	urldate = {2021-11-06},
	journal = {arXiv:2106.01908 [cs]},
	author = {Shen, Yuming and Shen, Ziyi and Wang, Menghan and Qin, Jie and Torr, Philip H. S. and Shao, Ling},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01908},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021},
	file = {Shen et al_2021_You Never Cluster Alone.pdf:/Users/johannesreichle/Zotero/storage/SJ7NZPJZ/Shen et al_2021_You Never Cluster Alone.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/3JBHRY6C/2106.html:text/html},
}

@article{ma_arbitrary-oriented_2018,
	title = {Arbitrary-{Oriented} {Scene} {Text} {Detection} via {Rotation} {Proposals}},
	volume = {20},
	issn = {1941-0077},
	doi = {10.1109/TMM.2018.2818020},
	abstract = {This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks, which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.},
	number = {11},
	journal = {IEEE Transactions on Multimedia},
	author = {Ma, Jianqi and Shao, Weiyuan and Ye, Hao and Wang, Li and Wang, Hong and Zheng, Yingbin and Xue, Xiangyang},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {arbitrary oriented, Computer architecture, Image edge detection, Microsoft Windows, Pipelines, Proposals, Robustness, rotation proposals, Scene text detection, Task analysis},
	pages = {3111--3122},
	file = {Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:/Users/johannesreichle/Zotero/storage/MUTNDE2W/Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/N8FNCBBL/8323240.html:text/html},
}

@misc{goswami_deeper_2018,
	title = {A deeper look at how {Faster}-{RCNN} works},
	url = {https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd},
	abstract = {Faster-RCNN is one of the most well known object detection neural networks [1,2]. It is also the basis for many derived networks for…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = jul,
	year = {2018},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/FCB2UPLV/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-11-06},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick_2015_Fast R-CNN.pdf:/Users/johannesreichle/Zotero/storage/WHTYMPGV/Girshick_2015_Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/X3L8V8HR/1504.html:text/html},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-11-06},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {Ren et al_2016_Faster R-CNN.pdf:/Users/johannesreichle/Zotero/storage/G6DDTA5M/Ren et al_2016_Faster R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/DRXQUEDA/1506.html:text/html},
}

@misc{goswami_comparison_2020,
	title = {Comparison of {Faster}-{RCNN} and {Detection} {Transformer} ({DETR})},
	url = {https://whatdhack.medium.com/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04},
	abstract = {Faster-RCNN is a well known network, arguably the gold standard, in object detection and segmentation. Detection Transformer ( DETR) on…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/RAFM6M6L/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04.html:text/html},
}

@misc{noauthor_deep_2021,
	title = {Deep {Learning} {Based} {OCR} for {Text} in the {Wild}},
	url = {https://nanonets.com/blog/deep-learning-ocr/},
	abstract = {Learn how to apply deep learning based OCR to recognize and extract unstructured text information from images using Tesseract and the OpenCV EAST engine.},
	language = {en},
	urldate = {2021-11-06},
	journal = {AI \& Machine Learning Blog},
	month = jun,
	year = {2021},
	annote = {good overview of techniques
has implementation (EAST coupled with tesseract) -{\textgreater} try how it works on data set},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/27F4KTAC/deep-learning-ocr.html:text/html},
}
