
@misc{beom_text_2021,
	title = {Text {Detector} for {OCR}},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/Text_Detector},
	abstract = {Text detection model that combines Retinanet with textboxes++ for OCR},
	urldate = {2021-09-18},
	author = {Beom},
	month = aug,
	year = {2021},
	note = {original-date: 2019-03-12T05:11:06Z},
}

@misc{beom_crnn_2021,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-09-18},
	author = {Beom},
	month = sep,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
	annote = {CRNN implementation (Keras)},
}

@misc{bhat_rajesh-bhatspark-ai-summit-2020-text-extraction_2021,
	title = {rajesh-bhat/spark-ai-summit-2020-text-extraction},
	copyright = {MIT},
	url = {https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/43eeb1f1a27e6ae84dcb0ef4cc11494dcc61cafb/CRNN_CTC_wandb.ipynb},
	urldate = {2021-09-22},
	author = {Bhat, Rajesh Shreedhar},
	month = aug,
	year = {2021},
	note = {original-date: 2020-06-02T14:21:10Z},
}

@inproceedings{chen_improvement_2018,
	address = {Shanghai, China},
	title = {Improvement {Research} and {Application} of {Text} {Recognition} {Algorithm} {Based} on {CRNN}},
	isbn = {978-1-4503-6605-2},
	url = {http://dl.acm.org/citation.cfm?doid=3297067.3297073},
	doi = {10.1145/3297067.3297073},
	abstract = {This paper is based on CRNN model to recognize the text in the images of football matches scene, and two improvements are proposed. Considering the edge feature of text is strong, this paper adds MFM layers into CRNN model aiming to enhance the contrast. In order to solve the problem of losing details of image static features in the process of getting contextual features, this paper fuses up these two kinds of features. The training and testing experiments carried out on public dataset and manual dataset respectively verify the validity of the improvements, and the recognition accurate rate is higher than original model.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Signal} {Processing} and {Machine} {Learning} - {SPML} '18},
	publisher = {ACM Press},
	author = {Chen, Lei and Li, Shaobin},
	year = {2018},
	pages = {166--170},
	file = {Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:/Users/johannesreichle/Zotero/storage/4JUQTVGL/Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:application/pdf},
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2021-09-24},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/M7XHZX79/backpropagation-in-convolutional-neural-networks.html:text/html},
}

@misc{shperber_gentle_2021,
	title = {A gentle introduction to {OCR}},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa},
	abstract = {How and why to apply deep learning to Optical Character Recognition},
	language = {en},
	urldate = {2021-09-18},
	journal = {Medium},
	author = {Shperber, Gidi},
	month = feb,
	year = {2021},
	annote = {Has a lot of approaches that can be checked},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/JSRM93VX/a-gentle-introduction-to-ocr-ee1469a201aa.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/PFLR88V5/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TSDHL78C/4376991.html:text/html},
}

@article{zhou_east_2017,
	title = {{EAST}: {An} {Efficient} and {Accurate} {Scene} {Text} {Detector}},
	shorttitle = {{EAST}},
	url = {http://arxiv.org/abs/1704.03155},
	abstract = {Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.},
	urldate = {2021-09-18},
	journal = {arXiv:1704.03155 [cs]},
	author = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
	month = jul,
	year = {2017},
	note = {arXiv: 1704.03155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2017, fix equation (3)},
	file = {arXiv Fulltext PDF:/Users/johannesreichle/Zotero/storage/GXDSSU9P/Zhou et al. - 2017 - EAST An Efficient and Accurate Scene Text Detecto.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/2VF6VH7T/1704.html:text/html},
}

@misc{bhat_text_nodate,
	title = {Text {Recognition} {With} {CRNN}-{CTC} {Network} - {Weights} \& {Biases}},
	url = {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2021-09-22},
	journal = {W\&B},
	author = {Bhat, Rajesh Shreedhar},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/9Q9S8C2K/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI.html:text/html},
}

@article{hochreiter_long_nodate,
	title = {Long {Short} {Term} {Memory}},
	abstract = {Recurrent backprop" for learning to store information over extended time periods takes too long. The main reason is insufficient, decaying error back flow. We describe a novel, efficient "Long Short Term Memory" (LSTM) that overcomes this and related problems. Unlike previous approaches, LSTM can learn to bridge arbitmry time lags by enforcing constant error flow. Using gradient descent, LSTM explicitly learns when to store information and when to access it. In experimental comparisons with "Real-T ime Recurrent Learning", "Recurrent Cascade-Correlation", "Elman nets", and "Neural Sequence Chunking", LSTM leads to many more successful runs, and learns much faster. Unlike its competitors, LSTM can solve tasks involving minimal time lags of more than 1000 time steps, even in noisy environments.},
	language = {en},
	author = {Hochreiter, Sepp and Schmidhuber, Jiirgen},
	pages = {12},
	file = {Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:/Users/johannesreichle/Zotero/storage/VPLXUVMT/Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:application/pdf},
}

@inproceedings{kloss_learning_2016,
	address = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	language = {en},
	urldate = {2021-09-26},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	month = oct,
	year = {2016},
	pages = {5238--5245},
	file = {Kloss et al. - 2016 - Learning where to search using visual attention.pdf:/Users/johannesreichle/Zotero/storage/VJ2CKK4W/Kloss et al. - 2016 - Learning where to search using visual attention.pdf:application/pdf},
}

@misc{odegua_how_2020,
	title = {How to put machine learning models into production},
	url = {https://stackoverflow.blog/2020/10/12/how-to-put-machine-learning-models-into-production/},
	abstract = {The goal of building a machine learning model is to solve a problem, and a machine learning model can only do so when it is in production and actively in use by consumers. As such, model deployment is as important as model building.},
	language = {en-US},
	urldate = {2021-09-30},
	journal = {Stack Overflow Blog},
	author = {Odegua, Rising},
	month = oct,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/ZG37NKW8/how-to-put-machine-learning-models-into-production.html:text/html},
}

@book{dumas_fundamentals_2013,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Business} {Process} {Management}},
	isbn = {978-3-642-33142-8 978-3-642-33143-5},
	url = {http://link.springer.com/10.1007/978-3-642-33143-5},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Dumas, Marlon and La Rosa, Marcello and Mendling, Jan and Reijers, Hajo A.},
	year = {2013},
	doi = {10.1007/978-3-642-33143-5},
	file = {Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:/Users/johannesreichle/Zotero/storage/XSJHQVLD/Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:application/pdf},
}

@article{frank_style-and-citation-guide_nodate,
	title = {style-and-citation-guide},
	language = {en},
	author = {Frank, Brigitte},
	pages = {5},
	file = {Frank - style-and-citation-guide.pdf:/Users/johannesreichle/Zotero/storage/8PYU73DA/Frank - style-and-citation-guide.pdf:application/pdf},
}

@book{johannesson_introduction_2021,
	address = {Cham},
	title = {An {Introduction} to {Design} {Science}},
	isbn = {978-3-030-78131-6 978-3-030-78132-3},
	url = {https://link.springer.com/10.1007/978-3-030-78132-3},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer International Publishing},
	author = {Johannesson, Paul and Perjons, Erik},
	year = {2021},
	doi = {10.1007/978-3-030-78132-3},
	file = {Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:/Users/johannesreichle/Zotero/storage/69A2NAR7/Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:application/pdf},
}

@book{cox_translating_2017,
	address = {Berkeley, CA},
	title = {Translating {Statistics} to {Make} {Decisions}: {A} {Guide} for the {Non}-{Statistician}},
	isbn = {978-1-4842-2255-3 978-1-4842-2256-0},
	shorttitle = {Translating {Statistics} to {Make} {Decisions}},
	url = {http://link.springer.com/10.1007/978-1-4842-2256-0},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Apress},
	author = {Cox, Victoria},
	year = {2017},
	doi = {10.1007/978-1-4842-2256-0},
	file = {Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:/Users/johannesreichle/Zotero/storage/S33M4ZEW/Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:application/pdf},
}

@inproceedings{ponti_everything_2017,
	title = {Everything {You} {Wanted} to {Know} about {Deep} {Learning} for {Computer} {Vision} but {Were} {Afraid} to {Ask}},
	doi = {10.1109/SIBGRAPI-T.2017.12},
	abstract = {Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.},
	booktitle = {2017 30th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ponti, Moacir Antonelli and Ribeiro, Leonardo Sampaio Ferraz and Nazare, Tiago Santana and Bui, Tu and Collomosse, John},
	month = oct,
	year = {2017},
	note = {ISSN: 2474-0705},
	keywords = {CNN, Computational modeling, computer vision, Computer vision, deep learning, Gallium nitride, image processing, Image processing, machine learning, Machine learning, Tensile stress},
	pages = {17--41},
	annote = {Extracted Annotations (14/10/2021, 10:29:11)
"Deep Learning methods are currently the stateof-the-art in many Computer Vision and Image Processing problems," (Ponti et al 2017:17)
"This is mainly due to two reasons: the availability of labelled image datasets with millions of images [1], [2], and computer hardware that allowed to speed-up computations." (Ponti et al 2017:17)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/5DFA5TFR/Ponti et al. - 2017 - Everything You Wanted to Know about Deep Learning .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/U9DQFTKT/8250222.html:text/html},
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, artificial intelligence, backpropagation, Computer architecture, convolution neural network, deep neural network architectures, Feature extraction, Feedforward neural networks, Machine learning algorithm, optimization, Recurrent neural networks, supervised and unsupervised learning, Training},
	pages = {53040--53065},
	annote = {Extracted Annotations (14/10/2021, 10:43:39)
"The painstakingly handcrafted feature extractors used in traditional learning, classication, and pattern recognition systems are not scalable for large-sized data sets." (Shrestha and Mahmood 2019:53040)
"Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures." (Shrestha and Mahmood 2019:53040)
"DNN is a type of neural network modeled as a multilayer perceptron (MLP) that is trained with algorithms to learn representations from data sets without any manual design of feature extractors. As the name Deep Learning suggests, it consists of higher or deeper number of processing layers, which contrasts with shallow learning model with fewer layers of units." (Shrestha and Mahmood 2019:53041)},
	annote = {Must read!
Good explanation of math behind CNN, LSTM but also optimization algoritghms (like SGD)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/A5UHX3PQ/Shrestha and Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@book{balas_handbook_2019,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Handbook of {Deep} {Learning} {Applications}},
	volume = {136},
	isbn = {978-3-030-11478-7 978-3-030-11479-4},
	url = {http://link.springer.com/10.1007/978-3-030-11479-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina Emilia and Roy, Sanjiban Sekhar and Sharma, Dharmendra and Samui, Pijush},
	year = {2019},
	doi = {10.1007/978-3-030-11479-4},
	annote = {Used in Introduction for Motivation part-{\textgreater} different Applications (see TOC)},
	file = {Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:/Users/johannesreichle/Zotero/storage/EHB78HRJ/Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:application/pdf},
}

@book{prince_computer_2012,
	title = {Computer {Vision}: {Models}, {Learning}, and {Inference}},
	isbn = {978-1-107-01179-3},
	shorttitle = {Computer {Vision}},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Prince, Simon J. D.},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: PmrICLzHutgC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics},
}

@book{das_machine_2021,
	address = {Cham, Switzerland},
	series = {Studies in computational intelligence},
	title = {Machine learning algorithms for industrial applications},
	isbn = {978-3-030-50640-7},
	language = {en},
	number = {volume 907},
	publisher = {Springer},
	editor = {Das, Santosh Kumar and Das, Shom Prasad and Dey, Nilanjan and Hassanien, Aboul Ella},
	year = {2021},
	file = {Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:/Users/johannesreichle/Zotero/storage/4ABV9L7G/Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:application/pdf},
}

@book{singh_computer_2021,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Computer {Vision} and {Image} {Processing}: 5th {International} {Conference}, {CVIP} 2020, {Prayagraj}, {India}, {December} 4-6, 2020, {Revised} {Selected} {Papers}, {Part} {I}},
	volume = {1376},
	isbn = {9789811610851 9789811610868},
	shorttitle = {Computer {Vision} and {Image} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-1086-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer Singapore},
	editor = {Singh, Satish Kumar and Roy, Partha and Raman, Balasubramanian and Nagabhushan, P.},
	year = {2021},
	doi = {10.1007/978-981-16-1086-8},
	file = {Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:/Users/johannesreichle/Zotero/storage/QEEQCRND/Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:application/pdf},
}

@misc{singh_optical_nodate,
	title = {Optical {Character} {Recognition} {Techniques}: {A} {Survey}},
	shorttitle = {Optical {Character} {Recognition} {Techniques}},
	abstract = {This paper presents a literature review on English OCR techniques. English OCR system is compulsory to convert numerous published books of English into editable computer text files. Latest research in this area has been able to grown some new methodologies to overcome the complexity of English writing style. Still these algorithms have not been tested for complete characters of English Alphabet. Hence, a system is required which can handle all classes of English text and identify characters among these classes.},
	author = {Singh, Sukhpreet},
	annote = {Extracted Annotations (17/10/2021, 21:45:46)
"Optical Character Recognition [1] - [5] is a process that can convert text, present in digital image, to editable text." (Singh :1)
"The next step is to apply an OCR (Optical Character Recognition) process, meaning that the scanned image of each document will be translated into machine process able text" (Singh :1)
"a post-processing step to correct these errors is a very important part of the post-processing chain." (Singh :1)},
	file = {Singh_Optical Character Recognition Techniques.pdf:/Users/johannesreichle/Zotero/storage/67Q5REKF/Singh_Optical Character Recognition Techniques.pdf:application/pdf;Citeseer - Snapshot:/Users/johannesreichle/Zotero/storage/98UBX8JM/download.html:text/html},
}

@article{oyedotun_deep_2015,
	title = {Deep {Learning} in {Character} {Recognition} {Considering} {Pattern} {Invariance} {Constraints}},
	volume = {7},
	issn = {2074904X, 20749058},
	url = {http://www.mecs-press.org/ijisa/ijisa-v7-n7/v7n7-1.html},
	doi = {10.5815/ijisa.2015.07.01},
	abstract = {Character recognition is a field of machine learning that has been under research for several decades. The particular success of neural networks in pattern recognition and therefore character recognition is laudable. Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex. This research work reviews some of the most successful pre-training approaches to initializing deep networks such as stacked auto encoders, and deep belief networks based on achieved error rates. More importantly, this research also parallels investigating the performance of deep networks on some common problems associated with pattern recognition systems such as translational invariance, rotational invariance, scale mismatch, and noise. To achieve this, Yoruba vowel characters databases have been used in this research.},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {International Journal of Intelligent Systems and Applications},
	author = {Oyedotun, Oyebade K. and Olaniyi, Ebenezer O. and Khashman, Adnan},
	month = jun,
	year = {2015},
	pages = {1--10},
	annote = {Extracted Annotations (22/10/2021, 16:59:10)
"Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex." (Oyedotun et al 2015:1)
"Neural networks, conversely, can learn the features of task on which they are designed and trained; they can also adapt to some moderate variations such as noise on the data they have been trained with, hence considered intelligent. The success of neural networks in contrast to other non-intelligent recognition approaches is striking, based on performance, and somewhat ease of design considering the capability of neural networks in approximation any mapping function of inputs to outputs while requiring „least‟ domain specific knowledge for its programming each time it is embedded in different applications. i.e. self-programming." (Oyedotun et al 2015:1)
"consideration on the amount of common pattern invariance achievable in learning has also changed; such common invariances include relatively moderate translation, rotation, scale mismatch, and noisy patterns." (Oyedotun et al 2015:2)
"Conversely, this paper takes an alternative approach, by presenting a work which focuses on invariance learning based on the neural network architectures, rather than complex training data manipulation schemes and painstaking mathematical foundations. It is noteworthy that this research did not employ any invariant feature extraction technique; hence, investigates how neural network structures and learning paradigms affect invariance learning. Also, to reinforce the application importance of this work, real life data, „handwritten characters‟, have been used to train and simulate the considered networks." (Oyedotun et al 2015:3)
"Unfortunately, the difficulty is to synthesize, and then to efficiently compute, the classification function that maps objects to categories, given that objects in a category can have widely varying input representations [7]; bearing in mind also that this approach increases computational load on the designed system. A better approach is to consider neural network architectures that allow some level of built-in invariance due to structure; and of course, this can usually still be augmented with some handcrafted invariance achieved through data manipulation schemes." (Oyedotun et al 2015:3)
"Such features include:  distributed representation of knowledge at each hidden layer.  distinct features are extracted by units or neurons in each hidden layer.  several units can be active concurrently." (Oyedotun et al 2015:3)
"Generally, it is conceived that in deep networks, the first hidden layer extracts some primary features about the input, then these features are combined in the second layer to more defined features, and these features are further combined into well more defined features in the following layers, and so on. This can be somewhat seen as a hierarchical representation of knowledge;" (Oyedotun et al 2015:3)
"Deep learning depicts neural network architectures of more than a single hidden layer (multilayer networks); in contrast to networks of single hidden layer which are commonly referred to as shallow networks." (Oyedotun et al 2015:3)
"Generative Architectures This class of deep networks is not required to be deterministic of the class patterns that the inputs belong, but is used to sample joint statistical distribution of data; moreover this class of networks relies on unsupervised learning." (Oyedotun et al 2015:4)
"Discriminative Architectures Discriminative deep networks actually are required to be deterministic of the correlation of input data to the classes of patterns therein. Moreover, this category of networks relies on supervised learning." (Oyedotun et al 2015:4)
"Hybrid Architectures Networks that belong to this class rely on the combination of generative and discriminative approach in their architectures. Generally, such networks are generatively pre-trained and then discriminately finetuned for deterministic purposes." (Oyedotun et al 2015:4)
"The application of auto encoders and therefore generative architectures leverage on the unavailability of labelled data or the required logistics and cost that may be necessary in labeling available data. It therefore follows that generative learning suffices in situations where we have large unlabelled data and small labelled data" (Oyedotun et al 2015:4)
"The auto encoder can be seen as an encoder-decoder system, where the encoder (input-hidden layer pair) receives the input, extracting essential features for reconstruction; while the decoder (hidden-output layer pair) part receives the features extracted from the hidden layer, performing reconstruction at its best." (Oyedotun et al 2015:4)
"The training approach that is used in achieving learning in generative network architectures is known as „greedy layer-wise pre-training‟." (Oyedotun et al 2015:5)
"A DBN is a deep network, which is graphical and probabilistic in nature; it is essentially a generative model too. A belief net is a directed acyclic graph composed of stochastic variables [19]." (Oyedotun et al 2015:5)
"Such a training scheme is aimed at maximizing the likelihood of the input vector at a layer below given a configuration of a hidden layer that is directly on top of it." (Oyedotun et al 2015:6)
"A restricted Boltzmann machine has only two layers (fig.8); the input (visible) and the hidden layer. The connections between the two layers are undirected, and there are no interconnections between units of the same layer as in the general Boltzmann machine. We can therefore say that from the restriction in interconnections of units in layers, units are conditionally independent. The RBM can be seen as a Markov network, where the visible layer consists of either Bernoulli (binary) or Gaussian (real values usually between from 0 to 1) stochastic units, and the hidden layer of stochastic Bernoulli units [22]." (Oyedotun et al 2015:6)
"The simulation results on the considered invariances for the different trained networks are shown in table 3. It can be seen that the SDAE has the lowest error rate on translation, while the DBN outperformed other networks on rotational and scale invariances." (Oyedotun et al 2015:8)
"hence we can conjure that these networks were able to explore a more complex space of solutions while learning to the deep nature; since hierarchical learning allows more distributed knowledge representation." (Oyedotun et al 2015:8)
"It will be seen that the deep belief networks on the average, performed best compared to the other networks on variances like translation, rotation and scale mismatch; while its tolerance to noise decreased noticeably as the level of noise was increased as shown in table 4, table 5, and fig.15." (Oyedotun et al 2015:9)
"These variances are common constraints that occur in real life recognition systems for handwritten characters, and some of the solutions have been constraining the users (writers) to some particular possible domains of writing spaces or earmarked pattern of writing in order for low error rates to be achieved." (Oyedotun et al 2015:9)
"It has been shown that another flavour of neural networks, "convolutional networks" and its deep variant give very motivating performance on some of these constraints [25], however the complexity of these networks is somewhat obvious." (Oyedotun et al 2015:9)},
	file = {Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:/Users/johannesreichle/Zotero/storage/IWBWRJCE/Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:application/pdf},
}

@inproceedings{zhao_improving_2020,
	title = {Improving {Deep} {Learning} based {Optical} {Character} {Recognition} via {Neural} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185798},
	abstract = {Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment. We compared it with different methods, and the experimental results proved the effectiveness of our method.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Zhao, Zhenyao and Jiang, Min and Guo, Shihui and Wang, Zhenzhong and Chao, Fei and Tan, Kay Chen},
	month = jul,
	year = {2020},
	keywords = {Optical character recognition software, Text recognition, Computer architecture, Feature extraction, Training, Object detection, Task analysis},
	pages = {1--7},
	annote = {Extracted Annotations (17/10/2021, 23:42:47)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)},
	annote = {Extracted Annotations (22/10/2021, 12:48:20)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)
"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)
"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)
"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)
"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)
"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)
"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)
"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	file = {Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:/Users/johannesreichle/Zotero/storage/SHJA8U7A/Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/G9CV8S98/9185798.html:text/html},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-10-19},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {RetinaNet},
	file = {Lin et al_2018_Focal Loss for Dense Object Detection.pdf:/Users/johannesreichle/Zotero/storage/3NLWH9LE/Lin et al_2018_Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8CJ43KHN/1708.html:text/html},
}

@misc{beom_crnn_2021-1,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-10-19},
	author = {Beom},
	month = oct,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
}

@article{liao_textboxes_2018,
	title = {{TextBoxes}++: {A} {Single}-{Shot} {Oriented} {Scene} {Text} {Detector}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{TextBoxes}++},
	url = {http://arxiv.org/abs/1801.02765},
	doi = {10.1109/TIP.2018.2825107},
	abstract = {Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes\_plusplus},
	number = {8},
	urldate = {2021-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.02765},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3676--3690},
	annote = {Comment: 15 pages},
	file = {Liao et al_2018_TextBoxes++.pdf:/Users/johannesreichle/Zotero/storage/BYY77N9D/Liao et al_2018_TextBoxes++.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NPE7JJW5/1801.html:text/html},
}

@article{shi_end--end_2015,
	title = {An {End}-to-{End} {Trainable} {Neural} {Network} for {Image}-based {Sequence} {Recognition} and {Its} {Application} to {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/1507.05717},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	urldate = {2021-10-19},
	journal = {arXiv:1507.05717 [cs]},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.05717},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 figures},
	file = {Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:/Users/johannesreichle/Zotero/storage/8PJQ4H5U/Shi et al_2015_An End-to-End Trainable Neural Network for Image-based Sequence Recognition and.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NAI47ZEJ/1507.html:text/html},
}

@misc{noauthor_search_nodate,
	title = {Search · · argman/{EAST}},
	url = {https://github.com/argman/EAST},
	abstract = {A tensorflow implementation of EAST text detector. Contribute to argman/EAST development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {Tensorflow EAST implementation},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/DSDTHJ9A/EAST.html:text/html},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - {SakuraRiven}/{EAST} at pythonrepo.com},
	url = {https://github.com/SakuraRiven/EAST},
	abstract = {PyTorch Re-Implementation of EAST: An Efficient and Accurate Scene Text Detector - GitHub - SakuraRiven/EAST at pythonrepo.com},
	language = {en},
	urldate = {2021-10-19},
	journal = {GitHub},
	annote = {EAST with PyTorch},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/2XJ788B8/EAST.html:text/html},
}

@misc{noauthor_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Arbitrary}-{Shaped} {Text} ({RRC}-{ArT})},
	url = {https://deepai.org/publication/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art},
	abstract = {09/16/19 - This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped
Text (RRC-ArT) that consists of three major challeng...},
	urldate = {2021-10-19},
	journal = {DeepAI},
	month = sep,
	year = {2019},
	annote = {Challenge that goes in the right direction -{\textgreater} read},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/R524XQNK/icdar2019-robust-reading-challenge-on-arbitrary-shaped-text-rrc-art.html:text/html},
}

@inproceedings{jenckel_anyocr_2016,
	address = {Cancun},
	title = {{anyOCR}: {A} sequence learning based {OCR} system for unlabeled historical documents},
	isbn = {978-1-5090-4847-2},
	shorttitle = {{anyOCR}},
	url = {http://ieeexplore.ieee.org/document/7900265/},
	doi = {10.1109/ICPR.2016.7900265},
	abstract = {Institutes and libraries around the globe are preserving the literary heritage by digitizing historical documents. However, to make this data easily accessible the scanned documents need to be transformed into search-able text. State of the art OCR systems using Long-Short-Term-Memory networks (LSTM) have been applied successfully to recognize text in both printed and handwritten form. Besides the general challenges with historical documents, e.g. poor image quality, damaged characters, etc., especially unknown scripts and old fonds make it difficult to provide the large amount of transcribed training data required for these methods to perform well. Transcribing the documents manually is very costly in terms of manhours and require language specific expertise. The unknown fonds and requirement for meaningful context also make the use of synthetic data unfeasible.},
	language = {en},
	urldate = {2021-10-20},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Jenckel, Martin and Bukhari, Syed Saqib and Dengel, Andreas},
	month = dec,
	year = {2016},
	pages = {4035--4040},
	file = {Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:/Users/johannesreichle/Zotero/storage/YDA4CDN8/Jenckel et al. - 2016 - anyOCR A sequence learning based OCR system for u.pdf:application/pdf},
}

@misc{noauthor_few-shot_2020,
	title = {Few-{Shot} {Learning} ({FSL}): {What} it is \& its {Applications}},
	shorttitle = {Few-{Shot} {Learning} ({FSL})},
	url = {https://research.aimultiple.com/few-shot-learning/},
	abstract = {Few-shot/low-shot learning definition, importance, use cases, approaches, implementations in Python \& differences between zero-shot learning},
	language = {en-US},
	urldate = {2021-10-27},
	month = nov,
	year = {2020},
	annote = {Great source for starting to learn about few shot learning -{\textgreater} basic stuff, overview, follow up sources!},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/P73M3WXD/few-shot-learning.html:text/html},
}

@misc{noauthor_cross-domain_nodate,
	title = {Cross-{Domain} {Few}-{Shot} {Learning} for {Mobile} {OCR} {\textbar} {Anyline}},
	url = {https://anyline.com/news/cross-domain-few-shot-learning-mobile-ocr},
	abstract = {Find out the latest progress from our research collaboration with AI Lab of JKU Linz, on the potential of the 'CHEF' Few-Shot learning method.},
	language = {en},
	urldate = {2021-10-29},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/LUSIBD9S/cross-domain-few-shot-learning-mobile-ocr.html:text/html},
}

@article{srivastava_comparative_2021,
	title = {Comparative analysis of deep learning image detection algorithms},
	volume = {8},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00434-w},
	doi = {10.1186/s40537-021-00434-w},
	abstract = {A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Journal of Big Data},
	author = {Srivastava, Shrey and Divekar, Amit Vishvas and Anilkumar, Chandu and Naik, Ishika and Kulkarni, Ved and Pattabiraman, V.},
	month = dec,
	year = {2021},
	pages = {66},
	annote = {Extracted Annotations (16/11/2021, 15:54:27)"From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms." (Srivastava et al 2021:66)"Fast R-CNN model as a method of object detection [3]. It makes use of the CNN method in the target detection field. The novelty of the method proposed by Girshick has proposed a window extraction algorithm instead of a conventional sliding window extraction procedure in the R-CNN model, there is separate training for the deep convolution network for feature isolation and the support vector machines for categorization [4]. In the fast RCNN method they have combined feature extraction with classification into a classification framework" (Srivastava et al 2021:67)"Whereas in the faster R-CNN method the proposal isolation region and bit of Fast R-CNN are put into a network template referred to as region proposal network (RPN). The accuracy of Fast R-CNN and Faster R-CNN is the same." (Srivastava et al 2021:67)"You Only Look Once (YOLO)—A one-time convolutional neural network for the prediction of the frame position and classification of multiple candidates is offered by YOLO. Endto-end target detection can be achieved this way. It uses a regression problem to solve object detection. A single end-to-end system completes the process of putting the output obtained from the original image to the category and position" (Srivastava et al 2021:67)"advanced YOLO v1 network model which optimizes the loss of function in YOLO v1, it has a new inception model structure, has a specialized pooling pyramid layer, and has better performance." (Srivastava et al 2021:68)"According to the team, SSD is a simple method and requires an object proposal as it is based on the complete elimination of the process that generates a proposal. It also eliminates the subsequent pixel and resampling stages. So, it combines everything into a single step. SSD is also very easy to train and is very straightforward when it comes to integrating it into the system. This makes detection easier. The primary feature of SSD is using multiscale convolutional bounding box outputs that are attached to several feature maps" (Srivastava et al 2021:68)"The best feature of Tiny SSD is its size of 2.3 MB which is even smaller than Tiny YOLO." (Srivastava et al 2021:68)"The paper also accesses some deep learning techniques for object detection systems. The current paper states that deep CNNs work on the principle of weight sharing. It gives us information about some crucial points in CNN. These features of CNN depicted in this paper are: [1] a. CNN is integration and involves the multiplication of two overlapping functions. b. Features maps are abstracted to reduce their complexity in terms of space c. Repetition of the process is done to produce the feature maps using filters. d. CNN utilizes different types of pooling layers." (Srivastava et al 2021:68)"In this work the multi-layered system they introduced the Squeeze-and-Excitation model as an additional layer to the SSD model. The improved model employed self-learning that further enhanced the accuracy of the system for small scale pedestrian detection." (Srivastava et al 2021:69)"Architectural Innovations (2014-2020): The well-known and widely used VGG architecture was developed in 2014 [22]. RCNN, based on VGG like many others, introduced the idea that objects are located in certain regions of the image; hence the name: region-based CNN [23]. Improved versions of RCNN—Fast RCNN [24] and Faster RCNN [3] came out in the subsequent years. Both of these reduced computation time, while maintaining the accuracy that RCNN is known for. Single Shot Multibox Detector (SSD), also based on VGG was developed around 2016 [8]. Another algorithm, You Only Look Once (YOLO), based on an architecture called DarkNet was first published in 2016 [6]. It is in active development; its third version was released in 2018 [25]." (Srivastava et al 2021:70)"SSD does not resample pixels or features for bounding box hypotheses and is as accurate as models that do. In addition to this, it is quite straightforward compared to methods that require object proposals because it completely eradicates feature resampling stages or pixel and proposal generation, by encompassing all computation in a single network. Therefore, SSD is very simple to train and can be easily integrated into systems that perform detection as one of their functions [8]. It's architecture heavily depends on the generation of bounding boxes and the extraction of feature maps, which are also known as default bounding boxes. Loss is calculated by the network, using comparisons of the offsets of the predicted classes and the default bounding boxes with the training samples' ground truth values, using different filters for every iteration. Using the back-propagation algorithm and the calculated loss value, all the parameters are updated." (Srivastava et al 2021:71)"SSD is built on a feed-forward complex network that builds a collection of standardsize bounding boxes and for each occurrence of an object in those boxes, a respective score. After score generation, non-maximum suppression is used to generate the final detection results. The preliminary network layers are built on a standard architecture utilized for high quality image classification (and truncated before any classification layers), which is a VGG-16 network. An auxiliary structure is added to the truncated base network such as convo6 to produce detections." (Srivastava et al 2021:71)"The reason for using auxiliary layers is because they allow us to extract the required features at multiple scales as well as reduce the size of our input with each layer that is traversed through [8]. For each cell in the image, the layer makes a certain number of predications. Each prediction consists of a boundary box and the box generates scores for all the classes it detects in this box including a score for no object at all." (Srivastava et al 2021:71)"Convolutional predictors for object detection: Every feature layer produces a fixed number of predictions by utilising convolutional filters. For every feature layer of size x × y having n channels, the rudimentary component for generating prediction variables of a potential detection result is a 3 × 3 × x small kernel that creates a confidence score for every class," (Srivastava et al 2021:71)non-maximum suppression: algorithm to find best proposed bounding box with IOU (note on p.71)"This computation results in a total of (s + 4) b filters that are applicable to every location in the feature map, resulting in (s + 4) × b × x × y outputs for a x × y feature ma" (Srivastava et al 2021:72)"All SSD predictions are divided into two types; negative matches or positive matches. Positive matches are only used by SSD to calculate the localization cost which is the misalignment of the boundary box with the default box." (Srivastava et al 2021:72)"However, SSD is not as efficient at detection for smaller objects, which can be solved by having a more efficient feature extractor backbone (e.g., ResNet101), with the addition of deconvolution layers along with skip connections to create additional large-scale context, and design a better network structure" (Srivastava et al 2021:73)"The algorithm of the original R-CNN technique is as follows: [29] 1. Using a Selective Search Algorithm, several candidate region proposals are extracted from the input image. In this algorithm, numerous candidate regions are generated in initial sub-segmentation. Then, regions which are similar are combined to form bigger regions using a greedy algorithm. These regions make up the final region proposals. 2. The CNN component warps the proposals and extracts distinct features as a vector output. 3. The features which are extracted are fed into an SVM (Support Vector Machine) for recognizing objects of interest in the proposal." (Srivastava et al 2021:75)"Fast R-CNN is an algorithm for object detection that solves some of the drawbacks of R-CNN. It uses an approach similar to that of its predecessor, but as opposed to using region proposals, the CNN utilizes the image itself for creating a convolutional feature map, following which region proposals are determined and warped from it. An RoI (Region of Interest) pooling layer is employed for reshaping the warped squares according to a predefined size for a fully connected layer to accept them. The region class is then predicted from the RoI vector with the help of a SoftMax laye" (Srivastava et al 2021:75)"YOLOv3 In modern times YOLO (You Only Look Once) is one of the most precise and accurate object detection algorithms available. It has been made on the basis of a newly altered and customized architecture named Darknet [25]" (Srivastava et al 2021:77)"YOLOv3 makes use of the latest darknet features like 53 layers and it has undergone training with one of the most reliable datasets called ImageNet. The layers used are from an architecture Darnnet-53 which is convolutional in nature. For detection, the aforementioned 53 layers were supplemented instead of the pre-existing 19 and this enhanced architecture was trained and instructed with PASCAL VOC." (Srivastava et al 2021:77)"It was found that Yolo-v3 is the fastest with SSD following closely and Faster RCNN coming in the last place. However, it can be said that the use case influences which algorithm is picked; if you are dealing with a relatively small dataset and don't need real-time results, it is best to go with Faster RCNN. Yolo-v3 is the one to pick if you need to analyse a live video feed. Meanwhile, SSD provides a good balance between speed and accuracy." (Srivastava et al 2021:90)},
	file = {Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:/Users/johannesreichle/Zotero/storage/2EQVX9L2/Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:application/pdf},
}

@article{chen_generative_2021,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	year = {2021},
	pages = {12},
	file = {Chen et al. - Generative Pretraining from Pixels.pdf:/Users/johannesreichle/Zotero/storage/A235GR5L/Chen et al. - Generative Pretraining from Pixels.pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Wolf et al_2020_Transformers.pdf:/Users/johannesreichle/Zotero/storage/WU9PCJ4B/Wolf et al_2020_Transformers.pdf:application/pdf},
}

@inproceedings{vicol_unbiased_2021,
	title = {Unbiased {Gradient} {Estimation} in {Unrolled} {Computation} {Graphs} with {Persistent} {Evolution} {Strategies}},
	url = {https://proceedings.mlr.press/v139/vicol21a.html},
	abstract = {Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.},
	language = {en},
	urldate = {2021-10-30},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10553--10563},
	file = {Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:/Users/johannesreichle/Zotero/storage/9NR7ES72/Vicol et al_2021_Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent.pdf:application/pdf;Supplementary PDF:/Users/johannesreichle/Zotero/storage/6FXHTMRX/Vicol et al. - 2021 - Unbiased Gradient Estimation in Unrolled Computati.pdf:application/pdf},
}

@article{yang_learning_2021,
	title = {Learning {High}-{Precision} {Bounding} {Box} for {Rotated} {Object} {Detection} via {Kullback}-{Leibler} {Divergence}},
	url = {http://arxiv.org/abs/2106.01883},
	abstract = {Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios. Taking the perspective that horizontal detection is a special case for rotated object detection, in this paper, we are motivated to change the design of rotation regression loss from induction paradigm to deduction methodology, in terms of the relation between rotation and horizontal detection. We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio. This mechanism can be vital for high-precision detection as a slight angle error would cause a serious accuracy drop for large aspect ratios objects. More importantly, we have proved that KLD is scale invariant. We further show that the KLD loss can be degenerated into the popular \$l\_\{n\}\$-norm loss for horizontal detection. Experimental results on seven datasets using different detectors show its consistent superiority, and codes are available at https://github.com/yangxue0827/RotationDetection.},
	urldate = {2021-11-02},
	journal = {arXiv:2106.01883 [cs]},
	author = {Yang, Xue and Yang, Xiaojiang and Yang, Jirui and Ming, Qi and Wang, Wentao and Tian, Qi and Yan, Junchi},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01883},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 16 pages, 5 figures, 8 tables, accepted by NeurIPS21, codes are available at https://github.com/yangxue0827/RotationDetection},
	annote = {Extracted Annotations (04/11/2021, 18:11:57)
"Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios." (Yang et al 2021:1)
"We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio." (Yang et al 2021:1)
"In this paper, we take a step back, and aim to develop (from a deductive perspective) a unified regression framework for rotation detection and its special case: horizontal detection. In fact, our new framework enjoys a coherent property that it can be degenerated into the current commonly used regression loss (e.g. ln -norm) in special cases (horizontal detection), as shown in Figure 1(b)." (Yang et al 2021:2)
"The mainstream classical object detection algorithms can be roughly divided according to the following standards: Two- [7, 8, 9, 11] or Single-stage [10, 18, 19] object detection, Anchor-free [20, 21, 22] or Anchor-based [8, 9, 10] object detection and CNN [8, 10, 20] or Transformer-based [23, 24] object detection. Although the pipelines may vary, the mainstream regression loss often uses the popular ln -norm loss (such as smooth L1 loss) or IoU-based loss" (Yang et al 2021:3)
"However, horizontal detectors do not provide accurate orientation and scale information." (Yang et al 2021:3)
"The overall regression loss for rotation detection is: Lreg = ln -norm (tx ; ty ; tw ; th ; t )" (Yang et al 2021:4)
"It can be seen that parameters are optimized independently, making the loss (or detection accuracy) sensitive to the under-fitting of any of the parameters. This mechanism is fatal to high-precision detection." (Yang et al 2021:4)
text has long aspect ratios -{\textgreater} angle parameter very important -{\textgreater} KLD approach is favored (note on p.4)
 
"Although GWD scheme has played a preliminary exploration of the deductive paradigm, it does not focus on achieving high-precision detection and scale invariance. In the following, we will propose our new approach based on the Kullback-Leibler divergence (KLD)" (Yang et al 2021:5)
Gaussians are constructed like with GWD scheme (note on p.5)
 
"ICDAR2015, MLT and MSRA-TD500 are commonly used for oriented scene text detection and spotting. ICDAR2015 includes 1,000 training images and 500 testing images. ICDAR2017 MLT is a multi-lingual text dataset, which includes 7,200 training images, 1,800 validation images and 9,000 testing images. MSRA-TD500 dataset consists of 300 training images and 200 testing images." (Yang et al 2021:7)
"Limitations. Despite the theoretical grounds and the promising experimental justifications, our method has an obvious limitation that it cannot be directly applied to quadrilateral detection [33, 44]." (Yang et al 2021:10)},
	file = {Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:/Users/johannesreichle/Zotero/storage/JF2GVMIQ/Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/H7Q9M3ZH/2106.html:text/html},
}

@article{xu_dp-ssl_2021,
	title = {{DP}-{SSL}: {Towards} {Robust} {Semi}-supervised {Learning} with {A} {Few} {Labeled} {Samples}},
	shorttitle = {{DP}-{SSL}},
	url = {http://arxiv.org/abs/2110.13740},
	abstract = {The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning{\textasciitilde}(MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82\% annotation accuracy on unlabeled data and 93.46\% classification accuracy on test data, which are higher than the SOTA results.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.13740 [cs]},
	author = {Xu, Yi and Ding, Jiandong and Zhang, Lu and Zhou, Shuigeng},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13740},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021; 16 pages with appendix},
	file = {Xu et al_2021_DP-SSL.pdf:/Users/johannesreichle/Zotero/storage/7RVZR9N7/Xu et al_2021_DP-SSL.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XLYQEFMP/2110.html:text/html},
}

@article{lu_soft_2021,
	title = {{SOFT}: {Softmax}-free {Transformer} with {Linear} {Complexity}},
	shorttitle = {{SOFT}},
	url = {http://arxiv.org/abs/2110.11945},
	abstract = {Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.11945 [cs]},
	author = {Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2021 Spotlight. Project page at https://fudan-zvg.github.io/SOFT/},
	file = {Lu et al_2021_SOFT.pdf:/Users/johannesreichle/Zotero/storage/W6LD7EJZ/Lu et al_2021_SOFT.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/ZXCCUE46/2110.html:text/html},
}

@inproceedings{raisi_transformer-based_2021,
	address = {Nashville, TN, USA},
	title = {Transformer-based {Text} {Detection} in the {Wild}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522851/},
	doi = {10.1109/CVPRW53098.2021.00353},
	abstract = {A major limitation to most state-of-the-art visual localization methods is their ineptitude to make use of ubiquitous signs and directions that are typically intuitive to humans. Localization methods can greatly beneﬁt from a system capable of reasoning about a variety of cues beyond low-level features, such as street signs, store names, building directories, room numbers, etc.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Raisi, Zobeir and Naiel, Mohamed A. and Younes, Georges and Wardell, Steven and Zelek, John S.},
	month = jun,
	year = {2021},
	pages = {3156--3165},
	file = {Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:/Users/johannesreichle/Zotero/storage/Y6FVVXIN/Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:application/pdf},
}

@article{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2021-11-05},
	journal = {arXiv:2005.12872 [cs]},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Carion et al_2020_End-to-End Object Detection with Transformers.pdf:/Users/johannesreichle/Zotero/storage/3RYU4T68/Carion et al_2020_End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/GBXATMTN/2005.html:text/html},
}

@misc{noauthor_accepted_nodate,
	title = {Accepted {Papers}},
	url = {https://nips.cc/Conferences/2021/AcceptedPapersInitial},
	urldate = {2021-11-05},
	file = {Accepted Papers:/Users/johannesreichle/Zotero/storage/BQ5SUEPT/AcceptedPapersInitial.html:text/html},
}

@article{zhang_fast_2021,
	title = {Fast {Multi}-{Resolution} {Transformer} {Fine}-tuning for {Extreme} {Multi}-label {Text} {Classification}},
	url = {http://arxiv.org/abs/2110.00685},
	abstract = {Extreme multi-label text classification (XMC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown significant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the fine-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively fine-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes significantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51\% to 54\%.},
	urldate = {2021-11-06},
	journal = {arXiv:2110.00685 [cs, stat]},
	author = {Zhang, Jiong and Chang, Wei-cheng and Yu, Hsiang-fu and Dhillon, Inderjit S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.00685},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:/Users/johannesreichle/Zotero/storage/FXMXFQ9J/Zhang et al_2021_Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XNYYFCCC/2110.html:text/html},
}

@article{sheng_centripetaltext_2021,
	title = {{CentripetalText}: {An} {Efficient} {Text} {Instance} {Representation} for {Scene} {Text} {Detection}},
	shorttitle = {{CentripetalText}},
	url = {http://arxiv.org/abs/2107.05945},
	abstract = {Scene text detection remains a grand challenge due to the variation in text curvatures, orientations, and aspect ratios. One of the hardest problems in this task is how to represent text instances of arbitrary shapes. Although many methods have been proposed to model irregular texts in a flexible manner, most of them lose simplicity and robustness. Their complicated post-processings and the regression under Dirac delta distribution undermine the detection performance and the generalization ability. In this paper, we propose an efficient text instance representation named CentripetalText (CT), which decomposes text instances into the combination of text kernels and centripetal shifts. Specifically, we utilize the centripetal shifts to implement pixel aggregation, guiding the external text pixels to the internal text kernels. The relaxation operation is integrated into the dense regression for centripetal shifts, allowing the correct prediction in a range instead of a specific value. The convenient reconstruction of text contours and the tolerance of prediction errors in our method guarantee the high detection accuracy and the fast inference speed, respectively. Besides, we shrink our text detector into a proposal generation module, namely CentripetalText Proposal Network, replacing Segmentation Proposal Network in Mask TextSpotter v3 and producing more accurate proposals. To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets. For the task of scene text detection, our approach achieves superior or competitive performance compared to other existing methods, e.g., F-measure of 86.3\% at 40.0 FPS on Total-Text, F-measure of 86.1\% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text recognition, our method outperforms Mask TextSpotter v3 by 1.1\% on Total-Text.},
	urldate = {2021-11-06},
	journal = {arXiv:2107.05945 [cs]},
	author = {Sheng, Tao and Chen, Jie and Lian, Zhouhui},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.05945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {Sheng et al_2021_CentripetalText.pdf:/Users/johannesreichle/Zotero/storage/GAFQN94L/Sheng et al_2021_CentripetalText.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HZFSY2GG/2107.html:text/html},
}

@article{shen_you_2021,
	title = {You {Never} {Cluster} {Alone}},
	url = {http://arxiv.org/abs/2106.01908},
	abstract = {Recent advances in self-supervised learning with instance-level contrastive objectives facilitate unsupervised clustering. However, a standalone datum is not perceiving the context of the holistic cluster, and may undergo sub-optimal assignment. In this paper, we extend the mainstream contrastive learning paradigm to a cluster-level scheme, where all the data subjected to the same cluster contribute to a unified representation that encodes the context of each data group. Contrastive learning with this representation then rewards the assignment of each datum. To implement this vision, we propose twin-contrast clustering (TCC). We define a set of categorical variables as clustering assignment confidence, which links the instance-level learning track with the cluster-level one. On one hand, with the corresponding assignment variables being the weight, a weighted aggregation along the data points implements the set representation of a cluster. We further propose heuristic cluster augmentation equivalents to enable cluster-level contrastive learning. On the other hand, we derive the evidence lower-bound of the instance-level contrastive objective with the assignments. By reparametrizing the assignment variables, TCC is trained end-to-end, requiring no alternating steps. Extensive experiments show that TCC outperforms the state-of-the-art on challenging benchmarks.},
	urldate = {2021-11-06},
	journal = {arXiv:2106.01908 [cs]},
	author = {Shen, Yuming and Shen, Ziyi and Wang, Menghan and Qin, Jie and Torr, Philip H. S. and Shao, Ling},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01908},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021},
	file = {Shen et al_2021_You Never Cluster Alone.pdf:/Users/johannesreichle/Zotero/storage/SJ7NZPJZ/Shen et al_2021_You Never Cluster Alone.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/3JBHRY6C/2106.html:text/html},
}

@article{ma_arbitrary-oriented_2018,
	title = {Arbitrary-{Oriented} {Scene} {Text} {Detection} via {Rotation} {Proposals}},
	volume = {20},
	issn = {1941-0077},
	doi = {10.1109/TMM.2018.2818020},
	abstract = {This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks, which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.},
	number = {11},
	journal = {IEEE Transactions on Multimedia},
	author = {Ma, Jianqi and Shao, Weiyuan and Ye, Hao and Wang, Li and Wang, Hong and Zheng, Yingbin and Xue, Xiangyang},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Pipelines, Computer architecture, Task analysis, arbitrary oriented, Image edge detection, Microsoft Windows, Proposals, Robustness, rotation proposals, Scene text detection},
	pages = {3111--3122},
	file = {Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:/Users/johannesreichle/Zotero/storage/MUTNDE2W/Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/N8FNCBBL/8323240.html:text/html},
}

@misc{goswami_deeper_2018,
	title = {A deeper look at how {Faster}-{RCNN} works},
	url = {https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd},
	abstract = {Faster-RCNN is one of the most well known object detection neural networks [1,2]. It is also the basis for many derived networks for…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = jul,
	year = {2018},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/FCB2UPLV/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-11-06},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick_2015_Fast R-CNN.pdf:/Users/johannesreichle/Zotero/storage/WHTYMPGV/Girshick_2015_Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/X3L8V8HR/1504.html:text/html},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-11-06},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {Ren et al_2016_Faster R-CNN.pdf:/Users/johannesreichle/Zotero/storage/G6DDTA5M/Ren et al_2016_Faster R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/DRXQUEDA/1506.html:text/html},
}

@misc{goswami_comparison_2020,
	title = {Comparison of {Faster}-{RCNN} and {Detection} {Transformer} ({DETR})},
	url = {https://whatdhack.medium.com/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04},
	abstract = {Faster-RCNN is a well known network, arguably the gold standard, in object detection and segmentation. Detection Transformer ( DETR) on…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/RAFM6M6L/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04.html:text/html},
}

@misc{noauthor_deep_2021,
	title = {Deep {Learning} {Based} {OCR} for {Text} in the {Wild}},
	url = {https://nanonets.com/blog/deep-learning-ocr/},
	abstract = {Learn how to apply deep learning based OCR to recognize and extract unstructured text information from images using Tesseract and the OpenCV EAST engine.},
	language = {en},
	urldate = {2021-11-06},
	journal = {AI \& Machine Learning Blog},
	month = jun,
	year = {2021},
	annote = {good overview of techniques
has implementation (EAST coupled with tesseract) -{\textgreater} try how it works on data set},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/27F4KTAC/deep-learning-ocr.html:text/html},
}

@misc{hwalsuklee_awesome-deep-text-detection-recognition_2021,
	title = {awesome-deep-text-detection-recognition},
	copyright = {Apache-2.0},
	url = {https://github.com/hwalsuklee/awesome-deep-text-detection-recognition},
	abstract = {A curated list of resources for text detection/recognition (optical character recognition ) with deep learning methods.},
	urldate = {2021-11-11},
	author = {hwalsuklee},
	month = nov,
	year = {2021},
	note = {original-date: 2017-11-30T05:43:19Z},
	keywords = {awesome-list, awesome-lists, deep-learning, ocr, ocr-detection, ocr-paper, ocr-paper-list, ocr-papers, ocr-recognition, text-detection, text-detection-recognition, text-recognition},
}

@article{mittal_deep_2020,
	title = {Deep learning-based object detection in low-altitude {UAV} datasets: {A} survey},
	volume = {104},
	shorttitle = {Deep learning-based object detection in low-altitude {UAV} datasets},
	doi = {10.1016/j.imavis.2020.104046},
	abstract = {Deep learning-based object detection solutions emerged from computer vision has captivated full attention in recent years. The growing UAV market trends and interest in potential applications such as surveillance, visual navigation, object detection, and sensors-based obstacle avoidance planning have been holding good promises in the area of deep learning. Object detection algorithms implemented in deep learning framework have rapidly became a method for processing of moving images captured from drones. The primary objective of the paper is to provide a comprehensive review of the state of the art deep learning based object detection algorithms and analyze recent contributions of these algorithms to low altitude UAV datasets. The core focus of the studies is low-altitude UAV datasets because relatively less contribution was seen in the literature when compared with standard or remote-sensing based datasets. The paper discusses the following algorithms: Faster RCNN, Cascade RCNN, R-FCN etc. into two-stage, YOLO and its variants, SSD, RetinaNet into one-stage and CornerNet, Objects as Point etc. under advanced stages in deep learning based detectors. Further, one-two and advanced stages of detectors are studied in detail focusing on low-altitude UAV datasets. The paper provides a broad summary of low altitude datasets along with their respective literature in detection algorithms for the potential use of researchers. Various research gaps and challenges for object detection and classification in UAV datasets that need to deal with for improving the performance are also listed.},
	journal = {Image and Vision Computing},
	author = {Mittal, Payal and Singh, Raman and Sharma, Akashdeep},
	month = dec,
	year = {2020},
	pages = {104046},
	file = {Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:/Users/johannesreichle/Zotero/storage/RHDJAA7Z/Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:application/pdf},
}

@article{hu_gtc_2020,
	title = {{GTC}: {Guided} {Training} of {CTC} {Towards} {Efficient} and {Accurate} {Scene} {Text} {Recognition}},
	shorttitle = {{GTC}},
	url = {http://arxiv.org/abs/2002.01276},
	abstract = {Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance. With the benefit of guided training, CTC model achieves robust and accurate prediction for both regular and irregular scene text while maintaining a fast inference speed. Moreover, to further leverage the potential of CTC decoder, a graph convolutional network (GCN) is proposed to learn the local correlations of extracted features. Extensive experiments on standard benchmarks demonstrate that our end-to-end model achieves a new state-of-the-art for regular and irregular scene text recognition and needs 6 times shorter inference time than attentionbased methods.},
	urldate = {2021-11-11},
	journal = {arXiv:2002.01276 [cs, eess]},
	author = {Hu, Wenyang and Cai, Xiaocong and Hou, Jun and Yi, Shuai and Lin, Zhiping},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.01276},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	annote = {Comment: Accepted by AAAI 2020},
	file = {arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/QQPHT4UG/2002.html:text/html},
}

@misc{noauthor_one-stage_nodate,
	title = {One-stage object detection},
	url = {https://machinethink.net/blog/object-detection/},
	urldate = {2021-11-16},
	file = {One-stage object detection:/Users/johannesreichle/Zotero/storage/U2Q8RYF5/object-detection.html:text/html},
}

@article{saunders_layers_2012,
	title = {The {Layers} of {Research} {Design}},
	url = {https://www.academia.edu/4107831/The_Layers_of_Research_Design},
	abstract = {Within this article we use the metaphor of the “Research Onion” (Saunders et al., 2012: 128) to illustrate how these final elements (the core of the research onion) need to be considered in relation to other design elements (the outer layers of the},
	language = {en},
	urldate = {2021-11-17},
	journal = {Rapport},
	author = {Saunders, Mark N. K.},
	year = {2012},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/MKLS6E4Z/The_Layers_of_Research_Design.html:text/html},
}

@article{qiao_text_2021,
	title = {Text {Perceptron}: {Towards} {End}-to-{End} {Arbitrary}-{Shaped} {Text} {Spotting}},
	shorttitle = {Text {Perceptron}},
	url = {http://arxiv.org/abs/2002.06820},
	abstract = {Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text.},
	urldate = {2021-11-17},
	journal = {arXiv:2002.06820 [cs]},
	author = {Qiao, Liang and Tang, Sanli and Cheng, Zhanzhan and Xu, Yunlu and Niu, Yi and Pu, Shiliang and Wu, Fei},
	month = oct,
	year = {2021},
	note = {arXiv: 2002.06820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI2020. Code is available at https://davar-lab.github.io/publication.html or https://github.com/hikopensource/DAVAR-Lab-OCR},
	file = {Qiao et al_2021_Text Perceptron.pdf:/Users/johannesreichle/Zotero/storage/GE89LJ5N/Qiao et al_2021_Text Perceptron.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/SA69IXXA/2002.html:text/html},
}

@article{torraco_writing_2005,
	title = {Writing {Integrative} {Literature} {Reviews}: {Guidelines} and {Examples}},
	volume = {4},
	issn = {1534-4843},
	shorttitle = {Writing {Integrative} {Literature} {Reviews}},
	url = {https://doi.org/10.1177/1534484305278283},
	doi = {10.1177/1534484305278283},
	abstract = {The integrative literature review is a distinctive form of research that generates new knowledge about the topic reviewed. Little guidance is available on how to write an integrative literature review. This article discusses how to organize and write an integrative literature review and cites examples of published integrative literature reviews that illustrate how this type of research has made substantive contributions to the knowledge base of human resource development.},
	language = {en},
	number = {3},
	urldate = {2021-11-18},
	journal = {Human Resource Development Review},
	author = {Torraco, Richard J.},
	month = sep,
	year = {2005},
	note = {Publisher: SAGE Publications},
	keywords = {integrative literature review, integrative research review: synthesis, literature review},
	pages = {356--367},
	file = {Torraco_2005_Writing Integrative Literature Reviews.pdf:/Users/johannesreichle/Zotero/storage/49PU84KP/Torraco_2005_Writing Integrative Literature Reviews.pdf:application/pdf},
}

@inproceedings{ye_textfusenet_2020,
	address = {Yokohama, Japan},
	title = {{TextFuseNet}: {Scene} {Text} {Detection} with {Richer} {Fused} {Features}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {{TextFuseNet}},
	url = {https://www.ijcai.org/proceedings/2020/72},
	doi = {10.24963/ijcai.2020/72},
	abstract = {Arbitrary shape text detection in natural scenes is an extremely challenging task. Unlike existing text detection approaches that only perceive texts based on limited feature representations, we propose a novel framework, namely TextFuseNet, to exploit the use of richer features fused for text detection. More speciﬁcally, we propose to perceive texts from three levels of feature representations, i.e., character-, word- and global-level, and then introduce a novel text representation fusion technique to help achieve robust arbitrary text detection. The multi-level feature representation can adequately describe texts by dissecting them into individual characters while still maintaining their general semantics. TextFuseNet then collects and merges the texts’ features from different levels using a multi-path fusion architecture which can effectively align and fuse different representations. In practice, our proposed TextFuseNet can learn a more adequate description of arbitrary shapes texts, suppressing false positives and producing more accurate detection results. Our proposed framework can also be trained with weak supervision for those datasets that lack character-level annotations. Experiments on several datasets show that the proposed TextFuseNet achieves state-of-the-art performance. Speciﬁcally, we achieve an F-measure of 94.3\% on ICDAR2013, 92.1\% on ICDAR2015, 87.1\% on Total-Text and 86.6\% on CTW-1500, respectively.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ye, Jian and Chen, Zhe and Liu, Juhua and Du, Bo},
	month = jul,
	year = {2020},
	pages = {516--522},
	file = {Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:/Users/johannesreichle/Zotero/storage/DDEV4B5A/Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:application/pdf},
}

@inproceedings{chen_tinynet_2019,
	address = {Beijing, China},
	title = {{TinyNet}: {A} {Lightweight}, {Modular}, and {Unified} {Network} {Architecture} for {The} {Internet} of {Things}},
	isbn = {978-1-4503-6886-5},
	shorttitle = {{TinyNet}},
	url = {http://dl.acm.org/citation.cfm?doid=3342280.3342290},
	doi = {10.1145/3342280.3342290},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2019 {Conference} {Posters} and {Demos} on   - {SIGCOMM} {Posters} and {Demos} '19},
	publisher = {ACM Press},
	author = {Chen, Gonglong and Wang, Yihui and Li, Huikang and Dong, Wei},
	year = {2019},
	pages = {9--11},
	file = {Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:/Users/johannesreichle/Zotero/storage/93RHUC63/Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:application/pdf},
}

@article{wang_pyramid_2021,
	title = {Pyramid {Vision} {Transformer}: {A} {Versatile} {Backbone} for {Dense} {Prediction} without {Convolutions}},
	shorttitle = {Pyramid {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2102.12122},
	abstract = {Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer{\textasciitilde}(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.},
	urldate = {2021-11-23},
	journal = {arXiv:2102.12122 [cs]},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	month = aug,
	year = {2021},
	note = {arXiv: 2102.12122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICCV 2021},
	file = {Wang et al_2021_Pyramid Vision Transformer.pdf:/Users/johannesreichle/Zotero/storage/BV7XAMYM/Wang et al_2021_Pyramid Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8AR4JD76/2102.html:text/html},
}

@inproceedings{benali_amjoud_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Neural} {Networks} {Backbones} for {Object} {Detection}},
	isbn = {978-3-030-51935-3},
	doi = {10.1007/978-3-030-51935-3_30},
	abstract = {Detecting objects in images is an extremely important step in many image and video analysis applications. Object detection is considered as one of the main challenges in the field of computer vision, which focuses on identifying and locating objects of different classes in an image. In this paper, we aim to highlight the important role of deep learning and convolutional neural networks in particular in the object detection task. We analyze and focus on the various state-of-the-art convolutional neural networks serving as a backbone in object detection models. We test and evaluate them in the common datasets and benchmarks up-to-date. We Also outline the main features of each architecture. We demonstrate that the application of some convolutional neural network architectures has yielded very promising state-of-the-art results in image classification in the first place and then in the object detection task. The results have surpassed all the traditional methods, and in some cases, outperformed the human being’s performance.},
	language = {en},
	booktitle = {Image and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Benali Amjoud, Ayoub and Amrouch, Mustapha},
	editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
	year = {2020},
	keywords = {Object detection, Convolutional neural networks, Review},
	pages = {282--289},
	file = {Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:/Users/johannesreichle/Zotero/storage/EHNQ56PL/Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:application/pdf},
}

@misc{noauthor_new_nodate,
	title = {New mobile neural network architectures},
	url = {https://machinethink.net/blog/mobile-architectures/},
	urldate = {2021-11-23},
	file = {New mobile neural network architectures:/Users/johannesreichle/Zotero/storage/832WZH9H/mobile-architectures.html:text/html},
}

@inproceedings{vogelsang_requirements_2019,
	title = {Requirements {Engineering} for {Machine} {Learning}: {Perspectives} from {Data} {Scientists}},
	shorttitle = {Requirements {Engineering} for {Machine} {Learning}},
	doi = {10.1109/REW.2019.00050},
	abstract = {Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Vogelsang, Andreas and Borg, Markus},
	month = sep,
	year = {2019},
	keywords = {machine learning, Adaptation models, Analytical models, Business, data science, Encoding, interview study, Interviews, requirements engineering, Requirements engineering, Synthetic aperture sonar},
	pages = {245--251},
	annote = {Extracted Annotations (23/11/2021, 16:07:51)"characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems." (Vogelsang and Borg 2019:245)"We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process." (Vogelsang and Borg 2019:245)"In addition, a recent survey suggests that Requirements Engineering (RE) is the most difficult activity for the development of ML-based systems [2]." (Vogelsang and Borg 2019:245)"In a recent survey, Ishikawa and Yoshioka [2] reported that RE was listed as the most difficult activity for the development of ML-based systems: "The dominant concerns [. . . ] pertain to decision making with the customers. In the conventional setting, this activity involved requirements analysis and specification in the initial phase and an acceptance inspection in the final phase. This activity flow is not possible when working with ML-based systems due to the impossibility of prior estimation or assurance of achievable accuracy."" (Vogelsang and Borg 2019:246)"She argues that there is no unified collection or consideration of many NFRs for ML, including a consideration of ML-specific quality trade-off data." (Vogelsang and Borg 2019:246)"In ML systems, the quality of the resulting predictions can be considered a functional requirement" (Vogelsang and Borg 2019:247)"Quantification of quality targets is certainly also a challenge for conventional software [22], but training an ML model to go beyond a certain utility breakpoint turns into a functional requirement in practice." (Vogelsang and Borg 2019:247)""Explainability is twofold: On the one side, there is a need to explain the model (what has been learned). On the other side, there is a need to explain single predictions of the model."" (Vogelsang and Borg 2019:248)""If there is a combination or transformation of features that is smaller but has a similar performance, we prefer that. [. . . ] We try to minimize the number of features to make the model more explainable."" (Vogelsang and Borg 2019:248)"Training data is an integral part of any ML system. We envision that requirements for (training) data play a larger role for specifying ML systems than for conventional systems." (Vogelsang and Borg 2019:248)"Based on our interviews, we would add "training data needs specified and validated requirements like code"." (Vogelsang and Borg 2019:249)""You could try, but it won't help" - the data is what it is, and it is up to the data scientist to make the most of it." (Vogelsang and Borg 2019:249)"In some regulated domains, there are constraints on the amount of data necessary to tackle some problems. P4: "If we work on models to predict the likelihood of loan losses, we are forced to consider data from at least 5 years."" (Vogelsang and Borg 2019:249)"The higher the quality of the data, the better the application will work. That's why the process before the training is important: how I clean and augment the data."" (Vogelsang and Borg 2019:249)""There are many dimensions of data quality. [. . . ] For me, the most important ones are completeness, consistency, and correctness". Completeness refers to the sparsity of data within each characteristic (i.e., does the data cover the whole range of possible values). Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being true. Correctness is strongly influenced by the way how the data was collected." (Vogelsang and Borg 2019:249)},
	file = {Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:/Users/johannesreichle/Zotero/storage/GSZJZHNW/Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/QT2UUEQG/8933800.html:text/html},
}

@inproceedings{arpteg_software_2018,
	title = {Software {Engineering} {Challenges} of {Deep} {Learning}},
	doi = {10.1109/SEAA.2018.00018},
	abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
	booktitle = {2018 44th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Arpteg, Anders and Brinne, Björn and Crnkovic-Friis, Luka and Bosch, Jan},
	month = aug,
	year = {2018},
	keywords = {Big Data, deep learning, machine learning, Machine learning, artificial intelligence, Buildings, Companies, Google, Oils, Software engineering, software engineering challenges},
	pages = {50--59},
	annote = {Extracted Annotations (23/11/2021, 14:37:19)
"A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges." (Arpteg et al 2018:50)
"Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of highquality systems." (Arpteg et al 2018:50)
"One of the main differences from traditional machine learning (ML) methods is that DL automatically learns how to represent data using multiple layers of abstraction [5], [6]. In traditional ML, a significant amount of work has to be spent on "feature engineering" to build this representation manually, but this process can now be automated to a higher degree. Having an automated and data-driven method for learning how to represent data improves both the performance of the model and reduces requirements for manual feature engineering work [7], [8]." (Arpteg et al 2018:50)
"A key difference between ML systems and non-ML systems is that data partly replaces code in a ML system, and a learning algorithm is used to automatically identify patterns in the data instead of writing hard coded rules." (Arpteg et al 2018:50)
"ML systems not only experience code-level debts but also dependencies related to changes to the external world. Data dependencies have been found to build similar debt as code dependencies." (Arpteg et al 2018:51)
"Deep Learning also makes it possible to compose complex models from a set of sub models and potentially reuse pretrained parameters with so called "transfer learning" techniques." (Arpteg et al 2018:51)
"It is not uncommon for pipelines to change, add and remove fields, or become deprecated. Keeping a deployed production-ready ML system up to date with all these changes require a significant amount of work, and requires supporting monitoring and logging systems to be able to detect when these changes occur." (Arpteg et al 2018:51)
"This section presents a list of concisely described challenges in the intersection between ML and SE. They have been grouped into three categories: development, production, and organizational challenges." (Arpteg et al 2018:53)
"With the addition of data dependencies and a high degree of configuration parameters, it can be very challenging to properly maintain ML systems in the long run. Also, it is not uncommon to perform hyperparameter tuning of models, potentially by making use of automated meta-optimization methods that generate hundreds of versions of the same data and model but with different configuration parameters [35]. Deep learning can also add the requirement of specific hardware." (Arpteg et al 2018:53)
"The great advances that have been made in fields such as computer vision and speech recognition, have been accomplished by replacing a modular processing pipeline with large neural networks that are trained end-to-end [37]. In essence, transparency is traded for accuracy. This is an unavoidable reality." (Arpteg et al 2018:53)
"A major challenge in developing DL systems is the difficulties in estimating the results before a system has been trained and tested." (Arpteg et al 2018:53)
"1) Experiment Management: During the development of ML models, a large number of experiments are usually performed to identify the optimal model. Each experiment can differ from other experiments in a number of ways and it is important to ensure reproducible results for these experiments. To have reproducible results, it may be necessary to know the exact version of components such as: 1) Hardware (e.g. GPU models primarily) 2) Platform (e.g. operating system and installed packages) 3) Source code (e.g. model training and pre-processing) 4) Configuration (e.g. model configuration and preprocessing settings) 5) Training data (e.g. input signals and target values) 6) Model state (e.g. versions of trained models)." (Arpteg et al 2018:53)
"Resource Limitations: Working with data that require distributed system adds another magnitude of complexity compared to single machine solutions. It is not only the volume of the data that may require a distributed solution, but also computational needs for extracting and transforming data, training and evaluating the model, and/or serving the model in production." (Arpteg et al 2018:54)
"It is challenging to provide a sample that includes all the edge cases that may exist in the full dataset. Also, as the external world is dynamic and changes over time, new edge cases will continue to appear later in time." (Arpteg et al 2018:54)
"There can be many reasons for the need of special techniques to handle resource limitations such as a lack of memory (CPU or GPU), long training time, or low-latency serving needs." (Arpteg et al 2018:56)},
	file = {Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/79S64LYS/Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/WKSGQ2B7/8498185.html:text/html},
}

@inproceedings{bengio_modeling_2000,
	title = {Modeling {High}-{Dimensional} {Discrete} {Data} with {Multi}-{Layer} {Neural} {Networks}},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper/1999/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
	urldate = {2021-11-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Bengio, Samy},
	year = {2000},
	file = {Bengio_Bengio_2000_Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks.pdf:/Users/johannesreichle/Zotero/storage/SK5PW9Z6/Bengio_Bengio_2000_Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks.pdf:application/pdf},
}

@article{ranzato_sparse_nodate,
	title = {Sparse {Feature} {Learning} for {Deep} {Belief} {Networks}},
	abstract = {Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efﬁcient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.},
	language = {en},
	author = {Ranzato, Marc’Aurelio and Boureau, Y-Lan and LeCun, Yann},
	pages = {8},
	file = {Ranzato et al. - Sparse Feature Learning for Deep Belief Networks.pdf:/Users/johannesreichle/Zotero/storage/C89NLYZ5/Ranzato et al. - Sparse Feature Learning for Deep Belief Networks.pdf:application/pdf},
}

@article{okoli_guide_2010,
	title = {A guide to conducting a systematic literature review of information systems research},
	author = {Okoli, Chitu and Schabram, Kira},
	year = {2010},
	file = {Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:/Users/johannesreichle/Zotero/storage/VHAVNWH2/Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:application/pdf},
}

@book{ridley_literature_2012,
	title = {The {Literature} {Review}: {A} {Step}-by-{Step} {Guide} for {Students}},
	isbn = {978-1-4462-6804-9},
	shorttitle = {The {Literature} {Review}},
	abstract = {Lecturers - request an e-inspection copy of this text or contact your local SAGE representative to discuss your course needs.  This second edition of Diana Ridley's bestselling book provides a step-by-step guide to conducting a literature search and literature review, using cases and examples throughout to demonstrate best practice. Ridley outlines practical strategies for conducting a systematic search of the available literature, reading and note taking and writing up your literature review as part of an undergraduate research project, Masters dissertation or PhD thesis. New to this edition are:  Examples drawn from a wide range of disciplines  A new chapter on conducting systematic reviews  Increased guidance on evaluating the quality of online sources and online literature  Enhanced guidance in dealing with copyright and permissions issues.   Visit the Companion Website for The Literature Review This book also comes with a companion website containing a wide range of examples of successful literature reviews from various academic disciplines.  SAGE Study Skills are essential study guides for students of all levels. From how to write great essays and succeeding at university, to writing your undergraduate dissertation and doing postgraduate research, SAGE Study Skills help you get the best from your time at university. Visit the SAGE Study Skills hub for tips, quizzes and videos on study success!},
	language = {en},
	publisher = {SAGE},
	author = {Ridley, Diana},
	month = jul,
	year = {2012},
	keywords = {Study Aids / General},
}

@article{snyder_literature_2019,
	title = {Literature review as a research methodology: {An} overview and guidelines},
	volume = {104},
	issn = {0148-2963},
	shorttitle = {Literature review as a research methodology},
	url = {http://www.sciencedirect.com/science/article/pii/S0148296319304564},
	doi = {10.1016/j.jbusres.2019.07.039},
	abstract = {Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.},
	language = {en},
	urldate = {2020-04-06},
	journal = {Journal of Business Research},
	author = {Snyder, Hannah},
	month = nov,
	year = {2019},
	keywords = {Integrative review, Literature review, Research methodology, Synthesis, Systematic review},
	pages = {333--339},
	annote = {Extracted Annotations (23/11/2021, 16:18:00)"A literature review can broadly be described as a more or less systematic way of collecting and synthesizing previous research (Baumeister \& Leary, 1997; Tranfield, Denyer, \& Smart, 2003)." (Snyder 2019:333)"By integratingfindings and perspectives from many empiricalfindings, a literature review can address research questions with a power that no single study has." (Snyder 2019:333)"In addition, a literature review is an excellent way of synthesizing researchfindings to show evidence on a meta-level and to uncover areas in which more research is needed, which is a critical component of creating theoretical frameworks and building conceptual models" (Snyder 2019:333)"he paper has several contributions. First, this paper separates between different types of review methodologies; systematic," (Snyder 2019:333)"semi-systematic and integrative approaches and argues that depending on purpose and the quality of execution, each type of approach can be very effective. While systematic reviews have strict requirements for search strategy and selecting articles for inclusion in the review, they are effective in synthesizing what the collection of studies are showing in a particular question and can provide evidence of effect that can inform policy and practice." (Snyder 2019:334)"Instead, a semi-systematic review approach could be a good strategy for example map theoretical approaches or themes as well as identifying knowledge gaps within the literature. In some cases, a research question requires a more creative collection of data, in these cases; an integrative review approach can be useful when the purpose of the review is not to cover all articles ever published on the topic but rather to combine perspectives to create new theoretical models." (Snyder 2019:334)"mature topics, the purpose of using an integrative review method is to overview the knowledge base, to critically review and potentially reconceptualize, and to expand on the theoretical foundation of the specific topic as it develops." (Snyder 2019:336)"This includes selecting search terms and appropriate databases and deciding on inclusion and exclusion criteria." (Snyder 2019:337)"does it make a substantial, practical, or theoretical contribution?early stated and motivated?" (Snyder 2019:338)},
	file = {Snyder_2019_Literature review as a research methodology.pdf:/Users/johannesreichle/Zotero/storage/HJIZLCQU/Snyder_2019_Literature review as a research methodology.pdf:application/pdf;ScienceDirect Snapshot:/Users/johannesreichle/Zotero/storage/YBMI7WLU/S0148296319304564.html:text/html},
}

@article{webster_analyzing_2002,
	title = {Analyzing the past to prepare for the future: {Writing} a literature review},
	shorttitle = {Analyzing the past to prepare for the future},
	journal = {MIS quarterly},
	author = {Webster, Jane and Watson, Richard T.},
	year = {2002},
	note = {Publisher: JSTOR},
	pages = {xiii--xxiii},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/T9JG4FKL/4132319.html:text/html;Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:/Users/johannesreichle/Zotero/storage/76JDULLA/Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:application/pdf},
}

@book{zowghi_requirements_2014,
	address = {Berlin, Heidelberg},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Requirements {Engineering}},
	volume = {432},
	isbn = {978-3-662-43609-7 978-3-662-43610-3},
	url = {http://link.springer.com/10.1007/978-3-662-43610-3},
	language = {en},
	urldate = {2021-11-23},
	publisher = {Springer Berlin Heidelberg},
	editor = {Zowghi, Didar and Jin, Zhi and Junqueira Barbosa, Simone Diniz and Chen, Phoebe and Cuzzocrea, Alfredo and Du, Xiaoyong and Filipe, Joaquim and Kara, Orhun and Kotenko, Igor and Sivalingam, Krishna M. and Ślęzak, Dominik and Washio, Takashi and Yang, Xiaokang},
	year = {2014},
	doi = {10.1007/978-3-662-43610-3},
	file = {Zowghi and Jin - 2014 - Requirements Engineering.pdf:/Users/johannesreichle/Zotero/storage/RTD5ZNET/Zowghi and Jin - 2014 - Requirements Engineering.pdf:application/pdf},
}

@article{noauthor_ieee_1998,
	title = {{IEEE} {Standard} for a {Software} {Quality} {Metrics} {Methodology}},
	doi = {10.1109/IEEESTD.1998.243394},
	abstract = {A methodology for establishing quality requirements and identifying, implementing, analyzing and validating the process and product software quality metrics is defined. The methodology spans the entire software life-cycle.},
	journal = {IEEE Std 1061-1998},
	month = dec,
	year = {1998},
	note = {Conference Name: IEEE Std 1061-1998},
	keywords = {IEEE standards},
	pages = {i--},
	file = {1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:/Users/johannesreichle/Zotero/storage/NU3DCQRT/1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/7Z7IEYIP/749159.html:text/html},
}

@book{kotonya_requirements_1998,
	edition = {1st},
	title = {Requirements {Engineering}: {Processes} and {Techniques}},
	isbn = {978-0-471-97208-2},
	shorttitle = {Requirements {Engineering}},
	abstract = {Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws},
	publisher = {Wiley Publishing},
	author = {Kotonya, Gerald and Sommerville, Ian},
	year = {1998},
}

@incollection{chung_non-functional_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Non}-{Functional} {Requirements} in {Software} {Engineering}},
	isbn = {978-3-642-02463-4},
	url = {https://doi.org/10.1007/978-3-642-02463-4_19},
	abstract = {Essentially a software system’s utility is determined by both its functionality and its non-functional characteristics, such as usability, flexibility, performance, interoperability and security. Nonetheless, there has been a lop-sided emphasis in the functionality of the software, even though the functionality is not useful or usable without the necessary non-functional characteristics. In this chapter, we review the state of the art on the treatment of non-functional requirements (hereafter, NFRs), while providing some prospects for future directions.},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Conceptual {Modeling}: {Foundations} and {Applications}: {Essays} in {Honor} of {John} {Mylopoulos}},
	publisher = {Springer},
	author = {Chung, Lawrence and do Prado Leite, Julio Cesar Sampaio},
	editor = {Borgida, Alexander T. and Chaudhri, Vinay K. and Giorgini, Paolo and Yu, Eric S.},
	year = {2009},
	doi = {10.1007/978-3-642-02463-4_19},
	keywords = {alternatives, goal-oriented requirements engineering, NFRs, Non-functional requirements, requirements engineering, satisficing, selection criteria, softgoals},
	pages = {363--379},
	file = {Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:/Users/johannesreichle/Zotero/storage/GB3VDQR4/Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:application/pdf},
}
