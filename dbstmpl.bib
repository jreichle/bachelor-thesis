
@misc{beom_crnn_2021,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-09-18},
	author = {Beom},
	month = sep,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
	annote = {CRNN implementation (Keras)},
}

@misc{bhat_rajesh-bhatspark-ai-summit-2020-text-extraction_2021,
	title = {rajesh-bhat/spark-ai-summit-2020-text-extraction},
	copyright = {MIT},
	url = {https://github.com/rajesh-bhat/spark-ai-summit-2020-text-extraction/blob/43eeb1f1a27e6ae84dcb0ef4cc11494dcc61cafb/CRNN_CTC_wandb.ipynb},
	urldate = {2021-09-22},
	author = {Bhat, Rajesh Shreedhar},
	month = aug,
	year = {2021},
	note = {original-date: 2020-06-02T14:21:10Z},
}

@inproceedings{chen_improvement_2018,
	address = {Shanghai, China},
	title = {Improvement {Research} and {Application} of {Text} {Recognition} {Algorithm} {Based} on {CRNN}},
	isbn = {978-1-4503-6605-2},
	url = {http://dl.acm.org/citation.cfm?doid=3297067.3297073},
	doi = {10.1145/3297067.3297073},
	abstract = {This paper is based on CRNN model to recognize the text in the images of football matches scene, and two improvements are proposed. Considering the edge feature of text is strong, this paper adds MFM layers into CRNN model aiming to enhance the contrast. In order to solve the problem of losing details of image static features in the process of getting contextual features, this paper fuses up these two kinds of features. The training and testing experiments carried out on public dataset and manual dataset respectively verify the validity of the improvements, and the recognition accurate rate is higher than original model.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Signal} {Processing} and {Machine} {Learning} - {SPML} '18},
	publisher = {ACM Press},
	author = {Chen, Lei and Li, Shaobin},
	year = {2018},
	pages = {166--170},
	file = {Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:/Users/johannesreichle/Zotero/storage/4JUQTVGL/Chen and Li - 2018 - Improvement Research and Application of Text Recog.pdf:application/pdf},
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2021-09-24},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/M7XHZX79/backpropagation-in-convolutional-neural-networks.html:text/html},
}

@misc{shperber_gentle_2021,
	title = {A gentle introduction to {OCR}},
	url = {https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa},
	abstract = {How and why to apply deep learning to Optical Character Recognition},
	language = {en},
	urldate = {2021-09-18},
	journal = {Medium},
	author = {Shperber, Gidi},
	month = feb,
	year = {2021},
	annote = {Has a lot of approaches that can be checked},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/JSRM93VX/a-gentle-introduction-to-ocr-ee1469a201aa.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Filters, Independent component analysis, Inspection, Open source software, Optical character recognition software, Pipelines, Prototypes, Search engines, Testing, Text recognition},
	pages = {629--633},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/PFLR88V5/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TSDHL78C/4376991.html:text/html},
}

@misc{bhat_text_nodate,
	title = {Text {Recognition} {With} {CRNN}-{CTC} {Network} - {Weights} \& {Biases}},
	url = {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI},
	abstract = {Weights \& Biases, developer tools for machine learning},
	language = {en},
	urldate = {2021-09-22},
	journal = {W\&B},
	author = {Bhat, Rajesh Shreedhar},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/9Q9S8C2K/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI.html:text/html},
}

@inproceedings{kloss_learning_2016,
	address = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	language = {en},
	urldate = {2021-09-26},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	month = oct,
	year = {2016},
	pages = {5238--5245},
	file = {Kloss et al. - 2016 - Learning where to search using visual attention.pdf:/Users/johannesreichle/Zotero/storage/VJ2CKK4W/Kloss et al. - 2016 - Learning where to search using visual attention.pdf:application/pdf},
}

@book{dumas_fundamentals_2013,
	address = {Berlin, Heidelberg},
	title = {Fundamentals of {Business} {Process} {Management}},
	isbn = {978-3-642-33142-8 978-3-642-33143-5},
	url = {http://link.springer.com/10.1007/978-3-642-33143-5},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Dumas, Marlon and La Rosa, Marcello and Mendling, Jan and Reijers, Hajo A.},
	year = {2013},
	doi = {10.1007/978-3-642-33143-5},
	file = {Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:/Users/johannesreichle/Zotero/storage/XSJHQVLD/Dumas et al. - 2013 - Fundamentals of Business Process Management.pdf:application/pdf},
}

@article{frank_style-and-citation-guide_nodate,
	title = {style-and-citation-guide},
	language = {en},
	author = {Frank, Brigitte},
	pages = {5},
	file = {Frank - style-and-citation-guide.pdf:/Users/johannesreichle/Zotero/storage/8PYU73DA/Frank - style-and-citation-guide.pdf:application/pdf},
}

@book{johannesson_introduction_2021,
	address = {Cham},
	title = {An {Introduction} to {Design} {Science}},
	isbn = {978-3-030-78131-6 978-3-030-78132-3},
	url = {https://link.springer.com/10.1007/978-3-030-78132-3},
	language = {en},
	urldate = {2021-10-12},
	publisher = {Springer International Publishing},
	author = {Johannesson, Paul and Perjons, Erik},
	year = {2021},
	doi = {10.1007/978-3-030-78132-3},
	file = {Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:/Users/johannesreichle/Zotero/storage/69A2NAR7/Johannesson and Perjons - 2021 - An Introduction to Design Science.pdf:application/pdf},
}

@book{cox_translating_2017,
	address = {Berkeley, CA},
	title = {Translating {Statistics} to {Make} {Decisions}: {A} {Guide} for the {Non}-{Statistician}},
	isbn = {978-1-4842-2255-3 978-1-4842-2256-0},
	shorttitle = {Translating {Statistics} to {Make} {Decisions}},
	url = {http://link.springer.com/10.1007/978-1-4842-2256-0},
	language = {en},
	urldate = {2021-10-13},
	publisher = {Apress},
	author = {Cox, Victoria},
	year = {2017},
	doi = {10.1007/978-1-4842-2256-0},
	file = {Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:/Users/johannesreichle/Zotero/storage/S33M4ZEW/Cox - 2017 - Translating Statistics to Make Decisions A Guide .pdf:application/pdf},
}

@inproceedings{ponti_everything_2017,
	title = {Everything {You} {Wanted} to {Know} about {Deep} {Learning} for {Computer} {Vision} but {Were} {Afraid} to {Ask}},
	doi = {10.1109/SIBGRAPI-T.2017.12},
	abstract = {Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.},
	booktitle = {2017 30th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ponti, Moacir Antonelli and Ribeiro, Leonardo Sampaio Ferraz and Nazare, Tiago Santana and Bui, Tu and Collomosse, John},
	month = oct,
	year = {2017},
	note = {ISSN: 2474-0705},
	keywords = {CNN, Computational modeling, computer vision, Computer vision, deep learning, Gallium nitride, image processing, Image processing, machine learning, Machine learning, Tensile stress},
	pages = {17--41},
	annote = {Extracted Annotations (14/10/2021, 10:29:11)
"Deep Learning methods are currently the stateof-the-art in many Computer Vision and Image Processing problems," (Ponti et al 2017:17)
"This is mainly due to two reasons: the availability of labelled image datasets with millions of images [1], [2], and computer hardware that allowed to speed-up computations." (Ponti et al 2017:17)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/5DFA5TFR/Ponti et al. - 2017 - Everything You Wanted to Know about Deep Learning .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/U9DQFTKT/8250222.html:text/html},
}

@article{shrestha_review_2019,
	title = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	journal = {IEEE Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Deep learning, artificial intelligence, backpropagation, Computer architecture, convolution neural network, deep neural network architectures, Feature extraction, Feedforward neural networks, Machine learning algorithm, optimization, Recurrent neural networks, supervised and unsupervised learning, Training},
	pages = {53040--53065},
	annote = {Extracted Annotations (14/10/2021, 10:43:39)
"The painstakingly handcrafted feature extractors used in traditional learning, classication, and pattern recognition systems are not scalable for large-sized data sets." (Shrestha and Mahmood 2019:53040)
"Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures." (Shrestha and Mahmood 2019:53040)
"DNN is a type of neural network modeled as a multilayer perceptron (MLP) that is trained with algorithms to learn representations from data sets without any manual design of feature extractors. As the name Deep Learning suggests, it consists of higher or deeper number of processing layers, which contrasts with shallow learning model with fewer layers of units." (Shrestha and Mahmood 2019:53041)},
	annote = {Must read!
Good explanation of math behind CNN, LSTM but also optimization algoritghms (like SGD)},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/A5UHX3PQ/Shrestha and Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@book{balas_handbook_2019,
	address = {Cham},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Handbook of {Deep} {Learning} {Applications}},
	volume = {136},
	isbn = {978-3-030-11478-7 978-3-030-11479-4},
	url = {http://link.springer.com/10.1007/978-3-030-11479-4},
	language = {en},
	urldate = {2021-10-14},
	publisher = {Springer International Publishing},
	editor = {Balas, Valentina Emilia and Roy, Sanjiban Sekhar and Sharma, Dharmendra and Samui, Pijush},
	year = {2019},
	doi = {10.1007/978-3-030-11479-4},
	annote = {Used in Introduction for Motivation part-{\textgreater} different Applications (see TOC)},
	file = {Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:/Users/johannesreichle/Zotero/storage/EHB78HRJ/Balas et al. - 2019 - Handbook of Deep Learning Applications.pdf:application/pdf},
}

@book{prince_computer_2012,
	title = {Computer {Vision}: {Models}, {Learning}, and {Inference}},
	isbn = {978-1-107-01179-3},
	shorttitle = {Computer {Vision}},
	abstract = {This modern treatment of computer vision focuses on learning and inference in probabilistic models as a unifying theme. It shows how to use training data to learn the relationships between the observed image data and the aspects of the world that we wish to estimate, such as the 3D structure or the object class, and how to exploit these relationships to make new inferences about the world from new image data. With minimal prerequisites, the book starts from the basics of probability and model fitting and works up to real examples that the reader can implement and modify to build useful vision systems. Primarily meant for advanced undergraduate and graduate students, the detailed methodological presentation will also be useful for practitioners of computer vision. - Covers cutting-edge techniques, including graph cuts, machine learning, and multiple view geometry. - A unified approach shows the common basis for solutions of important computer vision problems, such as camera calibration, face recognition, and object tracking. - More than 70 algorithms are described in sufficient detail to implement. - More than 350 full-color illustrations amplify the text. - The treatment is self-contained, including all of the background mathematics. - Additional resources at www.computervisionmodels.com.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Prince, Simon J. D.},
	month = jun,
	year = {2012},
	note = {Google-Books-ID: PmrICLzHutgC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics},
}

@book{das_machine_2021,
	address = {Cham, Switzerland},
	series = {Studies in computational intelligence},
	title = {Machine learning algorithms for industrial applications},
	isbn = {978-3-030-50640-7},
	language = {en},
	number = {volume 907},
	publisher = {Springer},
	editor = {Das, Santosh Kumar and Das, Shom Prasad and Dey, Nilanjan and Hassanien, Aboul Ella},
	year = {2021},
	file = {Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:/Users/johannesreichle/Zotero/storage/4ABV9L7G/Das et al. - 2021 - Machine learning algorithms for industrial applica.pdf:application/pdf},
}

@book{singh_computer_2021,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Computer {Vision} and {Image} {Processing}: 5th {International} {Conference}, {CVIP} 2020, {Prayagraj}, {India}, {December} 4-6, 2020, {Revised} {Selected} {Papers}, {Part} {I}},
	volume = {1376},
	isbn = {9789811610851 9789811610868},
	shorttitle = {Computer {Vision} and {Image} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-1086-8},
	language = {en},
	urldate = {2021-10-17},
	publisher = {Springer Singapore},
	editor = {Singh, Satish Kumar and Roy, Partha and Raman, Balasubramanian and Nagabhushan, P.},
	year = {2021},
	doi = {10.1007/978-981-16-1086-8},
	file = {Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:/Users/johannesreichle/Zotero/storage/QEEQCRND/Singh et al. - 2021 - Computer Vision and Image Processing 5th Internat.pdf:application/pdf},
}

@article{oyedotun_deep_2015,
	title = {Deep {Learning} in {Character} {Recognition} {Considering} {Pattern} {Invariance} {Constraints}},
	volume = {7},
	issn = {2074904X, 20749058},
	url = {http://www.mecs-press.org/ijisa/ijisa-v7-n7/v7n7-1.html},
	doi = {10.5815/ijisa.2015.07.01},
	abstract = {Character recognition is a field of machine learning that has been under research for several decades. The particular success of neural networks in pattern recognition and therefore character recognition is laudable. Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex. This research work reviews some of the most successful pre-training approaches to initializing deep networks such as stacked auto encoders, and deep belief networks based on achieved error rates. More importantly, this research also parallels investigating the performance of deep networks on some common problems associated with pattern recognition systems such as translational invariance, rotational invariance, scale mismatch, and noise. To achieve this, Yoruba vowel characters databases have been used in this research.},
	language = {en},
	number = {7},
	urldate = {2021-10-17},
	journal = {International Journal of Intelligent Systems and Applications},
	author = {Oyedotun, Oyebade K. and Olaniyi, Ebenezer O. and Khashman, Adnan},
	month = jun,
	year = {2015},
	pages = {1--10},
	annote = {Extracted Annotations (22/10/2021, 16:59:10)
"Research has also long shown that a single hidden layer network has the capability to approximate any function; while, the problems associated with training deep networks therefore led to little attention given to it. Recently, the breakthrough in training deep networks through various pre-training schemes have led to the resurgence and massive interest in them, significantly outperforming shallow networks in several pattern recognition contests; moreover the more elaborate distributed representation of knowledge present in the different hidden layers concords with findings on the biological visual cortex." (Oyedotun et al 2015:1)
"Neural networks, conversely, can learn the features of task on which they are designed and trained; they can also adapt to some moderate variations such as noise on the data they have been trained with, hence considered intelligent. The success of neural networks in contrast to other non-intelligent recognition approaches is striking, based on performance, and somewhat ease of design considering the capability of neural networks in approximation any mapping function of inputs to outputs while requiring „least‟ domain specific knowledge for its programming each time it is embedded in different applications. i.e. self-programming." (Oyedotun et al 2015:1)
"consideration on the amount of common pattern invariance achievable in learning has also changed; such common invariances include relatively moderate translation, rotation, scale mismatch, and noisy patterns." (Oyedotun et al 2015:2)
"Conversely, this paper takes an alternative approach, by presenting a work which focuses on invariance learning based on the neural network architectures, rather than complex training data manipulation schemes and painstaking mathematical foundations. It is noteworthy that this research did not employ any invariant feature extraction technique; hence, investigates how neural network structures and learning paradigms affect invariance learning. Also, to reinforce the application importance of this work, real life data, „handwritten characters‟, have been used to train and simulate the considered networks." (Oyedotun et al 2015:3)
"Unfortunately, the difficulty is to synthesize, and then to efficiently compute, the classification function that maps objects to categories, given that objects in a category can have widely varying input representations [7]; bearing in mind also that this approach increases computational load on the designed system. A better approach is to consider neural network architectures that allow some level of built-in invariance due to structure; and of course, this can usually still be augmented with some handcrafted invariance achieved through data manipulation schemes." (Oyedotun et al 2015:3)
"Such features include:  distributed representation of knowledge at each hidden layer.  distinct features are extracted by units or neurons in each hidden layer.  several units can be active concurrently." (Oyedotun et al 2015:3)
"Generally, it is conceived that in deep networks, the first hidden layer extracts some primary features about the input, then these features are combined in the second layer to more defined features, and these features are further combined into well more defined features in the following layers, and so on. This can be somewhat seen as a hierarchical representation of knowledge;" (Oyedotun et al 2015:3)
"Deep learning depicts neural network architectures of more than a single hidden layer (multilayer networks); in contrast to networks of single hidden layer which are commonly referred to as shallow networks." (Oyedotun et al 2015:3)
"Generative Architectures This class of deep networks is not required to be deterministic of the class patterns that the inputs belong, but is used to sample joint statistical distribution of data; moreover this class of networks relies on unsupervised learning." (Oyedotun et al 2015:4)
"Discriminative Architectures Discriminative deep networks actually are required to be deterministic of the correlation of input data to the classes of patterns therein. Moreover, this category of networks relies on supervised learning." (Oyedotun et al 2015:4)
"Hybrid Architectures Networks that belong to this class rely on the combination of generative and discriminative approach in their architectures. Generally, such networks are generatively pre-trained and then discriminately finetuned for deterministic purposes." (Oyedotun et al 2015:4)
"The application of auto encoders and therefore generative architectures leverage on the unavailability of labelled data or the required logistics and cost that may be necessary in labeling available data. It therefore follows that generative learning suffices in situations where we have large unlabelled data and small labelled data" (Oyedotun et al 2015:4)
"The auto encoder can be seen as an encoder-decoder system, where the encoder (input-hidden layer pair) receives the input, extracting essential features for reconstruction; while the decoder (hidden-output layer pair) part receives the features extracted from the hidden layer, performing reconstruction at its best." (Oyedotun et al 2015:4)
"The training approach that is used in achieving learning in generative network architectures is known as „greedy layer-wise pre-training‟." (Oyedotun et al 2015:5)
"A DBN is a deep network, which is graphical and probabilistic in nature; it is essentially a generative model too. A belief net is a directed acyclic graph composed of stochastic variables [19]." (Oyedotun et al 2015:5)
"Such a training scheme is aimed at maximizing the likelihood of the input vector at a layer below given a configuration of a hidden layer that is directly on top of it." (Oyedotun et al 2015:6)
"A restricted Boltzmann machine has only two layers (fig.8); the input (visible) and the hidden layer. The connections between the two layers are undirected, and there are no interconnections between units of the same layer as in the general Boltzmann machine. We can therefore say that from the restriction in interconnections of units in layers, units are conditionally independent. The RBM can be seen as a Markov network, where the visible layer consists of either Bernoulli (binary) or Gaussian (real values usually between from 0 to 1) stochastic units, and the hidden layer of stochastic Bernoulli units [22]." (Oyedotun et al 2015:6)
"The simulation results on the considered invariances for the different trained networks are shown in table 3. It can be seen that the SDAE has the lowest error rate on translation, while the DBN outperformed other networks on rotational and scale invariances." (Oyedotun et al 2015:8)
"hence we can conjure that these networks were able to explore a more complex space of solutions while learning to the deep nature; since hierarchical learning allows more distributed knowledge representation." (Oyedotun et al 2015:8)
"It will be seen that the deep belief networks on the average, performed best compared to the other networks on variances like translation, rotation and scale mismatch; while its tolerance to noise decreased noticeably as the level of noise was increased as shown in table 4, table 5, and fig.15." (Oyedotun et al 2015:9)
"These variances are common constraints that occur in real life recognition systems for handwritten characters, and some of the solutions have been constraining the users (writers) to some particular possible domains of writing spaces or earmarked pattern of writing in order for low error rates to be achieved." (Oyedotun et al 2015:9)
"It has been shown that another flavour of neural networks, "convolutional networks" and its deep variant give very motivating performance on some of these constraints [25], however the complexity of these networks is somewhat obvious." (Oyedotun et al 2015:9)},
	file = {Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:/Users/johannesreichle/Zotero/storage/IWBWRJCE/Near East UniversityElectrical & Electronic Engineering, Lefkosa, via Mersin-10, TurkeyMember, Centre of Innovation for Artificial Intelligence, CiAi et al. - 2015 - Deep Learning in Character Recognition Considering.pdf:application/pdf},
}

@inproceedings{zhao_improving_2020,
	title = {Improving {Deep} {Learning} based {Optical} {Character} {Recognition} via {Neural} {Architecture} {Search}},
	doi = {10.1109/CEC48606.2020.9185798},
	abstract = {Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment. We compared it with different methods, and the experimental results proved the effectiveness of our method.},
	booktitle = {2020 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Zhao, Zhenyao and Jiang, Min and Guo, Shihui and Wang, Zhenzhong and Chao, Fei and Tan, Kay Chen},
	month = jul,
	year = {2020},
	keywords = {Optical character recognition software, Text recognition, Computer architecture, Feature extraction, Training, Object detection, Task analysis},
	pages = {1--7},
	annote = {Extracted Annotations (17/10/2021, 23:42:47)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)},
	annote = {Extracted Annotations (22/10/2021, 12:48:20)
"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one." (Zhao et al 2020:1)
"automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)
"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)
"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)
"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)
"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)
"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)
"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)
"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)
"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	annote = {Extracted Annotations (30/11/2021, 12:04:05)"Optical character rcecognition (OCR) is a process of converting images of typed, handwritten or printed text into machine-encoded one. In recent years, the methods represented by deep learning have greatly improved the performance of OCR systems, but the main challenges of such systems are 1) to accurately perform text detection in complex scenes and 2) to identify and set the optimal parameters to optimize the performance of the system. In this paper, we propose an OCR method based on Neural Architecture Search technique, called AutOCR. The characteristic of the proposed method is the automatic design of text detection framework using an evolutionary computation neural architecture search method. This design can not only accurately recognize the text in a complex environment, but also avoid the process of experts participating in parameter adjustment." (Zhao et al 2020:1)"OCR system includes two sub frameworks: text detection and text recognition. For a specific task, these two sub frameworks need to be designed according to the task requirements. For example, the mobile OCR system is more sensitive to the speed of operation, and the document OCR system requires higher recognition accuracy. Therefore, once the target task requirements change, the experts need to redesign the OCR system, which is time-consuming, laborintensive and inefficient. The automatic design of OCR system by machines can effectively alleviate this problem. However, there are very few existing works on automating the design of the OCR system." (Zhao et al 2020:1)"Network architecture search (NAS) automates the architecture design of the deep neural network, and has made great achievements in image classification, language models [11]-[14] and object detection [15]-[18] in recent years. Architectures designed by many state-of-the-art NAS methods have even achieved better performance than hand-crafted ones." (Zhao et al 2020:1)"n our AutOCR framework, text recognition framework uses the currently excellent tesseract engine [5], which can be trained for the special font of the target task." (Zhao et al 2020:1)"Compared with different OCR systems using Faster R-CNN [23], Mask R-CNN [8] or Yolo v3 [22], AutOCR achieves a comparable performance." (Zhao et al 2020:2)"OCR process can be divided into two phases: 1) Detect position coordinates containing text in input image. 2) Recognize text based on position coordinates. Compared to text recognition, text detection is often more challenging" (Zhao et al 2020:2)"ne type of solution [1]-[4], [6], [26] for text detection is to treat text in an image as a specific object and then detect it with an object detection framework." (Zhao et al 2020:2)"At present, CNN-based object detection can be divided into two major methods: two-step method based on R-CNN [23], [27] and one-step method based on YOLO [10], [22]. R-CNN based object detection: R-CNN uses the ability of convolutional neural networks (CNN) to extract image features. It views a detection problem as a classification problem leveraging the development of classification. It uses CNN to extract deep features of proposals generated by selective search [28] and then uses Support Vector Machine (SVM) to classify these features. YOLO based object detection: YOLO's approach is to extract feature maps on the entire image and then directly regresses the bounding boxes on the feature maps. SSD [10] is based on YOLO, which uses different aspect ratio boxes at different stages to predict the bounding box and further improve YOLO's performance. Generally, the two-step method is slower than the one-step method, but has higher accuracy. The DetNAS used in our framework is a two-step method." (Zhao et al 2020:2)"Mainstream methods are three types: reinforcement learning (RL) based approach, evolutionary algorithms (EA) and gradient-based approach" (Zhao et al 2020:2)},
	file = {Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:/Users/johannesreichle/Zotero/storage/SHJA8U7A/Zhao et al_2020_Improving Deep Learning based Optical Character Recognition via Neural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/G9CV8S98/9185798.html:text/html},
}

@misc{beom_crnn_2021-1,
	title = {{CRNN} ({CNN}+{RNN})},
	copyright = {MIT},
	url = {https://github.com/qjadud1994/CRNN-Keras},
	abstract = {CRNN (CNN+RNN) for OCR using Keras / License Plate Recognition},
	urldate = {2021-10-19},
	author = {Beom},
	month = oct,
	year = {2021},
	note = {original-date: 2018-01-14T07:52:25Z},
}

@article{liao_textboxes_2018,
	title = {{TextBoxes}++: {A} {Single}-{Shot} {Oriented} {Scene} {Text} {Detector}},
	volume = {27},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{TextBoxes}++},
	url = {http://arxiv.org/abs/1801.02765},
	doi = {10.1109/TIP.2018.2825107},
	abstract = {Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes\_plusplus},
	number = {8},
	urldate = {2021-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.02765},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {3676--3690},
	annote = {Comment: 15 pages},
	file = {Liao et al_2018_TextBoxes++.pdf:/Users/johannesreichle/Zotero/storage/BYY77N9D/Liao et al_2018_TextBoxes++.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/NPE7JJW5/1801.html:text/html},
}

@article{srivastava_comparative_2021,
	title = {Comparative analysis of deep learning image detection algorithms},
	volume = {8},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00434-w},
	doi = {10.1186/s40537-021-00434-w},
	abstract = {A computer views all kinds of visual media as an array of numerical values. As a consequence of this approach, they require image processing algorithms to inspect contents of images. This project compares 3 major image processing algorithms: Single Shot Detection (SSD), Faster Region based Convolutional Neural Networks (Faster R-CNN), and You Only Look Once (YOLO) to find the fastest and most efficient of three. In this comparative analysis, using the Microsoft COCO (Common Object in Context) dataset, the performance of these three algorithms is evaluated and their strengths and limitations are analysed based on parameters such as accuracy, precision and F1 score. From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Journal of Big Data},
	author = {Srivastava, Shrey and Divekar, Amit Vishvas and Anilkumar, Chandu and Naik, Ishika and Kulkarni, Ved and Pattabiraman, V.},
	month = dec,
	year = {2021},
	pages = {66},
	annote = {Extracted Annotations (16/11/2021, 15:54:27)"From the results of the analysis, it can be concluded that the suitability of any of the algorithms over the other two is dictated to a great extent by the use cases they are applied in. In an identical testing environment, YOLO-v3 outperforms SSD and Faster R-CNN, making it the best of the three algorithms." (Srivastava et al 2021:66)"Fast R-CNN model as a method of object detection [3]. It makes use of the CNN method in the target detection field. The novelty of the method proposed by Girshick has proposed a window extraction algorithm instead of a conventional sliding window extraction procedure in the R-CNN model, there is separate training for the deep convolution network for feature isolation and the support vector machines for categorization [4]. In the fast RCNN method they have combined feature extraction with classification into a classification framework" (Srivastava et al 2021:67)"Whereas in the faster R-CNN method the proposal isolation region and bit of Fast R-CNN are put into a network template referred to as region proposal network (RPN). The accuracy of Fast R-CNN and Faster R-CNN is the same." (Srivastava et al 2021:67)"You Only Look Once (YOLO)—A one-time convolutional neural network for the prediction of the frame position and classification of multiple candidates is offered by YOLO. Endto-end target detection can be achieved this way. It uses a regression problem to solve object detection. A single end-to-end system completes the process of putting the output obtained from the original image to the category and position" (Srivastava et al 2021:67)"advanced YOLO v1 network model which optimizes the loss of function in YOLO v1, it has a new inception model structure, has a specialized pooling pyramid layer, and has better performance." (Srivastava et al 2021:68)"According to the team, SSD is a simple method and requires an object proposal as it is based on the complete elimination of the process that generates a proposal. It also eliminates the subsequent pixel and resampling stages. So, it combines everything into a single step. SSD is also very easy to train and is very straightforward when it comes to integrating it into the system. This makes detection easier. The primary feature of SSD is using multiscale convolutional bounding box outputs that are attached to several feature maps" (Srivastava et al 2021:68)"The best feature of Tiny SSD is its size of 2.3 MB which is even smaller than Tiny YOLO." (Srivastava et al 2021:68)"The paper also accesses some deep learning techniques for object detection systems. The current paper states that deep CNNs work on the principle of weight sharing. It gives us information about some crucial points in CNN. These features of CNN depicted in this paper are: [1] a. CNN is integration and involves the multiplication of two overlapping functions. b. Features maps are abstracted to reduce their complexity in terms of space c. Repetition of the process is done to produce the feature maps using filters. d. CNN utilizes different types of pooling layers." (Srivastava et al 2021:68)"In this work the multi-layered system they introduced the Squeeze-and-Excitation model as an additional layer to the SSD model. The improved model employed self-learning that further enhanced the accuracy of the system for small scale pedestrian detection." (Srivastava et al 2021:69)"Architectural Innovations (2014-2020): The well-known and widely used VGG architecture was developed in 2014 [22]. RCNN, based on VGG like many others, introduced the idea that objects are located in certain regions of the image; hence the name: region-based CNN [23]. Improved versions of RCNN—Fast RCNN [24] and Faster RCNN [3] came out in the subsequent years. Both of these reduced computation time, while maintaining the accuracy that RCNN is known for. Single Shot Multibox Detector (SSD), also based on VGG was developed around 2016 [8]. Another algorithm, You Only Look Once (YOLO), based on an architecture called DarkNet was first published in 2016 [6]. It is in active development; its third version was released in 2018 [25]." (Srivastava et al 2021:70)"SSD does not resample pixels or features for bounding box hypotheses and is as accurate as models that do. In addition to this, it is quite straightforward compared to methods that require object proposals because it completely eradicates feature resampling stages or pixel and proposal generation, by encompassing all computation in a single network. Therefore, SSD is very simple to train and can be easily integrated into systems that perform detection as one of their functions [8]. It's architecture heavily depends on the generation of bounding boxes and the extraction of feature maps, which are also known as default bounding boxes. Loss is calculated by the network, using comparisons of the offsets of the predicted classes and the default bounding boxes with the training samples' ground truth values, using different filters for every iteration. Using the back-propagation algorithm and the calculated loss value, all the parameters are updated." (Srivastava et al 2021:71)"SSD is built on a feed-forward complex network that builds a collection of standardsize bounding boxes and for each occurrence of an object in those boxes, a respective score. After score generation, non-maximum suppression is used to generate the final detection results. The preliminary network layers are built on a standard architecture utilized for high quality image classification (and truncated before any classification layers), which is a VGG-16 network. An auxiliary structure is added to the truncated base network such as convo6 to produce detections." (Srivastava et al 2021:71)"The reason for using auxiliary layers is because they allow us to extract the required features at multiple scales as well as reduce the size of our input with each layer that is traversed through [8]. For each cell in the image, the layer makes a certain number of predications. Each prediction consists of a boundary box and the box generates scores for all the classes it detects in this box including a score for no object at all." (Srivastava et al 2021:71)"Convolutional predictors for object detection: Every feature layer produces a fixed number of predictions by utilising convolutional filters. For every feature layer of size x × y having n channels, the rudimentary component for generating prediction variables of a potential detection result is a 3 × 3 × x small kernel that creates a confidence score for every class," (Srivastava et al 2021:71)non-maximum suppression: algorithm to find best proposed bounding box with IOU (note on p.71)"This computation results in a total of (s + 4) b filters that are applicable to every location in the feature map, resulting in (s + 4) × b × x × y outputs for a x × y feature ma" (Srivastava et al 2021:72)"All SSD predictions are divided into two types; negative matches or positive matches. Positive matches are only used by SSD to calculate the localization cost which is the misalignment of the boundary box with the default box." (Srivastava et al 2021:72)"However, SSD is not as efficient at detection for smaller objects, which can be solved by having a more efficient feature extractor backbone (e.g., ResNet101), with the addition of deconvolution layers along with skip connections to create additional large-scale context, and design a better network structure" (Srivastava et al 2021:73)"The algorithm of the original R-CNN technique is as follows: [29] 1. Using a Selective Search Algorithm, several candidate region proposals are extracted from the input image. In this algorithm, numerous candidate regions are generated in initial sub-segmentation. Then, regions which are similar are combined to form bigger regions using a greedy algorithm. These regions make up the final region proposals. 2. The CNN component warps the proposals and extracts distinct features as a vector output. 3. The features which are extracted are fed into an SVM (Support Vector Machine) for recognizing objects of interest in the proposal." (Srivastava et al 2021:75)"Fast R-CNN is an algorithm for object detection that solves some of the drawbacks of R-CNN. It uses an approach similar to that of its predecessor, but as opposed to using region proposals, the CNN utilizes the image itself for creating a convolutional feature map, following which region proposals are determined and warped from it. An RoI (Region of Interest) pooling layer is employed for reshaping the warped squares according to a predefined size for a fully connected layer to accept them. The region class is then predicted from the RoI vector with the help of a SoftMax laye" (Srivastava et al 2021:75)"YOLOv3 In modern times YOLO (You Only Look Once) is one of the most precise and accurate object detection algorithms available. It has been made on the basis of a newly altered and customized architecture named Darknet [25]" (Srivastava et al 2021:77)"YOLOv3 makes use of the latest darknet features like 53 layers and it has undergone training with one of the most reliable datasets called ImageNet. The layers used are from an architecture Darnnet-53 which is convolutional in nature. For detection, the aforementioned 53 layers were supplemented instead of the pre-existing 19 and this enhanced architecture was trained and instructed with PASCAL VOC." (Srivastava et al 2021:77)"It was found that Yolo-v3 is the fastest with SSD following closely and Faster RCNN coming in the last place. However, it can be said that the use case influences which algorithm is picked; if you are dealing with a relatively small dataset and don't need real-time results, it is best to go with Faster RCNN. Yolo-v3 is the one to pick if you need to analyse a live video feed. Meanwhile, SSD provides a good balance between speed and accuracy." (Srivastava et al 2021:90)},
	file = {Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:/Users/johannesreichle/Zotero/storage/2EQVX9L2/Srivastava et al. - 2021 - Comparative analysis of deep learning image detect.pdf:application/pdf},
}

@article{yang_learning_2021,
	title = {Learning {High}-{Precision} {Bounding} {Box} for {Rotated} {Object} {Detection} via {Kullback}-{Leibler} {Divergence}},
	url = {http://arxiv.org/abs/2106.01883},
	abstract = {Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios. Taking the perspective that horizontal detection is a special case for rotated object detection, in this paper, we are motivated to change the design of rotation regression loss from induction paradigm to deduction methodology, in terms of the relation between rotation and horizontal detection. We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio. This mechanism can be vital for high-precision detection as a slight angle error would cause a serious accuracy drop for large aspect ratios objects. More importantly, we have proved that KLD is scale invariant. We further show that the KLD loss can be degenerated into the popular \$l\_\{n\}\$-norm loss for horizontal detection. Experimental results on seven datasets using different detectors show its consistent superiority, and codes are available at https://github.com/yangxue0827/RotationDetection.},
	urldate = {2021-11-02},
	journal = {arXiv:2106.01883 [cs]},
	author = {Yang, Xue and Yang, Xiaojiang and Yang, Jirui and Ming, Qi and Wang, Wentao and Tian, Qi and Yan, Junchi},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01883},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 16 pages, 5 figures, 8 tables, accepted by NeurIPS21, codes are available at https://github.com/yangxue0827/RotationDetection},
	annote = {Extracted Annotations (04/11/2021, 18:11:57)
"Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios." (Yang et al 2021:1)
"We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. It will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio." (Yang et al 2021:1)
"In this paper, we take a step back, and aim to develop (from a deductive perspective) a unified regression framework for rotation detection and its special case: horizontal detection. In fact, our new framework enjoys a coherent property that it can be degenerated into the current commonly used regression loss (e.g. ln -norm) in special cases (horizontal detection), as shown in Figure 1(b)." (Yang et al 2021:2)
"The mainstream classical object detection algorithms can be roughly divided according to the following standards: Two- [7, 8, 9, 11] or Single-stage [10, 18, 19] object detection, Anchor-free [20, 21, 22] or Anchor-based [8, 9, 10] object detection and CNN [8, 10, 20] or Transformer-based [23, 24] object detection. Although the pipelines may vary, the mainstream regression loss often uses the popular ln -norm loss (such as smooth L1 loss) or IoU-based loss" (Yang et al 2021:3)
"However, horizontal detectors do not provide accurate orientation and scale information." (Yang et al 2021:3)
"The overall regression loss for rotation detection is: Lreg = ln -norm (tx ; ty ; tw ; th ; t )" (Yang et al 2021:4)
"It can be seen that parameters are optimized independently, making the loss (or detection accuracy) sensitive to the under-fitting of any of the parameters. This mechanism is fatal to high-precision detection." (Yang et al 2021:4)
text has long aspect ratios -{\textgreater} angle parameter very important -{\textgreater} KLD approach is favored (note on p.4)
 
"Although GWD scheme has played a preliminary exploration of the deductive paradigm, it does not focus on achieving high-precision detection and scale invariance. In the following, we will propose our new approach based on the Kullback-Leibler divergence (KLD)" (Yang et al 2021:5)
Gaussians are constructed like with GWD scheme (note on p.5)
 
"ICDAR2015, MLT and MSRA-TD500 are commonly used for oriented scene text detection and spotting. ICDAR2015 includes 1,000 training images and 500 testing images. ICDAR2017 MLT is a multi-lingual text dataset, which includes 7,200 training images, 1,800 validation images and 9,000 testing images. MSRA-TD500 dataset consists of 300 training images and 200 testing images." (Yang et al 2021:7)
"Limitations. Despite the theoretical grounds and the promising experimental justifications, our method has an obvious limitation that it cannot be directly applied to quadrilateral detection [33, 44]." (Yang et al 2021:10)},
	file = {Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:/Users/johannesreichle/Zotero/storage/JF2GVMIQ/Yang et al_2021_Learning High-Precision Bounding Box for Rotated Object Detection via.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/H7Q9M3ZH/2106.html:text/html},
}

@article{lu_soft_2021,
	title = {{SOFT}: {Softmax}-free {Transformer} with {Linear} {Complexity}},
	shorttitle = {{SOFT}},
	url = {http://arxiv.org/abs/2110.11945},
	abstract = {Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.11945 [cs]},
	author = {Lu, Jiachen and Yao, Jinghan and Zhang, Junge and Zhu, Xiatian and Xu, Hang and Gao, Weiguo and Xu, Chunjing and Xiang, Tao and Zhang, Li},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.11945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: NeurIPS 2021 Spotlight. Project page at https://fudan-zvg.github.io/SOFT/},
	file = {Lu et al_2021_SOFT.pdf:/Users/johannesreichle/Zotero/storage/W6LD7EJZ/Lu et al_2021_SOFT.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/ZXCCUE46/2110.html:text/html},
}

@inproceedings{raisi_transformer-based_2021,
	address = {Nashville, TN, USA},
	title = {Transformer-based {Text} {Detection} in the {Wild}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9522851/},
	doi = {10.1109/CVPRW53098.2021.00353},
	abstract = {A major limitation to most state-of-the-art visual localization methods is their ineptitude to make use of ubiquitous signs and directions that are typically intuitive to humans. Localization methods can greatly beneﬁt from a system capable of reasoning about a variety of cues beyond low-level features, such as street signs, store names, building directories, room numbers, etc.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Raisi, Zobeir and Naiel, Mohamed A. and Younes, Georges and Wardell, Steven and Zelek, John S.},
	month = jun,
	year = {2021},
	pages = {3156--3165},
	file = {Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:/Users/johannesreichle/Zotero/storage/Y6FVVXIN/Raisi et al. - 2021 - Transformer-based Text Detection in the Wild.pdf:application/pdf},
}

@misc{noauthor_accepted_nodate,
	title = {Accepted {Papers}},
	url = {https://nips.cc/Conferences/2021/AcceptedPapersInitial},
	urldate = {2021-11-05},
	file = {Accepted Papers:/Users/johannesreichle/Zotero/storage/BQ5SUEPT/AcceptedPapersInitial.html:text/html},
}

@article{sheng_centripetaltext_2021,
	title = {{CentripetalText}: {An} {Efficient} {Text} {Instance} {Representation} for {Scene} {Text} {Detection}},
	shorttitle = {{CentripetalText}},
	url = {http://arxiv.org/abs/2107.05945},
	abstract = {Scene text detection remains a grand challenge due to the variation in text curvatures, orientations, and aspect ratios. One of the hardest problems in this task is how to represent text instances of arbitrary shapes. Although many methods have been proposed to model irregular texts in a flexible manner, most of them lose simplicity and robustness. Their complicated post-processings and the regression under Dirac delta distribution undermine the detection performance and the generalization ability. In this paper, we propose an efficient text instance representation named CentripetalText (CT), which decomposes text instances into the combination of text kernels and centripetal shifts. Specifically, we utilize the centripetal shifts to implement pixel aggregation, guiding the external text pixels to the internal text kernels. The relaxation operation is integrated into the dense regression for centripetal shifts, allowing the correct prediction in a range instead of a specific value. The convenient reconstruction of text contours and the tolerance of prediction errors in our method guarantee the high detection accuracy and the fast inference speed, respectively. Besides, we shrink our text detector into a proposal generation module, namely CentripetalText Proposal Network, replacing Segmentation Proposal Network in Mask TextSpotter v3 and producing more accurate proposals. To validate the effectiveness of our method, we conduct experiments on several commonly used scene text benchmarks, including both curved and multi-oriented text datasets. For the task of scene text detection, our approach achieves superior or competitive performance compared to other existing methods, e.g., F-measure of 86.3\% at 40.0 FPS on Total-Text, F-measure of 86.1\% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text recognition, our method outperforms Mask TextSpotter v3 by 1.1\% on Total-Text.},
	urldate = {2021-11-06},
	journal = {arXiv:2107.05945 [cs]},
	author = {Sheng, Tao and Chen, Jie and Lian, Zhouhui},
	month = oct,
	year = {2021},
	note = {arXiv: 2107.05945},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by NeurIPS 2021},
	file = {Sheng et al_2021_CentripetalText.pdf:/Users/johannesreichle/Zotero/storage/GAFQN94L/Sheng et al_2021_CentripetalText.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HZFSY2GG/2107.html:text/html},
}

@article{ma_arbitrary-oriented_2018,
	title = {Arbitrary-{Oriented} {Scene} {Text} {Detection} via {Rotation} {Proposals}},
	volume = {20},
	issn = {1941-0077},
	doi = {10.1109/TMM.2018.2818020},
	abstract = {This paper introduces a novel rotation-based framework for arbitrary-oriented text detection in natural scene images. We present the Rotation Region Proposal Networks, which are designed to generate inclined proposals with text orientation angle information. The angle information is then adapted for bounding box regression to make the proposals more accurately fit into the text region in terms of the orientation. The Rotation Region-of-Interest pooling layer is proposed to project arbitrary-oriented proposals to a feature map for a text region classifier. The whole framework is built upon a region-proposal-based architecture, which ensures the computational efficiency of the arbitrary-oriented text detection compared with previous text detection systems. We conduct experiments using the rotation-based framework on three real-world scene text detection datasets and demonstrate its superiority in terms of effectiveness and efficiency over previous approaches.},
	number = {11},
	journal = {IEEE Transactions on Multimedia},
	author = {Ma, Jianqi and Shao, Weiyuan and Ye, Hao and Wang, Li and Wang, Hong and Zheng, Yingbin and Xue, Xiangyang},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Pipelines, Computer architecture, Task analysis, arbitrary oriented, Image edge detection, Microsoft Windows, Proposals, Robustness, rotation proposals, Scene text detection},
	pages = {3111--3122},
	file = {Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:/Users/johannesreichle/Zotero/storage/MUTNDE2W/Ma et al_2018_Arbitrary-Oriented Scene Text Detection via Rotation Proposals.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/N8FNCBBL/8323240.html:text/html},
}

@misc{goswami_deeper_2018,
	title = {A deeper look at how {Faster}-{RCNN} works},
	url = {https://whatdhack.medium.com/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd},
	abstract = {Faster-RCNN is one of the most well known object detection neural networks [1,2]. It is also the basis for many derived networks for…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = jul,
	year = {2018},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/FCB2UPLV/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd.html:text/html},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-11-06},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick_2015_Fast R-CNN.pdf:/Users/johannesreichle/Zotero/storage/WHTYMPGV/Girshick_2015_Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/X3L8V8HR/1504.html:text/html},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-11-06},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {Ren et al_2016_Faster R-CNN.pdf:/Users/johannesreichle/Zotero/storage/G6DDTA5M/Ren et al_2016_Faster R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/DRXQUEDA/1506.html:text/html},
}

@misc{goswami_comparison_2020,
	title = {Comparison of {Faster}-{RCNN} and {Detection} {Transformer} ({DETR})},
	url = {https://whatdhack.medium.com/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04},
	abstract = {Faster-RCNN is a well known network, arguably the gold standard, in object detection and segmentation. Detection Transformer ( DETR) on…},
	language = {en},
	urldate = {2021-11-06},
	journal = {Medium},
	author = {Goswami, Subrata},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/RAFM6M6L/comparison-of-faster-rcnn-and-detection-transformer-detr-f67c2f5a2a04.html:text/html},
}

@misc{hwalsuklee_awesome-deep-text-detection-recognition_2021,
	title = {awesome-deep-text-detection-recognition},
	copyright = {Apache-2.0},
	url = {https://github.com/hwalsuklee/awesome-deep-text-detection-recognition},
	abstract = {A curated list of resources for text detection/recognition (optical character recognition ) with deep learning methods.},
	urldate = {2021-11-11},
	author = {hwalsuklee},
	month = nov,
	year = {2021},
	note = {original-date: 2017-11-30T05:43:19Z},
	keywords = {awesome-list, awesome-lists, deep-learning, ocr, ocr-detection, ocr-paper, ocr-paper-list, ocr-papers, ocr-recognition, text-detection, text-detection-recognition, text-recognition},
}

@article{mittal_deep_2020,
	title = {Deep learning-based object detection in low-altitude {UAV} datasets: {A} survey},
	volume = {104},
	shorttitle = {Deep learning-based object detection in low-altitude {UAV} datasets},
	doi = {10.1016/j.imavis.2020.104046},
	abstract = {Deep learning-based object detection solutions emerged from computer vision has captivated full attention in recent years. The growing UAV market trends and interest in potential applications such as surveillance, visual navigation, object detection, and sensors-based obstacle avoidance planning have been holding good promises in the area of deep learning. Object detection algorithms implemented in deep learning framework have rapidly became a method for processing of moving images captured from drones. The primary objective of the paper is to provide a comprehensive review of the state of the art deep learning based object detection algorithms and analyze recent contributions of these algorithms to low altitude UAV datasets. The core focus of the studies is low-altitude UAV datasets because relatively less contribution was seen in the literature when compared with standard or remote-sensing based datasets. The paper discusses the following algorithms: Faster RCNN, Cascade RCNN, R-FCN etc. into two-stage, YOLO and its variants, SSD, RetinaNet into one-stage and CornerNet, Objects as Point etc. under advanced stages in deep learning based detectors. Further, one-two and advanced stages of detectors are studied in detail focusing on low-altitude UAV datasets. The paper provides a broad summary of low altitude datasets along with their respective literature in detection algorithms for the potential use of researchers. Various research gaps and challenges for object detection and classification in UAV datasets that need to deal with for improving the performance are also listed.},
	journal = {Image and Vision Computing},
	author = {Mittal, Payal and Singh, Raman and Sharma, Akashdeep},
	month = dec,
	year = {2020},
	pages = {104046},
	file = {Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:/Users/johannesreichle/Zotero/storage/RHDJAA7Z/Mittal et al_2020_Deep learning-based object detection in low-altitude UAV datasets.pdf:application/pdf},
}

@misc{noauthor_one-stage_nodate,
	title = {One-stage object detection},
	url = {https://machinethink.net/blog/object-detection/},
	urldate = {2021-11-16},
	file = {One-stage object detection:/Users/johannesreichle/Zotero/storage/U2Q8RYF5/object-detection.html:text/html},
}

@article{saunders_layers_2012,
	title = {The {Layers} of {Research} {Design}},
	url = {https://www.academia.edu/4107831/The_Layers_of_Research_Design},
	abstract = {Within this article we use the metaphor of the “Research Onion” (Saunders et al., 2012: 128) to illustrate how these final elements (the core of the research onion) need to be considered in relation to other design elements (the outer layers of the},
	language = {en},
	urldate = {2021-11-17},
	journal = {Rapport},
	author = {Saunders, Mark N. K.},
	year = {2012},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/MKLS6E4Z/The_Layers_of_Research_Design.html:text/html},
}

@article{qiao_text_2021,
	title = {Text {Perceptron}: {Towards} {End}-to-{End} {Arbitrary}-{Shaped} {Text} {Spotting}},
	shorttitle = {Text {Perceptron}},
	url = {http://arxiv.org/abs/2002.06820},
	abstract = {Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text.},
	urldate = {2021-11-17},
	journal = {arXiv:2002.06820 [cs]},
	author = {Qiao, Liang and Tang, Sanli and Cheng, Zhanzhan and Xu, Yunlu and Niu, Yi and Pu, Shiliang and Wu, Fei},
	month = oct,
	year = {2021},
	note = {arXiv: 2002.06820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by AAAI2020. Code is available at https://davar-lab.github.io/publication.html or https://github.com/hikopensource/DAVAR-Lab-OCR},
	file = {Qiao et al_2021_Text Perceptron.pdf:/Users/johannesreichle/Zotero/storage/GE89LJ5N/Qiao et al_2021_Text Perceptron.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/SA69IXXA/2002.html:text/html},
}

@article{torraco_writing_2005,
	title = {Writing {Integrative} {Literature} {Reviews}: {Guidelines} and {Examples}},
	volume = {4},
	issn = {1534-4843},
	shorttitle = {Writing {Integrative} {Literature} {Reviews}},
	url = {https://doi.org/10.1177/1534484305278283},
	doi = {10.1177/1534484305278283},
	abstract = {The integrative literature review is a distinctive form of research that generates new knowledge about the topic reviewed. Little guidance is available on how to write an integrative literature review. This article discusses how to organize and write an integrative literature review and cites examples of published integrative literature reviews that illustrate how this type of research has made substantive contributions to the knowledge base of human resource development.},
	language = {en},
	number = {3},
	urldate = {2021-11-18},
	journal = {Human Resource Development Review},
	author = {Torraco, Richard J.},
	month = sep,
	year = {2005},
	note = {Publisher: SAGE Publications},
	keywords = {integrative literature review, integrative research review: synthesis, literature review},
	pages = {356--367},
	annote = {Extracted Annotations (12/12/2021, 13:44:15)
"TABLE 2: A Checklist for Writing an Integrative Literature Review" (Torraco 2005:365)},
	file = {Torraco_2005_Writing Integrative Literature Reviews.pdf:/Users/johannesreichle/Zotero/storage/49PU84KP/Torraco_2005_Writing Integrative Literature Reviews.pdf:application/pdf},
}

@inproceedings{ye_textfusenet_2020,
	address = {Yokohama, Japan},
	title = {{TextFuseNet}: {Scene} {Text} {Detection} with {Richer} {Fused} {Features}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {{TextFuseNet}},
	url = {https://www.ijcai.org/proceedings/2020/72},
	doi = {10.24963/ijcai.2020/72},
	abstract = {Arbitrary shape text detection in natural scenes is an extremely challenging task. Unlike existing text detection approaches that only perceive texts based on limited feature representations, we propose a novel framework, namely TextFuseNet, to exploit the use of richer features fused for text detection. More speciﬁcally, we propose to perceive texts from three levels of feature representations, i.e., character-, word- and global-level, and then introduce a novel text representation fusion technique to help achieve robust arbitrary text detection. The multi-level feature representation can adequately describe texts by dissecting them into individual characters while still maintaining their general semantics. TextFuseNet then collects and merges the texts’ features from different levels using a multi-path fusion architecture which can effectively align and fuse different representations. In practice, our proposed TextFuseNet can learn a more adequate description of arbitrary shapes texts, suppressing false positives and producing more accurate detection results. Our proposed framework can also be trained with weak supervision for those datasets that lack character-level annotations. Experiments on several datasets show that the proposed TextFuseNet achieves state-of-the-art performance. Speciﬁcally, we achieve an F-measure of 94.3\% on ICDAR2013, 92.1\% on ICDAR2015, 87.1\% on Total-Text and 86.6\% on CTW-1500, respectively.},
	language = {en},
	urldate = {2021-11-18},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ye, Jian and Chen, Zhe and Liu, Juhua and Du, Bo},
	month = jul,
	year = {2020},
	pages = {516--522},
	file = {Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:/Users/johannesreichle/Zotero/storage/DDEV4B5A/Ye et al. - 2020 - TextFuseNet Scene Text Detection with Richer Fuse.pdf:application/pdf},
}

@inproceedings{chen_tinynet_2019,
	address = {Beijing, China},
	title = {{TinyNet}: {A} {Lightweight}, {Modular}, and {Unified} {Network} {Architecture} for {The} {Internet} of {Things}},
	isbn = {978-1-4503-6886-5},
	shorttitle = {{TinyNet}},
	url = {http://dl.acm.org/citation.cfm?doid=3342280.3342290},
	doi = {10.1145/3342280.3342290},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2019 {Conference} {Posters} and {Demos} on   - {SIGCOMM} {Posters} and {Demos} '19},
	publisher = {ACM Press},
	author = {Chen, Gonglong and Wang, Yihui and Li, Huikang and Dong, Wei},
	year = {2019},
	pages = {9--11},
	file = {Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:/Users/johannesreichle/Zotero/storage/93RHUC63/Chen et al. - 2019 - TinyNet A Lightweight, Modular, and Unified Netwo.pdf:application/pdf},
}

@article{wang_pyramid_2021,
	title = {Pyramid {Vision} {Transformer}: {A} {Versatile} {Backbone} for {Dense} {Prediction} without {Convolutions}},
	shorttitle = {Pyramid {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2102.12122},
	abstract = {Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer{\textasciitilde}(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.},
	urldate = {2021-11-23},
	journal = {arXiv:2102.12122 [cs]},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
	month = aug,
	year = {2021},
	note = {arXiv: 2102.12122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICCV 2021},
	file = {Wang et al_2021_Pyramid Vision Transformer.pdf:/Users/johannesreichle/Zotero/storage/BV7XAMYM/Wang et al_2021_Pyramid Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8AR4JD76/2102.html:text/html},
}

@inproceedings{benali_amjoud_convolutional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Convolutional {Neural} {Networks} {Backbones} for {Object} {Detection}},
	isbn = {978-3-030-51935-3},
	doi = {10.1007/978-3-030-51935-3_30},
	abstract = {Detecting objects in images is an extremely important step in many image and video analysis applications. Object detection is considered as one of the main challenges in the field of computer vision, which focuses on identifying and locating objects of different classes in an image. In this paper, we aim to highlight the important role of deep learning and convolutional neural networks in particular in the object detection task. We analyze and focus on the various state-of-the-art convolutional neural networks serving as a backbone in object detection models. We test and evaluate them in the common datasets and benchmarks up-to-date. We Also outline the main features of each architecture. We demonstrate that the application of some convolutional neural network architectures has yielded very promising state-of-the-art results in image classification in the first place and then in the object detection task. The results have surpassed all the traditional methods, and in some cases, outperformed the human being’s performance.},
	language = {en},
	booktitle = {Image and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Benali Amjoud, Ayoub and Amrouch, Mustapha},
	editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
	year = {2020},
	keywords = {Object detection, Convolutional neural networks, Review},
	pages = {282--289},
	file = {Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:/Users/johannesreichle/Zotero/storage/EHNQ56PL/Benali Amjoud_Amrouch_2020_Convolutional Neural Networks Backbones for Object Detection.pdf:application/pdf},
}

@misc{noauthor_new_nodate,
	title = {New mobile neural network architectures},
	url = {https://machinethink.net/blog/mobile-architectures/},
	urldate = {2021-11-23},
	file = {New mobile neural network architectures:/Users/johannesreichle/Zotero/storage/832WZH9H/mobile-architectures.html:text/html},
}

@inproceedings{vogelsang_requirements_2019,
	title = {Requirements {Engineering} for {Machine} {Learning}: {Perspectives} from {Data} {Scientists}},
	shorttitle = {Requirements {Engineering} for {Machine} {Learning}},
	doi = {10.1109/REW.2019.00050},
	abstract = {Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Vogelsang, Andreas and Borg, Markus},
	month = sep,
	year = {2019},
	keywords = {machine learning, Adaptation models, Analytical models, Business, data science, Encoding, interview study, Interviews, requirements engineering, Requirements engineering, Synthetic aperture sonar},
	pages = {245--251},
	annote = {Extracted Annotations (15/12/2021, 16:57:40)"characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems." (Vogelsang and Borg 2019:245)"We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process." (Vogelsang and Borg 2019:245)"In addition, a recent survey suggests that Requirements Engineering (RE) is the most difficult activity for the development of ML-based systems [2]." (Vogelsang and Borg 2019:245)"In a recent survey, Ishikawa and Yoshioka [2] reported that RE was listed as the most difficult activity for the development of ML-based systems: "The dominant concerns [. . . ] pertain to decision making with the customers. In the conventional setting, this activity involved requirements analysis and specification in the initial phase and an acceptance inspection in the final phase. This activity flow is not possible when working with ML-based systems due to the impossibility of prior estimation or assurance of achievable accuracy."" (Vogelsang and Borg 2019:246)"She argues that there is no unified collection or consideration of many NFRs for ML, including a consideration of ML-specific quality trade-off data." (Vogelsang and Borg 2019:246)"stressed the importance of quality targets for ML models: "For a successful project, the requirements must be clear. Especially the evaluation metric must be specified". In ML systems, the quality of the resulting predictions can be considered a functional requirement (P3: "I consider predictive power as functional requirement."" (Vogelsang and Borg 2019:247)"Quantification of quality targets is certainly also a challenge for conventional software [22], but training an ML model to go beyond a certain utility breakpoint turns into a functional requirement in practice." (Vogelsang and Borg 2019:247)""Explainability is twofold: On the one side, there is a need to explain the model (what has been learned). On the other side, there is a need to explain single predictions of the model."" (Vogelsang and Borg 2019:248)""If there is a combination or transformation of features that is smaller but has a similar performance, we prefer that. [. . . ] We try to minimize the number of features to make the model more explainable."" (Vogelsang and Borg 2019:248)"ML systems are designed to discriminate. ML algorithms identify recurring patterns in data (i.e., stereotypes) and apply these patterns to judge about unseen data." (Vogelsang and Borg 2019:248)"An example is how the General Data Protection Regulation (GDPR) constrains that personal data can only be used in ways specified by an explicit consent." (Vogelsang and Borg 2019:248)"Training data is an integral part of any ML system. We envision that requirements for (training) data play a larger role for specifying ML systems than for conventional systems. We may even have data requirements as a new class of requirements." (Vogelsang and Borg 2019:248)"Based on our interviews, we would add "training data needs specified and validated requirements like code"." (Vogelsang and Borg 2019:249)""You could try, but it won't help" - the data is what it is, and it is up to the data scientist to make the most of it." (Vogelsang and Borg 2019:249)"That means, requirements on data quantity should not be specified on the number of examples but rather on the diversity of the examples" (Vogelsang and Borg 2019:249)"In some regulated domains, there are constraints on the amount of data necessary to tackle some problems. P4: "If we work on models to predict the likelihood of loan losses, we are forced to consider data from at least 5 years."" (Vogelsang and Borg 2019:249)"The higher the quality of the data, the better the application will work. That's why the process before the training is important: how I clean and augment the data."" (Vogelsang and Borg 2019:249)""There are many dimensions of data quality. [. . . ] For me, the most important ones are completeness, consistency, and correctness". Completeness refers to the sparsity of data within each characteristic (i.e., does the data cover the whole range of possible values). Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being true. Correctness is strongly influenced by the way how the data was collected." (Vogelsang and Borg 2019:249)},
	annote = {Extracted Annotations (23/11/2021, 16:07:51)"characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems." (Vogelsang and Borg 2019:245)"We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process." (Vogelsang and Borg 2019:245)"In addition, a recent survey suggests that Requirements Engineering (RE) is the most difficult activity for the development of ML-based systems [2]." (Vogelsang and Borg 2019:245)"In a recent survey, Ishikawa and Yoshioka [2] reported that RE was listed as the most difficult activity for the development of ML-based systems: "The dominant concerns [. . . ] pertain to decision making with the customers. In the conventional setting, this activity involved requirements analysis and specification in the initial phase and an acceptance inspection in the final phase. This activity flow is not possible when working with ML-based systems due to the impossibility of prior estimation or assurance of achievable accuracy."" (Vogelsang and Borg 2019:246)"She argues that there is no unified collection or consideration of many NFRs for ML, including a consideration of ML-specific quality trade-off data." (Vogelsang and Borg 2019:246)"In ML systems, the quality of the resulting predictions can be considered a functional requirement" (Vogelsang and Borg 2019:247)"Quantification of quality targets is certainly also a challenge for conventional software [22], but training an ML model to go beyond a certain utility breakpoint turns into a functional requirement in practice." (Vogelsang and Borg 2019:247)""Explainability is twofold: On the one side, there is a need to explain the model (what has been learned). On the other side, there is a need to explain single predictions of the model."" (Vogelsang and Borg 2019:248)""If there is a combination or transformation of features that is smaller but has a similar performance, we prefer that. [. . . ] We try to minimize the number of features to make the model more explainable."" (Vogelsang and Borg 2019:248)"Training data is an integral part of any ML system. We envision that requirements for (training) data play a larger role for specifying ML systems than for conventional systems." (Vogelsang and Borg 2019:248)"Based on our interviews, we would add "training data needs specified and validated requirements like code"." (Vogelsang and Borg 2019:249)""You could try, but it won't help" - the data is what it is, and it is up to the data scientist to make the most of it." (Vogelsang and Borg 2019:249)"In some regulated domains, there are constraints on the amount of data necessary to tackle some problems. P4: "If we work on models to predict the likelihood of loan losses, we are forced to consider data from at least 5 years."" (Vogelsang and Borg 2019:249)"The higher the quality of the data, the better the application will work. That's why the process before the training is important: how I clean and augment the data."" (Vogelsang and Borg 2019:249)""There are many dimensions of data quality. [. . . ] For me, the most important ones are completeness, consistency, and correctness". Completeness refers to the sparsity of data within each characteristic (i.e., does the data cover the whole range of possible values). Consistency refers to the format and representation of data that should be the same in the dataset. Correctness refers to the degree to which you can rely on the data actually being true. Correctness is strongly influenced by the way how the data was collected." (Vogelsang and Borg 2019:249)},
	file = {Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:/Users/johannesreichle/Zotero/storage/GSZJZHNW/Vogelsang_Borg_2019_Requirements Engineering for Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/QT2UUEQG/8933800.html:text/html},
}

@inproceedings{arpteg_software_2018,
	title = {Software {Engineering} {Challenges} of {Deep} {Learning}},
	doi = {10.1109/SEAA.2018.00018},
	abstract = {Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.},
	booktitle = {2018 44th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Arpteg, Anders and Brinne, Björn and Crnkovic-Friis, Luka and Bosch, Jan},
	month = aug,
	year = {2018},
	keywords = {Big Data, deep learning, machine learning, Machine learning, artificial intelligence, Buildings, Companies, Google, Oils, Software engineering, software engineering challenges},
	pages = {50--59},
	annote = {Extracted Annotations (23/11/2021, 14:37:19)
"A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges." (Arpteg et al 2018:50)
"Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of highquality systems." (Arpteg et al 2018:50)
"One of the main differences from traditional machine learning (ML) methods is that DL automatically learns how to represent data using multiple layers of abstraction [5], [6]. In traditional ML, a significant amount of work has to be spent on "feature engineering" to build this representation manually, but this process can now be automated to a higher degree. Having an automated and data-driven method for learning how to represent data improves both the performance of the model and reduces requirements for manual feature engineering work [7], [8]." (Arpteg et al 2018:50)
"A key difference between ML systems and non-ML systems is that data partly replaces code in a ML system, and a learning algorithm is used to automatically identify patterns in the data instead of writing hard coded rules." (Arpteg et al 2018:50)
"ML systems not only experience code-level debts but also dependencies related to changes to the external world. Data dependencies have been found to build similar debt as code dependencies." (Arpteg et al 2018:51)
"Deep Learning also makes it possible to compose complex models from a set of sub models and potentially reuse pretrained parameters with so called "transfer learning" techniques." (Arpteg et al 2018:51)
"It is not uncommon for pipelines to change, add and remove fields, or become deprecated. Keeping a deployed production-ready ML system up to date with all these changes require a significant amount of work, and requires supporting monitoring and logging systems to be able to detect when these changes occur." (Arpteg et al 2018:51)
"This section presents a list of concisely described challenges in the intersection between ML and SE. They have been grouped into three categories: development, production, and organizational challenges." (Arpteg et al 2018:53)
"With the addition of data dependencies and a high degree of configuration parameters, it can be very challenging to properly maintain ML systems in the long run. Also, it is not uncommon to perform hyperparameter tuning of models, potentially by making use of automated meta-optimization methods that generate hundreds of versions of the same data and model but with different configuration parameters [35]. Deep learning can also add the requirement of specific hardware." (Arpteg et al 2018:53)
"The great advances that have been made in fields such as computer vision and speech recognition, have been accomplished by replacing a modular processing pipeline with large neural networks that are trained end-to-end [37]. In essence, transparency is traded for accuracy. This is an unavoidable reality." (Arpteg et al 2018:53)
"A major challenge in developing DL systems is the difficulties in estimating the results before a system has been trained and tested." (Arpteg et al 2018:53)
"1) Experiment Management: During the development of ML models, a large number of experiments are usually performed to identify the optimal model. Each experiment can differ from other experiments in a number of ways and it is important to ensure reproducible results for these experiments. To have reproducible results, it may be necessary to know the exact version of components such as: 1) Hardware (e.g. GPU models primarily) 2) Platform (e.g. operating system and installed packages) 3) Source code (e.g. model training and pre-processing) 4) Configuration (e.g. model configuration and preprocessing settings) 5) Training data (e.g. input signals and target values) 6) Model state (e.g. versions of trained models)." (Arpteg et al 2018:53)
"Resource Limitations: Working with data that require distributed system adds another magnitude of complexity compared to single machine solutions. It is not only the volume of the data that may require a distributed solution, but also computational needs for extracting and transforming data, training and evaluating the model, and/or serving the model in production." (Arpteg et al 2018:54)
"It is challenging to provide a sample that includes all the edge cases that may exist in the full dataset. Also, as the external world is dynamic and changes over time, new edge cases will continue to appear later in time." (Arpteg et al 2018:54)
"There can be many reasons for the need of special techniques to handle resource limitations such as a lack of memory (CPU or GPU), long training time, or low-latency serving needs." (Arpteg et al 2018:56)},
	file = {Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/79S64LYS/Arpteg et al_2018_Software Engineering Challenges of Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/WKSGQ2B7/8498185.html:text/html},
}

@article{okoli_guide_2010,
	title = {A guide to conducting a systematic literature review of information systems research},
	author = {Okoli, Chitu and Schabram, Kira},
	year = {2010},
	file = {Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:/Users/johannesreichle/Zotero/storage/VHAVNWH2/Okoli_Schabram_2010_A guide to conducting a systematic literature review of information systems.pdf:application/pdf},
}

@article{snyder_literature_2019,
	title = {Literature review as a research methodology: {An} overview and guidelines},
	volume = {104},
	issn = {0148-2963},
	shorttitle = {Literature review as a research methodology},
	url = {http://www.sciencedirect.com/science/article/pii/S0148296319304564},
	doi = {10.1016/j.jbusres.2019.07.039},
	abstract = {Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.},
	language = {en},
	urldate = {2020-04-06},
	journal = {Journal of Business Research},
	author = {Snyder, Hannah},
	month = nov,
	year = {2019},
	keywords = {Integrative review, Literature review, Research methodology, Synthesis, Systematic review},
	pages = {333--339},
	annote = {Extracted Annotations (23/11/2021, 16:18:00)
"A literature review can broadly be described as a more or less systematic way of collecting and synthesizing previous research (Baumeister \& Leary, 1997; Tranfield, Denyer, \& Smart, 2003)." (Snyder 2019:333)
"By integratingfindings and perspectives from many empiricalfindings, a literature review can address research questions with a power that no single study has." (Snyder 2019:333)
"In addition, a literature review is an excellent way of synthesizing researchfindings to show evidence on a meta-level and to uncover areas in which more research is needed, which is a critical component of creating theoretical frameworks and building conceptual models" (Snyder 2019:333)
"he paper has several contributions. First, this paper separates between different types of review methodologies; systematic," (Snyder 2019:333)
"semi-systematic and integrative approaches and argues that depending on purpose and the quality of execution, each type of approach can be very effective. While systematic reviews have strict requirements for search strategy and selecting articles for inclusion in the review, they are effective in synthesizing what the collection of studies are showing in a particular question and can provide evidence of effect that can inform policy and practice." (Snyder 2019:334)
"Instead, a semi-systematic review approach could be a good strategy for example map theoretical approaches or themes as well as identifying knowledge gaps within the literature. In some cases, a research question requires a more creative collection of data, in these cases; an integrative review approach can be useful when the purpose of the review is not to cover all articles ever published on the topic but rather to combine perspectives to create new theoretical models." (Snyder 2019:334)
"mature topics, the purpose of using an integrative review method is to overview the knowledge base, to critically review and potentially reconceptualize, and to expand on the theoretical foundation of the specific topic as it develops." (Snyder 2019:336)
"This includes selecting search terms and appropriate databases and deciding on inclusion and exclusion criteria." (Snyder 2019:337)
"does it make a substantial, practical, or theoretical contribution?early stated and motivated?" (Snyder 2019:338)},
	file = {Snyder_2019_Literature review as a research methodology.pdf:/Users/johannesreichle/Zotero/storage/HJIZLCQU/Snyder_2019_Literature review as a research methodology.pdf:application/pdf;ScienceDirect Snapshot:/Users/johannesreichle/Zotero/storage/YBMI7WLU/S0148296319304564.html:text/html},
}

@article{webster_analyzing_2002,
	title = {Analyzing the past to prepare for the future: {Writing} a literature review},
	shorttitle = {Analyzing the past to prepare for the future},
	journal = {MIS quarterly},
	author = {Webster, Jane and Watson, Richard T.},
	year = {2002},
	note = {Publisher: JSTOR},
	pages = {xiii--xxiii},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/T9JG4FKL/4132319.html:text/html;Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:/Users/johannesreichle/Zotero/storage/76JDULLA/Webster_Watson_2002_Analyzing the past to prepare for the future.pdf:application/pdf},
}

@book{zowghi_requirements_2014,
	address = {Berlin, Heidelberg},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Requirements {Engineering}},
	volume = {432},
	isbn = {978-3-662-43609-7 978-3-662-43610-3},
	url = {http://link.springer.com/10.1007/978-3-662-43610-3},
	language = {en},
	urldate = {2021-11-23},
	publisher = {Springer Berlin Heidelberg},
	editor = {Zowghi, Didar and Jin, Zhi and Junqueira Barbosa, Simone Diniz and Chen, Phoebe and Cuzzocrea, Alfredo and Du, Xiaoyong and Filipe, Joaquim and Kara, Orhun and Kotenko, Igor and Sivalingam, Krishna M. and Ślęzak, Dominik and Washio, Takashi and Yang, Xiaokang},
	year = {2014},
	doi = {10.1007/978-3-662-43610-3},
	annote = {Extracted Annotations (24/11/2021, 10:50:14)
"The primary focus of requirements engineering has been functional requirements, which specify functions that a system or system component must deliver to users [3]. Increasingly, both researchers and practitioners realized that there exist many other requirements that play important role in shaping the target system, defining the development process, and managing the development project [4]. Non-Functional Requirements (NFR), as an umbrella term, then was coined to name these requirements [5]." (Zowghi and Jin 2014:10)},
	file = {Zowghi and Jin - 2014 - Requirements Engineering.pdf:/Users/johannesreichle/Zotero/storage/RTD5ZNET/Zowghi and Jin - 2014 - Requirements Engineering.pdf:application/pdf},
}

@article{noauthor_ieee_1998,
	title = {{IEEE} {Standard} for a {Software} {Quality} {Metrics} {Methodology}},
	doi = {10.1109/IEEESTD.1998.243394},
	abstract = {A methodology for establishing quality requirements and identifying, implementing, analyzing and validating the process and product software quality metrics is defined. The methodology spans the entire software life-cycle.},
	journal = {IEEE Std 1061-1998},
	month = dec,
	year = {1998},
	note = {Conference Name: IEEE Std 1061-1998},
	keywords = {IEEE standards},
	pages = {i--},
	file = {1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:/Users/johannesreichle/Zotero/storage/NU3DCQRT/1998_IEEE Standard for a Software Quality Metrics Methodology.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/7Z7IEYIP/749159.html:text/html},
}

@book{kotonya_requirements_1998,
	edition = {1st},
	title = {Requirements {Engineering}: {Processes} and {Techniques}},
	isbn = {978-0-471-97208-2},
	shorttitle = {Requirements {Engineering}},
	abstract = {Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws},
	publisher = {Wiley Publishing},
	author = {Kotonya, Gerald and Sommerville, Ian},
	year = {1998},
}

@incollection{chung_non-functional_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Non}-{Functional} {Requirements} in {Software} {Engineering}},
	isbn = {978-3-642-02463-4},
	url = {https://doi.org/10.1007/978-3-642-02463-4_19},
	abstract = {Essentially a software system’s utility is determined by both its functionality and its non-functional characteristics, such as usability, flexibility, performance, interoperability and security. Nonetheless, there has been a lop-sided emphasis in the functionality of the software, even though the functionality is not useful or usable without the necessary non-functional characteristics. In this chapter, we review the state of the art on the treatment of non-functional requirements (hereafter, NFRs), while providing some prospects for future directions.},
	language = {en},
	urldate = {2021-11-23},
	booktitle = {Conceptual {Modeling}: {Foundations} and {Applications}: {Essays} in {Honor} of {John} {Mylopoulos}},
	publisher = {Springer},
	author = {Chung, Lawrence and do Prado Leite, Julio Cesar Sampaio},
	editor = {Borgida, Alexander T. and Chaudhri, Vinay K. and Giorgini, Paolo and Yu, Eric S.},
	year = {2009},
	doi = {10.1007/978-3-642-02463-4_19},
	keywords = {requirements engineering, alternatives, goal-oriented requirements engineering, NFRs, Non-functional requirements, satisficing, selection criteria, softgoals},
	pages = {363--379},
	file = {Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:/Users/johannesreichle/Zotero/storage/GB3VDQR4/Chung_do Prado Leite_2009_On Non-Functional Requirements in Software Engineering.pdf:application/pdf},
}

@article{niu_26ms_2019,
	title = {26ms {Inference} {Time} for {ResNet}-50: {Towards} {Real}-{Time} {Execution} of all {DNNs} on {Smartphone}},
	shorttitle = {26ms {Inference} {Time} for {ResNet}-50},
	url = {http://arxiv.org/abs/1905.00571},
	abstract = {With the rapid emergence of a spectrum of high-end mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed. This work presents CADNN, a programming framework to efficiently execute DNN on mobile devices with the help of advanced model compression (sparsity) and a set of thorough architecture-aware optimization. The evaluation result demonstrates that CADNN outperforms all the state-of-the-art dense DNN execution frameworks like TensorFlow Lite and TVM.},
	urldate = {2021-11-24},
	journal = {arXiv:1905.00571 [cs, stat]},
	author = {Niu, Wei and Ma, Xiaolong and Wang, Yanzhi and Ren, Bin},
	month = may,
	year = {2019},
	note = {arXiv: 1905.00571},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Extracted Annotations (15/12/2021, 15:00:55)"With the rapid emergence of a spectrum of highend mobile devices, many applications that required desktop-level computation capability formerly can now run on these devices without any problem. However, without a careful optimization, executing Deep Neural Networks (a key building block of the real-time video stream processing that is the foundation of many popular applications) is still challenging, specifically, if an extremely low latency or high accuracy inference is needed." (Niu et al 2019:1)},
	file = {Niu et al_2019_26ms Inference Time for ResNet-50.pdf:/Users/johannesreichle/Zotero/storage/F7T9RZPR/Niu et al_2019_26ms Inference Time for ResNet-50.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/B2A2APAU/1905.html:text/html},
}

@article{nowruzi_how_2019,
	title = {How much real data do we actually need: {Analyzing} object detection performance using synthetic and real data},
	shorttitle = {How much real data do we actually need},
	url = {http://arxiv.org/abs/1907.07061},
	abstract = {In recent years, deep learning models have resulted in a huge amount of progress in various areas, including computer vision. By nature, the supervised training of deep models requires a large amount of data to be available. This ideal case is usually not tractable as the data annotation is a tremendously exhausting and costly task to perform. An alternative is to use synthetic data. In this paper, we take a comprehensive look into the effects of replacing real data with synthetic data. We further analyze the effects of having a limited amount of real data. We use multiple synthetic and real datasets along with a simulation tool to create large amounts of cheaply annotated synthetic data. We analyze the domain similarity of each of these datasets. We provide insights about designing a methodological procedure for training deep networks using these datasets.},
	urldate = {2021-11-25},
	journal = {arXiv:1907.07061 [cs]},
	author = {Nowruzi, Farzan Erlik and Kapoor, Prince and Kolhatkar, Dhanvin and Hassanat, Fahed Al and Laganiere, Robert and Rebut, Julien},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07061},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in International Conference on Machine Learning (ICML 2019) Workshop on AI for Autonomous Driving},
	annote = {Extracted Annotations (25/11/2021, 16:13:50)
"This is also ultimately valuable as the huge training session can be achieved independently from the smaller fine-tuning counterpart. It is shown that the photo-realism is not as important as the diversity of the data." (Nowruzi et al 2019:8)},
	annote = {Extracted Annotations (26/11/2021, 14:43:05)
"In recent years, deep learning models have resulted in a huge amount of progress in various areas, including computer vision. By nature, the supervised training of deep models requires a large amount of data to be available. This ideal case is usually not tractable as the data annotation is a tremendously exhausting and costly task to perform." (Nowruzi et al 2019:1)
"The necessity of large amounts of annotated data is a bottleneck in computer vision tasks." (Nowruzi et al 2019:2)
"This is also ultimately valuable as the huge training session can be achieved independently from the smaller fine-tuning counterpart. It is shown that the photo-realism is not as important as the diversity of the data." (Nowruzi et al 2019:8)},
	file = {Nowruzi et al_2019_How much real data do we actually need.pdf:/Users/johannesreichle/Zotero/storage/42P4ENVD/Nowruzi et al_2019_How much real data do we actually need.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/KKTNYECV/1907.html:text/html},
}

@inproceedings{ouyang_factors_2016,
	title = {Factors in {Finetuning} {Deep} {Model} for {Object} {Detection} {With} {Long}-{Tail} {Distribution}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.html},
	urldate = {2021-11-25},
	author = {Ouyang, Wanli and Wang, Xiaogang and Zhang, Cong and Yang, Xiaokang},
	year = {2016},
	pages = {864--873},
	annote = {Extracted Annotations (25/11/2021, 16:13:45)"We find that it is better to have the number of samples uniform across different classes for feature learning." (Ouyang et al 2016:871)},
	annote = {Extracted Annotations (26/11/2021, 14:30:56)
"Finetuning from a pretrained deep model is found to yield state-of-the-art performance for many vision tasks." (Ouyang et al 2016:864)
"Analysis and experimental investigation on the factors that influence the effectiveness of finetuning. The investigated factors include the influence of the pretraining and finetuning on different layers of the deep model, the influence of the long tail, the influence of the training sample number, the effectiveness of different subsets of object classes, and the influence from the subset of training data." (Ouyang et al 2016:865)
"We find that it is better to have the number of samples uniform across different classes for feature learning." (Ouyang et al 2016:871)},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/LBVJEWSZ/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.html:text/html;Ouyang et al_2016_Factors in Finetuning Deep Model for Object Detection With Long-Tail.pdf:/Users/johannesreichle/Zotero/storage/9PFLFFAE/Ouyang et al_2016_Factors in Finetuning Deep Model for Object Detection With Long-Tail.pdf:application/pdf},
}

@article{el_bahi_text_2019,
	title = {Text recognition in document images obtained by a smartphone based on deep convolutional and recurrent neural network},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-019-07855-z},
	doi = {10.1007/s11042-019-07855-z},
	abstract = {Automatic text recognition in document images is an important task in many real-world applications. Several systems have been proposed to accomplish this task. However, a little attention has been given to document images obtained by mobile phones. To meet this need, we propose a new system that integrates preprocessing, features extraction and classification in order to recognize text contained in the document images acquired by a smartphone. The preprocessing phase is applied to locate the text region, and then segment that region into text line images. In the second phase, a sliding window divides the text-line image into a sequence of frames; afterwards a deep convolutional neural network (CNN) model is used to extract features from each frame. Finally, an architecture that combines the bidirectional recurrent neural network (RNN), the gated recurrent units (GRU) block and the connectionist temporal classification (CTC) layer is explored to ensure the classification phase. The proposed system has been tested on the ICDAR2015 Smartphone document OCR dataset and the experimental results show that the proposed system is capable to achieve promising recognition rates.},
	language = {en},
	number = {18},
	urldate = {2021-11-26},
	journal = {Multimedia Tools and Applications},
	author = {El Bahi, Hassan and Zatni, Abdelkarim},
	month = sep,
	year = {2019},
	pages = {26453--26481},
	file = {El Bahi_Zatni_2019_Text recognition in document images obtained by a smartphone based on deep.pdf:/Users/johannesreichle/Zotero/storage/LH4DNH6K/El Bahi_Zatni_2019_Text recognition in document images obtained by a smartphone based on deep.pdf:application/pdf},
}

@inproceedings{sourvanos_challenges_2018,
	title = {Challenges in {Input} {Preprocessing} for {Mobile} {OCR} {Applications}: {A} {Realistic} {Testing} {Scenario}},
	shorttitle = {Challenges in {Input} {Preprocessing} for {Mobile} {OCR} {Applications}},
	doi = {10.1109/IISA.2018.8633688},
	abstract = {Applications of OCR range from automatic text input from images and videos and text-to-speech to automatic translation of textual information in real world scenes. Such wide applicability of OCR has promoted its use on mobile devices, such as smartphones. However, there are intrinsic complications when designing an end-to-end OCR system for smartphones, as there is a number of steps and challenges, inherent to OCR itself, which may prove arduous and inconvenient for a conventional mobile device to handle. For starters, before actual OCR, input needs to be preprocessed, in order to enhance the existing textual information and minimize noise and outliers. In this study, we attempt to document the challenges faced by a smartphone-enabled OCR application, experiment on the preprocessing steps using a realistic testing scenario and pose some open issues for future investigation.},
	booktitle = {2018 9th {International} {Conference} on {Information}, {Intelligence}, {Systems} and {Applications} ({IISA})},
	author = {Sourvanos, Nikolaos and Tsatiris, Georgios},
	month = jul,
	year = {2018},
	keywords = {Optical character recognition software, Testing, Text recognition, Image segmentation, Data preprocessing, Smart phones},
	pages = {1--5},
	file = {Sourvanos_Tsatiris_2018_Challenges in Input Preprocessing for Mobile OCR Applications.pdf:/Users/johannesreichle/Zotero/storage/SJUBEAAA/Sourvanos_Tsatiris_2018_Challenges in Input Preprocessing for Mobile OCR Applications.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/XT8CXNXF/8633688.html:text/html},
}

@inproceedings{ghosh_visual_2017,
	title = {Visual {Attention} {Models} for {Scene} {Text} {Recognition}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.158},
	abstract = {In this paper we propose an approach to lexicon-free recognition of text in scene images. Our approach relies on a LSTM-based soft visual attention model learned from convolutional features. A set of feature vectors are derived from an intermediate convolutional layer corresponding to different areas of the image. This permits encoding of spatial information into the image representation. In this way, the framework is able to learn how to selectively focus on different parts of the image. At every time step the recognizer emits one character using a weighted combination of the convolutional feature vectors according to the learned attention model. Training can be done end-to-end using only word level annotations. In addition, we show that modifying the beam search algorithm by integrating an explicit language model leads to significantly better recognition results. We validate the performance of our approach on standard SVT and ICDAR'03 scene text datasets, showing state-of-the-art performance in unconstrained text recognition.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Ghosh, Suman K. and Valveny, Ernest and Bagdanov, Andrew D.},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Text recognition, Computational modeling, Adaptation models, Character recognition, Decoding, Image recognition, Visualization},
	pages = {943--948},
	annote = {Extracted Annotations (26/11/2021, 13:20:38)
"However, robust reading of text in uncontrolled environments is very different from text recognition in document images and much more challenging due to multiple factors such as difficult acquisition conditions, low resolution, font variability, complex backgrounds, different lighting conditions, blur, etc." (Ghosh et al 2017:943)
"The problem of end-to-end scene text recognition is usually divided in two different tasks: word detection and word recognition." (Ghosh et al 2017:943)
"Existing word recognition methods can be broadly divided into dictionay-based methods, using some kind of predefined lexicon to guide the recognition, and unconstrained methods, able to recognize any word." (Ghosh et al 2017:943)
"Biassco et al.in [3] rely on sequential character classifiers. They use a massive number of annotated character bounding boxes to learn character classifiers. Binarization and sliding window methods are used to generate character proposals followed by a text/background classifier. Finally, character probabilities given by character classifiers are used in a beam search to recognize words. They also integrate a static character n-gram language model in every step of the beam search to incorporate an underlying language model." (Ghosh et al 2017:943)
"In contrast to the above strategies our approach neither recognizes individual characters in the word image nor uses any holistic representation to recognize the word. It rather uses a LSTM-based visual attention model on top of CNN features (based on [19]) to focus attention on relevant parts of the image at every step and infer a character present in the image (see figure 1)" (Ghosh et al 2017:944)
"The visual attention model can be trained using only word bounding boxes and does not need explicit character bounding boxes at training time." (Ghosh et al 2017:944)},
	file = {Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:/Users/johannesreichle/Zotero/storage/W7DQDRHK/Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/H5S99QNM/8270089.html:text/html},
}

@article{hu_gtc_2020,
	title = {{GTC}: {Guided} {Training} of {CTC} {Towards} {Efficient} and {Accurate} {Scene} {Text} {Recognition}},
	shorttitle = {{GTC}},
	url = {http://arxiv.org/abs/2002.01276},
	abstract = {Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance. With the benefit of guided training, CTC model achieves robust and accurate prediction for both regular and irregular scene text while maintaining a fast inference speed. Moreover, to further leverage the potential of CTC decoder, a graph convolutional network (GCN) is proposed to learn the local correlations of extracted features. Extensive experiments on standard benchmarks demonstrate that our end-to-end model achieves a new state-of-the-art for regular and irregular scene text recognition and needs 6 times shorter inference time than attentionbased methods.},
	urldate = {2021-11-30},
	journal = {arXiv:2002.01276 [cs, eess]},
	author = {Hu, Wenyang and Cai, Xiaocong and Hou, Jun and Yi, Shuai and Lin, Zhiping},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.01276},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Machine Learning},
	annote = {Comment: Accepted by AAAI 2020},
	annote = {Extracted Annotations (30/11/2021, 12:41:11)
"Connectionist Temporal Classification (CTC) and attention mechanism are two main approaches used in recent scene text recognition works. Compared with attention-based methods, CTC decoder has a much shorter inference time, yet a lower accuracy. To design an efficient and effective model, we propose the guided training of CTC (GTC), where CTC model learns a better alignment and feature representations from a more powerful attentional guidance" (Hu et al 2020:1)
"However, due to different sizes, fonts, colors and character placements of scene texts, scene text recognition is still a challenging task." (Hu et al 2020:1)},
	file = {Hu et al_2020_GTC.pdf:/Users/johannesreichle/Zotero/storage/NT5A62AM/Hu et al_2020_GTC.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/Y58C3PJB/2002.html:text/html},
}

@article{chen_text_2021,
	title = {Text {Recognition} in the {Wild}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Text {Recognition} in the {Wild}},
	url = {https://dl.acm.org/doi/10.1145/3440756},
	doi = {10.1145/3440756},
	abstract = {The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition.},
	language = {en},
	number = {2},
	urldate = {2021-11-30},
	journal = {ACM Computing Surveys},
	author = {Chen, Xiaoxue and Jin, Lianwen and Zhu, Yuanzhi and Luo, Canjie and Wang, Tianwei},
	month = apr,
	year = {2021},
	pages = {1--35},
	annote = {Extracted Annotations (06/01/2022, 15:59:08)"Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios." (Chen et al 2021:1)"In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency." (Chen et al 2021:1)"Specifically, rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios," (Chen et al 2021:1)"Recognizing text in natural scenes, also known as scene text recognition (STR), is usually considered as a special form of optical character recognition (OCR), that is, camera-based OCR. Although OCR in scanned documents is well developed [137, 241], STR remains challenging because of many factors, including complex backgrounds, various fonts, and imperfect imaging conditions. Figure 1 compares the following characteristics of STR and OCR in scanned documents. • Background. Unlike OCR in scanned documents, text in natural scenes can appear on anything (e.g., signboards, walls, or product packagings). Therefore, scene text images may contain very complex backgrounds. Moreover, the texture of the background can be visually similar to text, which causes additional challenges for recognition. • Form. Text in scanned documents is usually printed in a single color with regular font, consistent size, and uniform arrangement. Meanwhile, in natural scenes, text appears in multiple colors with irregular fonts, different sizes, and diverse orientations. The diversity of text thus makes STR more difficult than OCR in scanned documents. • Noise. Text in natural scenes is usually distorted by noise interference, such as nonuniform illumination, low resolution, and motion blurring. Imperfect imaging conditions cause failures in the STR. • Access. Scanned text is usually frontal and occupies the main part of an image. However, scene text is captured randomly, resulting in irregular deformations (such as perspective distortion). Furthermore, the various shapes of text increase the difficulty of recognizing characters and predicting text strings." (Chen et al 2021:2)"As illustrated in Figure 3, various fundamental problems have been defined at various stages of this task in the literature: text localization, text verification, text detection, text segmentation, text recognition, and end-to-end systems" (Chen et al 2021:4)"Text localization. The objective of text localization [102] is to localize text components precisely and group them into candidate text regions with as little background as possible" (Chen et al 2021:4)"Text verification. Text verification [89] aims at verifying text candidate regions as text or non-text." (Chen et al 2021:4)"Recent works [81, 196] have used a convolution neural network (CNN) to improve text/non-text discrimination." (Chen et al 2021:5)"Text detection. The function of text detection [201, 219] is to determine whether text is present using localization and verification procedures [222]." (Chen et al 2021:5)"Text segmentation. Text segmentation has been identified as one of the most challenging problems [184]. It includes text line segmentation [168, 223] and character segmentation [144, 167]. The former refers to splitting a region of multiple text lines into multiple subregions of single text lines. The latter refers to separating a text instance into multiple regions of single characters." (Chen et al 2021:5)"Compared with traditional methods, deep learning methods have the following advantages: (i) Automation: automatic feature representation learning can free researchers from empirically designing handcrafted features. (ii) Effectiveness: excellent recognition performance far exceeds traditional algorithms. (iii) Generalization: algorithms can be easily applied to similar vision-based problems." (Chen et al 2021:6)"The objective of STR is to translate a cropped text instance image into a target string sequence. There are two types of scene text in nature: regular and irregular." (Chen et al 2021:6)"Text Image Super-resolution (TextSR). Scene text is usually distorted by various noise interferences, such as low resolution." (Chen et al 2021:9)"Rectification. The function of rectification is to normalize the input text instance image, remove the distortion, and reduce the difficulty of irregular text recognition. Specifically, irregular text [217] refers to text with perspective distortion or an arbitrary curving shape, which usually causes additional challenges in recognition." (Chen et al 2021:9)"Image preprocessing includes but is not limited to the aforementioned types. It can significantly reduce the difficulties of recognition by improving image quality." (Chen et al 2021:10)"Feature Representation Stage Feature representation maps the input text instance image to a representation that reflects the attributes relevant for character recognition, while suppressing irrelevant features such as font, color, size, and background." (Chen et al 2021:10)"A deeper and more advanced feature extractor usually results in better representation power, which is suitable for improving STR with complex backgrounds. However, performance improvement costs increased memory and computation consumption" (Chen et al 2021:10)"Contextual cues are beneficial for image-based sequence recognition. Although recurrent neural network (RNN)-based [72] structures, such as BiLSTM or LSTM, can model character sequences, there are some inherent limitations. In contrast, CNNs or transformers [182] can not only effectively deal with long sequences but also be parallelized efficiently. Modeling language sequences using CNNs or transformer structures may be a new trend for sequence modeling because of its intrinsic superiority." (Chen et al 2021:11)"Given a text image with a complex background as input, an end-to-end system aims to directly convert all text regions into string sequences. Typically, this includes text detection, text recognition, and postprocessing." (Chen et al 2021:14)"Several factors promote the emergence of end-to-end systems: (i) Errors can accumulate in a cascade way of text detection and recognition, which may lead to a large fraction of garbage predictions, while an end-to-end system can prevent errors from being accumulated during the training. (ii) In an end-to-end system, text detection and recognition can share information and be jointly optimized to improve the overall performance. (iii) An end-to-end system is easier to maintain and adapt to new domains; whereas, maintaining a cascaded pipeline with data and model dependencies requires substantial engineering efforts. (iv) End-to-end systems exhibit competitive performance with faster inference and smaller storage requirements." (Chen et al 2021:14)"Although the current end-to-end systems work fairly well in many real-world scenarios, they have limitations. The following difficulties should be considered: (i) How can we efficiently bridge and share information between text detection and recognition? (ii) How can the significant differences in learning difficulty and convergence speed be balanced between text detection and recognition? (iii) How can joint optimization be improved? Moreover, a simple, compact, and powerful end-to-end system is yet to be developed." (Chen et al 2021:15)"In this section, we summarize the evaluation protocols for Latin and multilingual texts. 4.2.1 Evaluation Protocols for Latin Text. Recognition Protocols. The word recognition accuracy (WRA) and word error rate (WER) are two widely used recognition evaluation protocols for Latin text. • WRA. W RA is defined by W RA= Wr, (7) W whereW is the total number of words, andWr represents the number of correctly recognized words. • WER. W ER is defined by W ER = 1−W RA= 1− Wr. (8) W" (Chen et al 2021:21)"Most competitions [27, 67, 114] measured the algorithm recognition performance using a traditional evaluation metric, the normalized edit distance (NED):" (Chen et al 2021:22)"where D (.) stands for the Levenshtein distance. si and ˆi denote the predicted text and the corresponding ground truth, respectively. Furthermore, li and ˆi are their text lengths. N is the total number of text lines. The NED protocol measures mismatching between the predicted text and the corresponding ground truth. Therefore, the recognition score is usually calculated as 1-NED. End-to-end Protocols. Two main evaluation protocols for end-to-end systems have been used during recent competitions:" (Chen et al 2021:22)},
	file = {Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:/Users/johannesreichle/Zotero/storage/TVGZBTI2/Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:application/pdf;PDF with missing page:/Users/johannesreichle/Zotero/storage/URBXB24K/Chen et al_2021_Text Recognition in the Wild.pdf:application/pdf},
}

@article{long_scene_2021,
	title = {Scene {Text} {Detection} and {Recognition}: {The} {Deep} {Learning} {Era}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Scene {Text} {Detection} and {Recognition}},
	url = {https://link.springer.com/10.1007/s11263-020-01369-0},
	doi = {10.1007/s11263-020-01369-0},
	abstract = {With the rise and development of deep learning, computer vision has been tremendously transformed and reshaped. As an important research area in computer vision, scene text detection and recognition has been inevitably inﬂuenced by this wave of revolution, consequentially entering the era of deep learning. In recent years, the community has witnessed substantial advancements in mindset, methodology and performance. This survey is aimed at summarizing and analyzing the major changes and signiﬁcant progresses of scene text detection and recognition in the deep learning era. Through this article, we devote to: (1) introduce new insights and ideas; (2) highlight recent techniques and benchmarks; (3) look ahead into future trends. Speciﬁcally, we will emphasize the dramatic differences brought by deep learning and remaining grand challenges. We expect that this review paper would serve as a reference book for researchers in this ﬁeld. Related resources are also collected in our Github repository (https://github.com/Jyouhou/SceneTextPapers).},
	language = {en},
	number = {1},
	urldate = {2021-11-30},
	journal = {International Journal of Computer Vision},
	author = {Long, Shangbang and He, Xin and Yao, Cong},
	month = jan,
	year = {2021},
	pages = {161--184},
	annote = {Extracted Annotations (27/12/2021, 16:06:39)
"computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution," (Long et al 2021:161)
"on the other hand, the rich and precise high-level semantics embodied in text could be beneficial for understanding the world around us." (Long et al 2021:161)
"Diversity and Variability of Text in Natural Scenes Distinctive from scripts in documents, text in natural scene exhibit much higher diversity and variability" (Long et al 2021:161)
"Complexity and Interference of Backgrounds The backgrounds of natural scenes are virtually unpredictable." (Long et al 2021:161)
"Imperfect Imaging Conditions In" (Long et al 2021:162)
"1) Most methods utilize deeplearning based models; (2) Most researchers are approaching the problem from a diversity of perspectives, trying to solve different challenges. Methods driven by deep learning enjoy the advantage that automatic feature learning can save us from designing and testing a large amount of potential handcrafted features." (Long et al 2021:163)
"c, d are simplified pipeline. In c, detectors and recognizers are separate. In d, the detectors pass cropped Illustrations of tion system pipelines. feature maps to recognizers, which allows end-to-end training" (Long et al 2021:164)
"(1) text detection that detects and localizes text in natural images; (2) recognition system that transcribes and converts the content of the detected text regions into linguistic symbols; (3) end-to-end system that performs both text detection and recognition in one unified pipeline; (4) auxiliary methods that aim to support the main task of text detection and recognition, e.g. synthetic data generation." (Long et al 2021:164)
"Overall, in this stage, scene text detection algorithms still havelongandslowpipelines,thoughtheyhavereplacedsome hand-crafted features with learning-based ones. The design methodology is bottom-up and based on key components, such as single characters and text center lines." (Long et al 2021:165)
"with curved, oriented, or long text for one-staged methods due to the limitation of the receptive field, and the efficiency is limited for two-staged methods." (Long et al 2021:166)
"Character-level representation is yet another effective way. Baek et al. (2019b) propose to learn a segmentation map for character centers and links between them. Both components and links are predicted in the form of a Gaussian heat map. However, this method requires iterative weak supervision as real-world datasets are rarely equipped with character-level labels." (Long et al 2021:167)},
	annote = {Extracted Annotations (27/12/2021, 16:20:58)
"computer vision, scene text detection and recognition has been inevitably influenced by this wave of revolution," (Long et al 2021:161)
"on the other hand, the rich and precise high-level semantics embodied in text could be beneficial for understanding the world around us." (Long et al 2021:161)
"Diversity and Variability of Text in Natural Scenes Distinctive from scripts in documents, text in natural scene exhibit much higher diversity and variability" (Long et al 2021:161)
"Complexity and Interference of Backgrounds The backgrounds of natural scenes are virtually unpredictable." (Long et al 2021:161)
"Imperfect Imaging Conditions In" (Long et al 2021:162)
"1) Most methods utilize deeplearning based models; (2) Most researchers are approaching the problem from a diversity of perspectives, trying to solve different challenges. Methods driven by deep learning enjoy the advantage that automatic feature learning can save us from designing and testing a large amount of potential handcrafted features." (Long et al 2021:163)
"c, d are simplified pipeline. In c, detectors and recognizers are separate. In d, the detectors pass cropped Illustrations of tion system pipelines. feature maps to recognizers, which allows end-to-end training" (Long et al 2021:164)
"(1) text detection that detects and localizes text in natural images; (2) recognition system that transcribes and converts the content of the detected text regions into linguistic symbols; (3) end-to-end system that performs both text detection and recognition in one unified pipeline; (4) auxiliary methods that aim to support the main task of text detection and recognition, e.g. synthetic data generation." (Long et al 2021:164)
"Overall, in this stage, scene text detection algorithms still havelongandslowpipelines,thoughtheyhavereplacedsome hand-crafted features with learning-based ones. The design methodology is bottom-up and based on key components, such as single characters and text center lines." (Long et al 2021:165)
"with curved, oriented, or long text for one-staged methods due to the limitation of the receptive field, and the efficiency is limited for two-staged methods." (Long et al 2021:166)
"Character-level representation is yet another effective way. Baek et al. (2019b) propose to learn a segmentation map for character centers and links between them. Both components and links are predicted in the form of a Gaussian heat map. However, this method requires iterative weak supervision as real-world datasets are rarely equipped with character-level labels." (Long et al 2021:167)
"Character level annotations are more accurate and better. However, most existing datasets do not provide characterlevel annotating. Since characters are smaller and close to each other, character-level annotation is more costly and inconvenient." (Long et al 2021:171)
"enchmark Datasets and Evaluation Protocol" (Long et al 2021:172)},
	file = {Long et al. - 2021 - Scene Text Detection and Recognition The Deep Lea.pdf:/Users/johannesreichle/Zotero/storage/HCWGWF4P/Long et al. - 2021 - Scene Text Detection and Recognition The Deep Lea.pdf:application/pdf},
}

@article{watanabe_preliminary_2019,
	title = {Preliminary {Systematic} {Literature} {Review} of {Machine} {Learning} {System} {Development} {Process}},
	url = {http://arxiv.org/abs/1910.05528},
	abstract = {Previous machine learning (ML) system development research suggests that emerging software quality attributes are a concern due to the probabilistic behavior of ML systems. Assuming that detailed development processes depend on individual developers and are not discussed in detail. To help developers to standardize their ML system development processes, we conduct a preliminary systematic literature review on ML system development processes. A search query of 2358 papers identified 7 papers as well as two other papers determined in an ad-hoc review. Our findings include emphasized phases in ML system developments, frequently described practices and tailored traditional software development practices.},
	urldate = {2021-12-01},
	journal = {arXiv:1910.05528 [cs]},
	author = {Watanabe, Yasuhiro and Washizaki, Hironori and Sakamoto, Kazunori and Saito, Daisuke and Honda, Kiyoshi and Tsuda, Naohiko and Fukazawa, Yoshiaki and Yoshioka, Nobukazu},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.05528},
	keywords = {Computer Science - Machine Learning},
	annote = {Good example for literature review},
	file = {Watanabe et al_2019_Preliminary Systematic Literature Review of Machine Learning System Development.pdf:/Users/johannesreichle/Zotero/storage/NEUNMXHG/Watanabe et al_2019_Preliminary Systematic Literature Review of Machine Learning System Development.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/4GVXIP5X/1910.html:text/html},
}

@inproceedings{hu_towards_2020,
	title = {Towards {Requirements} {Specification} for {Machine}-learned {Perception} {Based} on {Human} {Performance}},
	doi = {10.1109/AIRE51212.2020.00014},
	abstract = {The application of machine learning (ML) based perception algorithms in safety-critical systems such as autonomous vehicles have raised major safety concerns due to the apparent risks to human lives. Yet assuring the safety of such systems is a challenging task, in a large part because ML components (MLCs) rarely have clearly specified requirements. Instead, they learn their intended tasks from the training data. One of the most well-studied properties that ensure the safety of MLCs is the robustness against small changes in images. But the range of changes considered small has not been systematically defined. In this paper, we propose an approach for specifying and testing requirements for robustness based on human perception. With this approach, the MLCs are required to be robust to changes that fall within the range defined based on human perception performance studies. We demonstrate the approach on a state-of-the-art object detector.},
	booktitle = {2020 {IEEE} {Seventh} {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Hu, Boyue Caroline and Salay, Rick and Czarnecki, Krzysztof and Rahimi, Mona and Selim, Gehan and Chechik, Marsha},
	month = sep,
	year = {2020},
	keywords = {Safety, Testing, Task analysis, Robustness, Visualization, Gaussian noise, Measurement, n/a},
	pages = {48--51},
	annote = {Extracted Annotations (02/12/2021, 11:48:21)"In addition, it is often difficult to rigorously specify the tasks that MLCs are expected to perform." (Hu et al 2020:48)"Ashmore et al. identified a list of desired properties of MLCs that should be considered as requirements: perfor-" (Hu et al 2020:48)"mance, robustness, reusability and interpretability" (Hu et al 2020:48)"Therefore, requirements that ensure robustness are crucial to assure that decisions made by ML can be trusted in safety-critical contexts" (Hu et al 2020:48)"Position and contributions: Specifying full requirements of the expected behaviour for MLCs may not be feasible." (Hu et al 2020:48)"Human performance is used to bound the amount of changes that the MLCs are required to be robust to. We present a systematic method of generating such requirements and a method for testing whether the requirements have been satisfied. Our requirements can be used for verification and safety guarantees for MLCs in safety-critical systems." (Hu et al 2020:49)"For our purposes, we only consider modifications that can be formally defined as transformations. Investigating modifications in images that cannot be formally expressed as transformations, e.g., changing the clothes of a pedestrian, is left as future work. Some examples of transformations are: Affine transformations [10] such as scaling and rotating. Transformations modifying different aspects of the perceptual context [22]: - Light sources, e.g., changing the color or brightness of the light. - Medium, e.g., adding weather conditions like rain or fog. - Objects, e.g., changing position of the object. - Observer (camera), e.g., different viewpoint and exposure of the camera, different amount of visual noise." (Hu et al 2020:49)"As shown in Fig. 1, we further refine requirements for robustness as invariant and equivariant requirements. An MLC can have multiple outputs and different outputs may be required to be invariant or equivariant with respect to given a transformation of the input. For example, an object detector produces a class label and a bounding box position and extent for each object it detects in the input image. With respect to a translation transformation that moves objects, we require that bounding box position is equivariant and moves a corresponding amount, while the class and bounding box extent is invariant." (Hu et al 2020:49)},
	file = {Hu et al_2020_Towards Requirements Specification for Machine-learned Perception Based on.pdf:/Users/johannesreichle/Zotero/storage/CE5E9RB3/Hu et al_2020_Towards Requirements Specification for Machine-learned Perception Based on.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/67IEQCH4/9233007.html:text/html},
}

@article{siebert_construction_2021,
	title = {Construction of a quality model for machine learning systems},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/10.1007/s11219-021-09557-y},
	doi = {10.1007/s11219-021-09557-y},
	abstract = {Nowadays, systems containing components based on machine learning (ML) methods are becoming more widespread. In order to ensure the intended behavior of a software system, there are standards that define necessary qualities of the system and its components (such as ISO/IEC 25010). Due to the different nature of ML, we have to re-interpret existing qualities for ML systems or add new ones (such as trustworthiness). We have to be very precise about which quality property is relevant for which entity of interest (such as completeness of training data or correctness of trained model), and how to objectively evaluate adherence to quality requirements. In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively. In addition to the overall construction process described, the main outcomes include a meta-model for specifying quality models for ML systems, reference elements regarding relevant views, entities, quality properties, and measures for ML systems based on existing research, an example instantiation of a quality model for a concrete industrial use case, and lessons learned from applying the construction process. We found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice. In the future, we want to learn how the term quality differs between different types of ML systems and come up with reference quality models for evaluating qualities of ML systems.},
	language = {en},
	urldate = {2021-12-01},
	journal = {Software Quality Journal},
	author = {Siebert, Julien and Joeckel, Lisa and Heidrich, Jens and Trendowicz, Adam and Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio},
	month = jun,
	year = {2021},
	annote = {Extracted Annotations (02/12/2021, 13:52:05)
"In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively." (Siebert et al 2021:1)
"e found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice." (Siebert et al 2021:1)
"A data-driven software component is a piece of software that" (Siebert et al 2021:1)
"solves a given task (e.g., image segmentation, sentiment analysis, classification, etc.), using methods from data science, such as machine learning (ML), data mining, natural language processing, signal processing, statistics, etc. The functionality of data-driven software components (or at least part of it) is not entirely defined by the programmer in the classical way (by programming it directly), but is derived (i.e., learned) from data. At the core of a data-driven software component lies the notion of a model (or several models, coupled together in a pipeline)." (Siebert et al 2021:2)
"In short, the behavior of such components is first and foremost fundamentally different from traditional (i.e., not data-driven) software: the relationship between the input and the outcome of the software component is usually non-linear." (Siebert et al 2021:2)
"This input-output relationship is also only defined for a subset of the data, which leads to uncertainty in outcomes for previously unseen data" (Siebert et al 2021:2)
"Second, common development principles from software engineering, such as encapsulation and modularity, have to be rethought. For example, changing the application context of a data-driven component (its intended scope), the type of model used, or the internal parameters, usually implies retraining the component. This is also referred to as the CACE principle: changing anything changes everything (Sculley et al., 2015)." (Siebert et al 2021:2)
"For instance, ISO/ IEC 25010 (ISO/IEC, 2011) defines quality models for software and systems, i.e., a hierarchy of quality properties of interest and how to quantify and assess them. Due to the different nature of data-driven software components, these quality models cannot be applied directly as they are." (Siebert et al 2021:2)
"To develop meaningful quality models, it is necessary to understand the application context of the use case and what kind of data-driven method is used." (Siebert et al 2021:2)
"To build a quality model, it is first necessary to define the usage scenarios (i.e., How will the system be used? By whom? What are the expectations in terms of quality? etc.). This naturally goes hand in hand with the definition of the relevant quality properties and the entities to be measured. It is also necessary to define how to measure these properties and, finally, to decide on the basis of these measurements what to do to improve certain properties of quality." (Siebert et al 2021:3)
"Technical robustness, reliability, dependability (e.g., correctness of output, estimation of model uncertainty, robustness against harmful inputs, errors, or unexpected situations)" (Siebert et al 2021:4)
"A model is by definition a re-presentation (i.e., a simplification) of some part of reality (i.e., the system under study)." (Siebert et al 2021:4)
"ML components usually consist of several sub-components organized in pipelines: e.g., data preparation (e.g., resizing or cropping images, cleaning text), features engineering, training and evaluating the models." (Siebert et al 2021:5)
"In (Nakajima,  2018), the authors distinguish between three main qualities, namely service quality, product quality, and platform quality. They also describe different views/entities of the system: the training dataset, the neural network, the hyper parameters," (Siebert et al 2021:5)
"As a last example, the authors in Hamada et al. (2020) provide five main qualities related to views/entities, namely data integrity, model robustness, system quality, process agility, and customer expectation, including a total of 49 quality sub-properties." (Siebert et al 2021:5)
"For example, for classification tasks, the goodness of fit can be measured by accuracy, precision, recall, f-score, etc" (Siebert et al 2021:6)
"This section describes the process we followed to construct a quality model for ML systems based on our previous work in the field (Goeb et al., 2015). It consists of six steps, which we will describe sequentially, but which are performed iteratively in practice. An overview of all steps and illustrations of the major outcomes for each step are presented in Fig. 2." (Siebert et al 2021:6)
"The most basic and common one is an inconsistent understanding of quality by developers and users of ML systems. Additionally, quality criteria considered by system developers differ from the quality requirements of a system's users. Furthermore, data scientists tend to evaluate the fulfillment of quality criteria and their mutual dependencies rather implicitly, which makes it difficult to comprehend the decisions they made regarding quality." (Siebert et al 2021:8)
"Most of all, it creates a basis for considering the quality of an ML system systematically." (Siebert et al 2021:8)
"The concept of a property is general and can be used on different levels of abstraction. Entities and their properties may be abstract or specific. The basic difference between the two is that in contrast to specific properties of entities, generic ones are rather difficult to quantify and evaluate." (Siebert et al 2021:8)
"The CRISP-DM model (short for CRoss Industry Standard Process for Data Mining) (Shearer, 2000) is an open standard describing the different phases encountered in data analysis projects. This model proposes six phases, namely, business understanding, data understanding, data preparation, modeling, evaluation, deployment. It is currently thought to be the de-facto standard for projects developing ML components," (Siebert et al 2021:10)
"Data understanding The data understanding phase can be seen as a requirement engineering phase specifically directed towards the data (it usually goes hand in hand with the business understanding phase). Indeed, the type of analysis method and the corresponding evaluation measures that can be used depend on several data-related factors (besides the analysis objective): the type of data available (e.g., unstructured data like text or images vs. structured data like tabular data), whether some ground truth is available (see the discussion in the previous section), the quality of the data (e.g., its resolution, its representativity; whether noise, outliers, or missing values are present, etc.), and how the data is gathered." (Siebert et al 2021:11)
"Modeling The modeling phase is probably the tip of the iceberg when it comes to developing ML components. This is where methods such as ML are applied to form and evaluate the artifacts that make up the component. As previously mentioned, this phase is strongly linked to the data preparation phase. In general, an ML component is composed of several sub-components from these two phases. The quality of the ML model is impacted by several aspects: the type of task to be solved (e.g., classification, clustering, regression, anomaly detection, dimensionality reduction, etc.), the type of model (neural network, decision tree, etc.), the data used for building (i.e., training), and evaluating the developed artifacts, as well as the manner in which the data is separated for training and validation, together with requirements on runtime complexity or safety constraints. Since the way the component is created is experimental, the way these experiments are managed (using hyperparameter search, cross-validation, independent train-test split) also plays a role in terms of quality. The modeling phase also contains an evaluation part that aims at evaluating the trained component with regard to the available data. It does so by measuring performance" (Siebert et al 2021:11)
"measures (such as precision, recall, etc. for classification tasks), performing sensitivity analysis, or testing against adversarial examples." (Siebert et al 2021:12)
"An ML component usually consists of several subcomponents organized in a directed acyclic graph (also called a pipeline)" (Siebert et al 2021:13)
"Again, the goal of this distinction is to separate quality properties related to a specific entity instance from those related to the entity type. For example, the appropriateness of a given model applies to a model type (like the family of decision trees), whereas the goodness of fit applies to a specific trained instance." (Siebert et al 2021:13)
"In this step, we created a table of reference elements to be used in an ML quality model. We used the quality requirements and the views defined in the previous sections to select pertinent entities for the use case. From that point on, we identified quality properties of interest for all entities." (Siebert et al 2021:15)
"On the higher levels of the hierarchy, we are interested in the general quality (property) of each step of the pipeline (entity). F" (Siebert et al 2021:19)
"The processing pipeline is modeled as a hierarchy of entities (steps and sub-steps)." (Siebert et al 2021:19)
"On the lower levels of the quality model, all relevant entities and properties can be found for each step of the pipeline, such as "trained model × stability". Each property of an entity has a set of measures assigned to it and an evaluation rule describing how to evaluate the measures" (Siebert et al 2021:19)},
	annote = {Extracted Annotations (07/12/2021, 17:43:41)"In this article, we present how to systematically construct quality models for ML systems based on an industrial use case. This quality model enables practitioners to specify and assess qualities for ML systems objectively." (Siebert et al 2021:1)"e found that it is crucial to follow a systematic process in order to come up with measurable quality properties that can be evaluated in practice." (Siebert et al 2021:1)"A data-driven software component is a piece of software that" (Siebert et al 2021:1)"solves a given task (e.g., image segmentation, sentiment analysis, classification, etc.), using methods from data science, such as machine learning (ML), data mining, natural language processing, signal processing, statistics, etc. The functionality of data-driven software components (or at least part of it) is not entirely defined by the programmer in the classical way (by programming it directly), but is derived (i.e., learned) from data. At the core of a data-driven software component lies the notion of a model (or several models, coupled together in a pipeline)." (Siebert et al 2021:2)"In short, the behavior of such components is first and foremost fundamentally different from traditional (i.e., not data-driven) software: the relationship between the input and the outcome of the software component is usually non-linear." (Siebert et al 2021:2)"This input-output relationship is also only defined for a subset of the data, which leads to uncertainty in outcomes for previously unseen data" (Siebert et al 2021:2)"Second, common development principles from software engineering, such as encapsulation and modularity, have to be rethought. For example, changing the application context of a data-driven component (its intended scope), the type of model used, or the internal parameters, usually implies retraining the component. This is also referred to as the CACE principle: changing anything changes everything (Sculley et al., 2015). Third, the development and integration of datadriven software components are a multi-disciplinary approach: it requires knowledge about the application domain, knowledge about how to construct models, and finally, knowledge about software engineering. Fourth, quality assurance, and specifically testing, works differently than in traditional software. This is because data-driven methods (such as ML, for instance) target problems where the expected solution is inherently difficult to formalize, and where test oracles are not directly available (Belani et al., 2019; Bosch et al., 2018; Horkoff, 2019; Zhang et al., 2020)." (Siebert et al 2021:2)"For instance, ISO/ IEC 25010 (ISO/IEC, 2011) defines quality models for software and systems, i.e., a hierarchy of quality properties of interest and how to quantify and assess them. Due to the different nature of data-driven software components, these quality models cannot be applied directly as they are." (Siebert et al 2021:2)"To develop meaningful quality models, it is necessary to understand the application context of the use case and what kind of data-driven method is used." (Siebert et al 2021:2)"In Sect. 7, we define different views one can take on an ML system and relevant entities, which will have to be evaluated for a specific use case and application context." (Siebert et al 2021:3)"To build a quality model, it is first necessary to define the usage scenarios (i.e., How will the system be used? By whom? What are the expectations in terms of quality? etc.). This naturally goes hand in hand with the definition of the relevant quality properties and the entities to be measured. It is also necessary to define how to measure these properties and, finally, to decide on the basis of these measurements what to do to improve certain properties of quality." (Siebert et al 2021:3)"Technical robustness, reliability, dependability (e.g., correctness of output, estimation of model uncertainty, robustness against harmful inputs, errors, or unexpected situations)" (Siebert et al 2021:4)"A model is by definition a re-presentation (i.e., a simplification) of some part of reality (i.e., the system under study)." (Siebert et al 2021:4)"To define usage scenarios and elicit requirements, it is necessary to understand the intended purpose of the model." (Siebert et al 2021:4)"ML components usually consist of several sub-components organized in pipelines: e.g., data preparation (e.g., resizing or cropping images, cleaning text), features engineering, training and evaluating the models." (Siebert et al 2021:5)"In (Nakajima,  2018), the authors distinguish between three main qualities, namely service quality, product quality, and platform quality. They also describe different views/entities of the system: the training dataset, the neural network, the hyper parameters," (Siebert et al 2021:5)"As a last example, the authors in Hamada et al. (2020) provide five main qualities related to views/entities, namely data integrity, model robustness, system quality, process agility, and customer expectation, including a total of 49 quality sub-properties." (Siebert et al 2021:5)"We also see that, because the field of data science is large, the importance of certain quality properties and measures for quantifying them depends on the concrete context and use case, and they have to be addressed in different tasks of the process model used." (Siebert et al 2021:5)"For example, for classification tasks, the goodness of fit can be measured by accuracy, precision, recall, f-score, etc" (Siebert et al 2021:6)"This section describes the process we followed to construct a quality model for ML systems based on our previous work in the field (Goeb et al., 2015). It consists of six steps, which we will describe sequentially, but which are performed iteratively in practice. An overview of all steps and illustrations of the major outcomes for each step are presented in Fig. 2." (Siebert et al 2021:6)"Define quality meta-model: First of all, we described the features of our ML quality model; that is, the basic structure we want to use for documenting all quality properties of interest and the measures/metrics for quantifying those properties" (Siebert et al 2021:6)"Define use case and application context: Previous research in the field of quality modeling concluded that the concept of "quality" highly depends on the application context and concrete use case." (Siebert et al 2021:6)"Identify relevant ML quality requirements: Developing an ML model can be split into different stages related to understanding the problem to solve, gathering the required data, and building the ML model itself" (Siebert et al 2021:7)"Identity relevant entities of an ML system: In an ML system, the ML model itself is only one entity of many. To build a comprehensive quality model for ML systems, it is important to analyze all relevant entities that could come into play, such as the data, the model itself, the infrastructure on which the model is executed," (Siebert et al 2021:7)"Identify reference elements of an ML quality model: Based on the identified quality requirements from the ML development process and the relevant entities of an ML system, we can create a table of reference elements to be used in an ML quality model." (Siebert et al 2021:7)"Instantiate quality model for use case:" (Siebert et al 2021:7)"The most basic and common one is an inconsistent understanding of quality by developers and users of ML systems. Additionally, quality criteria considered by system developers differ from the quality requirements of a system's users. Furthermore, data scientists tend to evaluate the fulfillment of quality criteria and their mutual dependencies rather implicitly, which makes it difficult to comprehend the decisions they made regarding quality." (Siebert et al 2021:8)"Most of all, it creates a basis for considering the quality of an ML system systematically." (Siebert et al 2021:8)"The concept of a property is general and can be used on different levels of abstraction. Entities and their properties may be abstract or specific. The basic difference between the two is that in contrast to specific properties of entities, generic ones are rather difficult to quantify and evaluate." (Siebert et al 2021:8)"Quality evaluation comprises four basic elements: measurement, evaluation, aggregation, and interpretation. Measurement consists of the collection of measurement data for the factors specified at the lowest level of the quality model's hierarchy according to the measures defined in the quality model. Evaluation involves assessing the fulfillment of quality preferences associated with the factor. Aggregation comprises the synthesis of assessments obtained on individual child factors in a bottom-up manner throughout the quality model hierarchy into an overall assessment of a system under assessment. Finally, interpretation is the translation of the potentially abstract quality assessments into evaluations that are understandable (intuitive) for human decision makers." (Siebert et al 2021:9)"The CRISP-DM model (short for CRoss Industry Standard Process for Data Mining) (Shearer, 2000) is an open standard describing the different phases encountered in data analysis projects. This model proposes six phases, namely, business understanding, data understanding, data preparation, modeling, evaluation, deployment. It is currently thought to be the de-facto standard for projects developing ML components," (Siebert et al 2021:10)"Data understanding The data understanding phase can be seen as a requirement engineering phase specifically directed towards the data (it usually goes hand in hand with the business understanding phase). Indeed, the type of analysis method and the corresponding evaluation measures that can be used depend on several data-related factors (besides the analysis objective): the type of data available (e.g., unstructured data like text or images vs. structured data like tabular data), whether some ground truth is available (see the discussion in the previous section), the quality of the data (e.g., its resolution, its representativity; whether noise, outliers, or missing values are present, etc.), and how the data is gathered." (Siebert et al 2021:11)"Modeling The modeling phase is probably the tip of the iceberg when it comes to developing ML components. This is where methods such as ML are applied to form and evaluate the artifacts that make up the component. As previously mentioned, this phase is strongly linked to the data preparation phase. In general, an ML component is composed of several sub-components from these two phases. The quality of the ML model is impacted by several aspects: the type of task to be solved (e.g., classification, clustering, regression, anomaly detection, dimensionality reduction, etc.), the type of model (neural network, decision tree, etc.), the data used for building (i.e., training), and evaluating the developed artifacts, as well as the manner in which the data is separated for training and validation, together with requirements on runtime complexity or safety constraints. Since the way the component is created is experimental, the way these experiments are managed (using hyperparameter search, cross-validation, independent train-test split) also plays a role in terms of quality. The modeling phase also contains an evaluation part that aims at evaluating the trained component with regard to the available data. It does so by measuring performance" (Siebert et al 2021:11)"measures (such as precision, recall, etc. for classification tasks), performing sensitivity analysis, or testing against adversarial examples." (Siebert et al 2021:12)"An ML component usually consists of several subcomponents organized in a directed acyclic graph (also called a pipeline)" (Siebert et al 2021:13)"Again, the goal of this distinction is to separate quality properties related to a specific entity instance from those related to the entity type. For example, the appropriateness of a given model applies to a model type (like the family of decision trees), whereas the goodness of fit applies to a specific trained instance." (Siebert et al 2021:13)"A given pipeline may be trained several times with different model types, training algorithms, or datasets in order to find the best combinations (also known as the Combined Model Selection and Hyperparameter optimization (CASH) problem" (Siebert et al 2021:14)"Since a decision outputted by an ML component is always subject to uncertainty, and since wrong decisions might impact the system's overall quality, considering the flow of information from the system input through all components to the system output is important for understanding the impact of a given ML component's quality on the overall system behavior" (Siebert et al 2021:14)"What we call the infrastructure view is closely related to the system view. However, here the view is more focused on the quality properties related to how the system is concretely implemented (e.g., hardware, training libraries)." (Siebert et al 2021:15)"The environment consists of elements that (1) are external to the system under consideration and (2) interact either directly or indirectly with the system" (Siebert et al 2021:15)"In this step, we created a table of reference elements to be used in an ML quality model. We used the quality requirements and the views defined in the previous sections to select pertinent entities for the use case. From that point on, we identified quality properties of interest for all entities." (Siebert et al 2021:15)"On the higher levels of the hierarchy, we are interested in the general quality (property) of each step of the pipeline (entity). F" (Siebert et al 2021:19)"The processing pipeline is modeled as a hierarchy of entities (steps and sub-steps)." (Siebert et al 2021:19)"On the lower levels of the quality model, all relevant entities and properties can be found for each step of the pipeline, such as "trained model × stability". Each property of an entity has a set of measures assigned to it and an evaluation rule describing how to evaluate the measures" (Siebert et al 2021:19)},
	file = {Siebert et al. - 2021 - Construction of a quality model for machine learni.pdf:/Users/johannesreichle/Zotero/storage/8NR2ADA9/Siebert et al. - 2021 - Construction of a quality model for machine learni.pdf:application/pdf},
}

@inproceedings{nakamichi_requirements-driven_2020,
	title = {Requirements-{Driven} {Method} to {Determine} {Quality} {Characteristics} and {Measurements} for {Machine} {Learning} {Software} and {Its} {Evaluation}},
	doi = {10.1109/RE48521.2020.00036},
	abstract = {As the applications of machine learning algorithms in various fields are widely demanded, the development of machine learning software systems (MLS) is rapidly increasing. The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise. In this paper, we propose a requirements-driven method to determine the quality characteristics of the MLS. Major contributions of this paper include: (1) Extending the quality characteristics of ISO 25010, which defines the conventional software quality, to those unique to MLS; this paper also defines its measuring method. (2) A method to identify requirements, i.e., issues to be determined in the requirements definition, in order to derive the quality characteristics and measurement methods for MLS, since the quality characteristics and the measurement method depend on the goals of the system under development. In order to evaluate the proposed method, we carried out an empirical study of the quality characteristics and measurement methods related to functional correctness and the maturity of the MLS for the enterprise. Based on the study, we compare the quality characteristics and measurement methods derived by the proposed method with those suggested by developers, and demonstrate the effectiveness of the proposed method.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Nakamichi, Koji and Ohashi, Kyoko and Namba, Isao and Yamamoto, Rieko and Aoyama, Mikio and Joeckel, Lisa and Siebert, Julien and Heidrich, Jens},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	keywords = {machine learning, Machine learning, Software systems, Data models, quality assurance, Quality assurance, quality characteristics, quality measures, quality requirements, Software measurement, software quality model},
	pages = {260--270},
	annote = {Extracted Annotations (03/12/2021, 11:05:59)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (07/12/2021, 17:43:39)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (08/12/2021, 11:37:27)
"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)
"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)
"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)
"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)
"It has been noted that MLS has essentially different characteristics from those of conventional software systems [12] [21]. Sculley et al. pointed out that machine learning component is only a small portion of the entire MLS. Therefore, it is necessary to consider the entire MLS instead of machine learning component [21]. This raises new problems in software engineering [1] [15]." (Nakamichi et al 2020:261)
"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)
"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)
"mportance of measurement of machine learning performance, explainability, and specific legal requirements as the difference from the conventional requirements" (Nakamichi et al 2020:261)
"Although the quality requirements of MLS are attracting attention, there have been few conventional works on the quality requirements of MLS in the context of enterprise systems." (Nakamichi et al 2020:261)
"The requirements analyst extracts important issues from the list of items to be considered during MLS development according to the customer's needs. On the other hand, MLS quality characteristics that are an extension of the ISO/IEC 25010 quality characteristics, measures, and measurement method are defined as MLS quality evaluation models. In addition, a mapping between the above issues and the related MLS quality characteristics is provided in advance. As a result, it is possible to derive the quality and measurement items to be guaranteed by the MLS based on the customer's needs." (Nakamichi et al 2020:262)
"The context of MLS discussed in this paper consists of environment/user, system/infrastructure, model, and data as shown in Fig. 2 [7]." (Nakamichi et al 2020:262)
"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)
"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)
"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)
"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	annote = {Extracted Annotations (25/12/2021, 11:56:47)"The quality of MLS is different from that of conventional software systems, in the sense that it depends on the amount and distribution of training data in a model learning and input data during operation. This is a major challenge in quality assurance of MLS development for the enterprise." (Nakamichi et al 2020:260)"Therefore, the research emerged on machine learning software engineering and machine learning engineering [1] [12], which aims to find solutions to these problems by systematizing the development from the viewpoint of software engineering." (Nakamichi et al 2020:260)"In this paper, we focus on the required quality of the features of MLS in order to meet the quality requirements of enterprise information systems." (Nakamichi et al 2020:260)"It is necessary that the MLS provider creates an MLS specification including not only functional requirements but also quality requirements, and agrees with the customer. The quality requirements include the degree of the property which MLS should realize, the assumed usage context of MLS, and constraints." (Nakamichi et al 2020:260)"It has been noted that MLS has essentially different characteristics from those of conventional software systems [12] [21]. Sculley et al. pointed out that machine learning component is only a small portion of the entire MLS. Therefore, it is necessary to consider the entire MLS instead of machine learning component [21]. This raises new problems in software engineering [1] [15]." (Nakamichi et al 2020:261)"However, few papers identified in the survey discuss the quality model of MLS. Kuwajima, et al. discussed on the safety as the most important quality characteristic in automated driving, which is noted as a key application of MLS. However, the paper suggests that the research on the quality problems is still at the early stage [13]." (Nakamichi et al 2020:261)"Regarding a definition of the requirements for MLS, Belani et al. proposed an outline of RE4AI taxonomy for MLS development from the viewpoint of requirements engineering" (Nakamichi et al 2020:261)"mportance of measurement of machine learning performance, explainability, and specific legal requirements as the difference from the conventional requirements" (Nakamichi et al 2020:261)"Although the quality requirements of MLS are attracting attention, there have been few conventional works on the quality requirements of MLS in the context of enterprise systems." (Nakamichi et al 2020:261)"The requirements analyst extracts important issues from the list of items to be considered during MLS development according to the customer's needs. On the other hand, MLS quality characteristics that are an extension of the ISO/IEC 25010 quality characteristics, measures, and measurement method are defined as MLS quality evaluation models. In addition, a mapping between the above issues and the related MLS quality characteristics is provided in advance. As a result, it is possible to derive the quality and measurement items to be guaranteed by the MLS based on the customer's needs." (Nakamichi et al 2020:262)"The context of MLS discussed in this paper consists of environment/user, system/infrastructure, model, and data as shown in Fig. 2 [7]." (Nakamichi et al 2020:262)"Table 1. List of MLS Issues (Part)" (Nakamichi et al 2020:262)"We first define a quality evaluation meta-model consisting of three parts as shown in Fig. 2. In the "MLS Quality Model" part, "Quality Characteristic" is defined hierarchically. In the "Quality Measurement of MLS" part, "Measure" is associated with "QualityCharacteristics" on the lowest layer," (Nakamichi et al 2020:263)"Table 2. List of MSL Quality Characteristics (Part)" (Nakamichi et al 2020:263)"We assume that the correctness of a function of MLS corresponds to the correctness of the output data. The output data, test data, and trained models of MLS were specified as entities related to the MLS output. We defined the properties of these entities to be evaluated in terms of functional correctness. The properties are listed below: (1) Representativeness: A property of the data; the degree to which the statistical characteristics of the actual data set are consistent with the statistics assumed for the data set. (2) Independence: A property of test data; the extent to which test data are independent from training data." (Nakamichi et al 2020:263)"(3) Fitness: A quality of the output data of the MLS and trained models, and the degree of correctness." (Nakamichi et al 2020:264)"First, we identified 22 issues related to the requirements specifications of MLS, which are classified as environment/user, system/infrastructure, model, and data. Then, we defined quality characteristics and sub-characteristics specific to MLS by extending the quality model of ISO 25010. The MLS quality characteristics consist of 34 properties, of which, 18 were MLS-specific and 16 were derived from ISO 25010. We also defined a measure for each property." (Nakamichi et al 2020:269)},
	file = {Nakamichi et al_2020_Requirements-Driven Method to Determine Quality Characteristics and.pdf:/Users/johannesreichle/Zotero/storage/GZJKLEGC/Nakamichi et al_2020_Requirements-Driven Method to Determine Quality Characteristics and.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/AWR84EVA/9218162.html:text/html},
}

@article{ashmore_assuring_2021,
	title = {Assuring the {Machine} {Learning} {Lifecycle}: {Desiderata}, {Methods}, and {Challenges}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Assuring the {Machine} {Learning} {Lifecycle}},
	url = {https://dl.acm.org/doi/10.1145/3453444},
	doi = {10.1145/3453444},
	abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic, and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence, and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our article provides a comprehensive survey of the state of the art in the
              assurance of ML
              , i.e., in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the
              machine learning lifecycle
              , i.e., of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The article begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
	language = {en},
	number = {5},
	urldate = {2021-12-01},
	journal = {ACM Computing Surveys},
	author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
	month = jun,
	year = {2021},
	pages = {1--39},
	annote = {Extracted Annotations (01/12/2021, 16:22:29)
"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)
"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)
"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)
"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)
"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)
"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	annote = {Extracted Annotations (02/12/2021, 11:48:08)
"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)
"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)
"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)
"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)
"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)
"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	annote = {Extracted Annotations (15/12/2021, 15:19:31)"Relevant—This property considers the intersection between the dataset and the desired behaviour in the intended operational domain. For example, a dataset that only included German road signs would not be Relevant for a system intended to operate on UK roads. (2) Complete—This property considers the way samples are distributed across the input domain and subspaces of it. In particular, it considers whether suitable distributions and combinations of features are present. For example, an image dataset that displayed an inappropriate correlation between image background and type of animal would not be complete [138]. (3) Balanced—This property considers the distribution of features that are included in the dataset. For classification problems, a key consideration is the balance between the number of samples in each class [63]. This property takes an internal perspective; it focuses on the dataset as an abstract entity to which a generic learning algorithm will be applied. In contrast, the Complete property takes an external perspective; it considers the dataset within the intended operational domain. (4) Accurate—This property considers how measurement (and measurement-like) issues can affect the way that samples reflect the intended operational domain. It covers aspects like sensor accuracy and labelling errors [26]. The correctness of data collection and preprocessing software is also relevant to this property, as is configuration management." (Ashmore et al 2021:7)"5.2.1 Model Selection. This activity decides the model type, variant and, where applicable, the structure of the model to be produced in the Model Learning stage. Numerous types of ML models are available [113, 152], including multiple types of classification models (which identify the category that the input belongs to), regression models (which predict a continuous-valued attribute), clustering models (which group similar items into sets), and reinforcement learning models (which provide an optimal set of actions, i.e., a policy, for solving, for instance, a navigation or planning problem)." (Ashmore et al 2021:13)"5.3 Desiderata From an assurance viewpoint, the models generated by the Model Learning stage should exhibit some or all of the key properties described below: (1) Performant—This property considers quantitative performance metrics applied to the model when deployed within a system. These metrics include traditional ML metrics such as classification accuracy, the receiver operator characteristic (ROC) and mean squared error, as well as metrics that consider the system and environment into which the models are deployed." (Ashmore et al 2021:13)"(2) Robust—This property considers the model's ability to perform well in circumstances where the inputs encountered at runtime are different to those present in the training data. Robustness may be considered with respect to environmental uncertainty, e.g., flooded roads, and system-level variability, e.g., sensor failure, i.e., from the general perspective used in formal verification rather than its ML interpretation as the ability of a model to generalise to data not encountered in training [20, 192]. (3) Reusable—This property considers the ability of a model, or of components of a model, to be reused in systems for which they were not originally intended. For example, a neural network trained for facial recognition in an authentication system may have features that can be reused to identify operator fatigue. More generally, the reuse of pre-trained or commodity off-the-shelf ML models in transfer learning can significantly speed up model learning [108]. (4) Interpretable—This property considers the extent to which the model can produce artefacts that support the analysis of its output, and thus of any decisions based on it. For example, a decision tree may support the production of a narrative explaining the decision to hand over control to a human operator." (Ashmore et al 2021:14)"An ML model is performant if it operates as expected according to a measure (or set of measures) that captures relevant characteristics of the model output." (Ashmore et al 2021:14)"Increasing model complexity generally reduces training errors, but noise in the training data may result in overfitting and in a failure of the model to generalise to real-world data." (Ashmore et al 2021:16)"5.4.3 Reusable. Machine learning is typically computationally expensive, and repurposing models from related domains can reduce the cost of training new models. Transfer learning [183] allows for a model learned in one domain to be exploited in a second domain, as long as the domains are similar enough so that features learned in the source domain are applicable to the target domain. Where this is the case, all or part of a model may be transferred to reduce the training cost. Convolutional neural networks (CNN) are particularly suited for partial model transfer [59] since the convolutional layers encode features in the input space, whilst the fully connected layers encode reasoning based on those features." (Ashmore et al 2021:17)},
	file = {Ashmore et al. - 2021 - Assuring the Machine Learning Lifecycle Desiderat.pdf:/Users/johannesreichle/Zotero/storage/RJRLZU8A/Ashmore et al. - 2021 - Assuring the Machine Learning Lifecycle Desiderat.pdf:application/pdf},
}

@inproceedings{seshia_formal_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Formal {Specification} for {Deep} {Neural} {Networks}},
	isbn = {978-3-030-01090-4},
	doi = {10.1007/978-3-030-01090-4_2},
	abstract = {The increasing use of deep neural networks in a variety of applications, including some safety-critical ones, has brought renewed interest in the topic of verification of neural networks. However, verification is most meaningful when performed with high-quality formal specifications. In this paper, we survey the landscape of formal specification for deep neural networks, and discuss the opportunities and challenges for formal methods for this domain.},
	language = {en},
	booktitle = {Automated {Technology} for {Verification} and {Analysis}},
	publisher = {Springer International Publishing},
	author = {Seshia, Sanjit A. and Desai, Ankush and Dreossi, Tommaso and Fremont, Daniel J. and Ghosh, Shromona and Kim, Edward and Shivakumar, Sumukh and Vazquez-Chanlatte, Marcell and Yue, Xiangyu},
	editor = {Lahiri, Shuvendu K. and Wang, Chao},
	year = {2018},
	pages = {20--34},
	annote = {Extracted Annotations (02/12/2021, 11:48:27)
"Consider a sample space Z of the form X × Y , and an ordered training set S = ((xi, yi))m=1, where xi ∈ X is the data and yi ∈ Y is the corresponding label. Let H be a hypothesis space (e.g., a particular neural network architecture parameterized by a weight vectorw). If the network computes a function from X to Y , we will denote it by fw; i.e., fw(x) = y. There is a loss (or risk) function : H × Z →R so that given a hypothesisw ∈ H and a sample (x, y) ∈ Z, we" (Seshia et al 2018:21)
"obtain a loss (w, (x, y)). We consider the case where we want to minimize the average loss over the training set S, 1 m LS(w) = m In the equation given above, λ {\textgreater} 0 and the term R(w) is called the regularizer; the latter seeks to enforce a notion of "simplicity" inw. Since S is fixed, we sometimes denote i(w) = (w, (xi, yi)) as a function only ofw. The training problem is to find aw that minimizes Ls(w); i.e., we wish to solve the following optimization problem: min LS(w) w∈H" (Seshia et al 2018:22)
"In recent years, a significant amount of work has addressed the robustness (or lack thereof) of neural networks to so-called "adversarial perturbations" of their inputs (for example, [5, 9, 27, 38, 42, 43, 54, 58]). Techniques used to demonstrate a lack of robustness are often referred to as "adversarial analysis."" (Seshia et al 2018:24)},
	file = {Seshia et al_2018_Formal Specification for Deep Neural Networks.pdf:/Users/johannesreichle/Zotero/storage/I2VTXLR2/Seshia et al_2018_Formal Specification for Deep Neural Networks.pdf:application/pdf},
}

@article{zhang_machine_2020,
	title = {Machine {Learning} {Testing}: {Survey}, {Landscapes} and {Horizons}},
	issn = {1939-3520},
	shorttitle = {Machine {Learning} {Testing}},
	doi = {10.1109/TSE.2019.2962027},
	abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 138 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in machine learning testing.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Software Engineering},
	keywords = {machine learning, Machine learning, Robustness, Software engineering, Data models, deep neural network, software testing, Software testing, Training data},
	pages = {1--1},
	file = {Zhang et al_2020_Machine Learning Testing.pdf:/Users/johannesreichle/Zotero/storage/SYKPYQ9U/Zhang et al_2020_Machine Learning Testing.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/YJDAU47Q/9000651.html:text/html},
}

@inproceedings{xing_convolutional_2019,
	address = {Seoul, Korea (South)},
	title = {Convolutional {Character} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010699/},
	doi = {10.1109/ICCV.2019.00922},
	abstract = {Recent progress has been made on developing a uniﬁed framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage frameworks by involving ROI pooling, which can degrade the performance on recognition tasks. In this work, we propose convolutional character networks (”CharNet”), which is a one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize a character as basic element, allowing us to overcome the main difﬁculty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful onestage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-theart approaches [26, 25] by a large margin, e.g., with improvements of 65.33\%→71.08\% (with generic lexicon) on ICDAR 2015, and 54.0\%→69.23\% on Total-Text, on endto-end text recognition. Code is available at: https:// github.com/MalongTech/research-charnet.},
	language = {en},
	urldate = {2021-12-10},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Xing, Linjie and Tian, Zhi and Huang, Weilin and Scott, Matthew},
	month = oct,
	year = {2019},
	pages = {9125--9135},
	file = {Xing et al. - 2019 - Convolutional Character Networks.pdf:/Users/johannesreichle/Zotero/storage/P5B8NDTU/Xing et al. - 2019 - Convolutional Character Networks.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
	annote = {Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models},
	file = {Goodfellow et al_2016_Deep learning.pdf:/Users/johannesreichle/Zotero/storage/675ZE798/Goodfellow et al_2016_Deep learning.pdf:application/pdf},
}

@book{abramowicz_business_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Business {Information} {Systems}: 22nd {International} {Conference}, {BIS} 2019, {Seville}, {Spain}, {June} 26–28, 2019, {Proceedings}, {Part} {II}},
	volume = {354},
	isbn = {978-3-030-20481-5 978-3-030-20482-2},
	shorttitle = {Business {Information} {Systems}},
	url = {http://link.springer.com/10.1007/978-3-030-20482-2},
	language = {en},
	urldate = {2021-12-14},
	publisher = {Springer International Publishing},
	editor = {Abramowicz, Witold and Corchuelo, Rafael},
	year = {2019},
	doi = {10.1007/978-3-030-20482-2},
	file = {Abramowicz and Corchuelo - 2019 - Business Information Systems 22nd International C.pdf:/Users/johannesreichle/Zotero/storage/5RZWASEF/Abramowicz and Corchuelo - 2019 - Business Information Systems 22nd International C.pdf:application/pdf},
}

@misc{noauthor_yangxue0827rotationdetection_nodate,
	title = {yangxue0827/{RotationDetection}: {This} is a tensorflow-based rotation detection benchmark, also called {AlphaRotate}.},
	url = {https://github.com/yangxue0827/RotationDetection},
	urldate = {2021-12-15},
	file = {yangxue0827/RotationDetection\: This is a tensorflow-based rotation detection benchmark, also called AlphaRotate.:/Users/johannesreichle/Zotero/storage/UD6Y49BQ/RotationDetection.html:text/html},
}

@inproceedings{ignatov_ai_2019,
	title = {{AI} {Benchmark}: {All} {About} {Deep} {Learning} on {Smartphones} in 2019},
	shorttitle = {{AI} {Benchmark}},
	doi = {10.1109/ICCVW.2019.00447},
	abstract = {The performance of mobile AI accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of SoCs. The current 4th generation of mobile NPUs is already approaching the results of CUDA-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep AI models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware acceleration for AI inference. We also discuss the recent changes in the Android ML pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Machine learning, Machine Learning, Acceleration, AI Benchmark, Android, Androids, Artificial Intelligence, Benchmark, Computer Vision, Deep Learning, Humanoid robots, Mobile, Mobile handsets, Performance evaluation, Smartphones, SoCs, TensorFlow},
	pages = {3617--3635},
	file = {Ignatov et al_2019_AI Benchmark.pdf:/Users/johannesreichle/Zotero/storage/YM4YKD6J/Ignatov et al_2019_AI Benchmark.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/MGC3F8N2/9022101.html:text/html},
}

@article{chng_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Arbitrary}-{Shaped} {Text} ({RRC}-{ArT})},
	url = {http://arxiv.org/abs/1909.07145},
	abstract = {This paper reports the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT) that consists of three major challenges: i) scene text detection, ii) scene text recognition, and iii) scene text spotting. A total of 78 submissions from 46 unique teams/individuals were received for this competition. The top performing score of each challenge is as follows: i) T1 - 82.65\%, ii) T2.1 - 74.3\%, iii) T2.2 - 85.32\%, iv) T3.1 - 53.86\%, and v) T3.2 - 54.91\%. Apart from the results, this paper also details the ArT dataset, tasks description, evaluation metrics and participants methods. The dataset, the evaluation kit as well as the results are publicly available at https://rrc.cvc.uab.es/?ch=14},
	urldate = {2021-12-21},
	journal = {arXiv:1909.07145 [cs]},
	author = {Chng, Chee-Kheng and Liu, Yuliang and Sun, Yipeng and Ng, Chun Chet and Luo, Canjie and Ni, Zihan and Fang, ChuanMing and Zhang, Shuaitao and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and Chan, Chee Seng and Jin, Lianwen},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.07145
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Technical report of ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT) Competition},
	file = {Chng et al_2019_ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT).pdf:/Users/johannesreichle/Zotero/storage/QYHGFJ2X/Chng et al_2019_ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text (RRC-ArT).pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/MV9GJITH/1909.html:text/html},
}

@inproceedings{baek_what_2019,
	address = {Seoul, Korea (South)},
	title = {What {Is} {Wrong} {With} {Scene} {Text} {Recognition} {Model} {Comparisons}? {Dataset} and {Model} {Analysis}},
	isbn = {978-1-72814-803-8},
	shorttitle = {What {Is} {Wrong} {With} {Scene} {Text} {Recognition} {Model} {Comparisons}?},
	url = {https://ieeexplore.ieee.org/document/9010273/},
	doi = {10.1109/ICCV.2019.00481},
	abstract = {Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the ﬁeld due to the inconsistent choices of training and evaluation datasets. This paper addresses this difﬁculty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a uniﬁed four-stage STR framework that most existing STR models ﬁt into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available1.},
	language = {en},
	urldate = {2021-12-21},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Baek, Jeonghun and Kim, Geewook and Lee, Junyeop and Park, Sungrae and Han, Dongyoon and Yun, Sangdoo and Oh, Seong Joon and Lee, Hwalsuk},
	month = oct,
	year = {2019},
	pages = {4714--4722},
	annote = {Extracted Annotations (26/12/2021, 10:22:48)"First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into." (Baek et al 2019:4714)"The maturity of Optical Character Recognition (OCR) systems has led to its successful application on cleaned documents, but most traditional OCR methods have failed to be as effective on STR tasks due to the diverse text appearances that occur in the real world and the imperfect conditions in which these scenes are captured." (Baek et al 2019:4714)"Finally, we study the module-wise contributions in terms of accuracy, speed, and memory demand, under a unified experimental setting" (Baek et al 2019:4715)"Due to the resemblence of STR to computer vision tasks (e.g. object detection) and sequence prediction tasks, STR has benefited from high-performance convolutional neural networks (CNNs) and recurrent neural networks (RNNs)." (Baek et al 2019:4717)"The four stages derived from existing STR models are as follows: 1. Transformation (Trans.) normalizes the input text image using the Spatial Transformer Network (STN [11]) to ease downstream stages. 2. Feature extraction (Feat.) maps the input image to a representation that focuses on the attributes relevant for character recognition, while suppressing irrelevant features such as font, color, size, and background. 3. Sequence modeling (Seq.) captures the contextual information within a sequence of characters for the next stage to predict each character more robustly, rather than doing it independently. 4. Prediction (Pred.) estimates the output character sequence from the identified features of an image." (Baek et al 2019:4717)"In this stage, from the input H, a module predict a sequence of characters, (i.e., Y = y1 , y2 , . . . ). By summing up previous works, we have two options for prediction: (1) Connectionist temporal classification (CTC) [6] and (2) attention-based sequence prediction (Attn) [24, 4]." (Baek et al 2019:4718)"Vertical texts: most of current STR models assumes horizontal text images, and thus structurally could not deal with vertical texts. Some STR models [28, 5] exploit vertical information also, however, vertical texts are not clearly covered yet. Further research would be needed to cover vertical texts." (Baek et al 2019:4721)},
	file = {Baek et al. - 2019 - What Is Wrong With Scene Text Recognition Model Co.pdf:/Users/johannesreichle/Zotero/storage/82G8BT9K/Baek et al. - 2019 - What Is Wrong With Scene Text Recognition Model Co.pdf:application/pdf},
}

@article{atienza_vision_2021,
	title = {Vision {Transformer} for {Fast} and {Efficient} {Scene} {Text} {Recognition}},
	url = {http://arxiv.org/abs/2105.08582},
	abstract = {Scene text recognition (STR) enables computers to read text in natural scenes such as object labels, road signs and instructions. STR helps machines perform informed decisions such as what object to pick, which direction to go, and what is the next step of action. In the body of work on STR, the focus has always been on recognition accuracy. There is little emphasis placed on speed and computational efficiency which are equally important especially for energy-constrained mobile machines. In this paper we propose ViTSTR, an STR with a simple single stage model architecture built on a compute and parameter efficient vision transformer (ViT). On a comparable strong baseline method such as TRBA with accuracy of 84.3\%, our small ViTSTR achieves a competitive accuracy of 82.6\% (84.2\% with data augmentation) at 2.4x speed up, using only 43.4\% of the number of parameters and 42.2\% FLOPS. The tiny version of ViTSTR achieves 80.3\% accuracy (82.1\% with data augmentation), at 2.5x the speed, requiring only 10.9\% of the number of parameters and 11.9\% FLOPS. With data augmentation, our base ViTSTR outperforms TRBA at 85.2\% accuracy (83.7\% without augmentation) at 2.3x the speed but requires 73.2\% more parameters and 61.5\% more FLOPS. In terms of trade-offs, nearly all ViTSTR configurations are at or near the frontiers to maximize accuracy, speed and computational efficiency all at the same time.},
	urldate = {2021-12-21},
	journal = {arXiv:2105.08582 [cs]},
	author = {Atienza, Rowel},
	month = may,
	year = {2021},
	note = {arXiv: 2105.08582},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear at ICDAR2021 Springer Lecture Notes in Computer Science series},
	file = {Atienza_2021_Vision Transformer for Fast and Efficient Scene Text Recognition.pdf:/Users/johannesreichle/Zotero/storage/8HCJS8TP/Atienza_2021_Vision Transformer for Fast and Efficient Scene Text Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/QVWX5KYD/2105.html:text/html},
}

@article{kumar_scene_2016,
	title = {Scene {Text} {Recognition} using {Artificial} {Neural} {Network}: {A} {Survey}},
	volume = {137},
	shorttitle = {Scene {Text} {Recognition} using {Artificial} {Neural} {Network}},
	doi = {10.5120/ijca2016908804},
	abstract = {Nowadays, scene text recognition has become an important emerging area of research in the field of image processing. In image processing, character recognition boosts the complexity in the area of Artificial Intelligence. Character recognition is not easy for computer programs in comparison to humans. In the broad spectrum of things, it may consider that recognizing patterns is the only thing which humans can do well and computers cannot. There are many reasons including various sources of variability, hypothesis and absence of hard-and-fast rules that define the appearance of a visual character. Hence; there is an unavoidable requirement for heuristic deduction of rules from different samples. This review highlights the superiority of artificial neural networks, a popular area of Artificial Intelligence, over various other available methods like fuzzy logic and genetic algorithm. In this paper, two methods are listed for character recognition – offline and online. The ―Offline‖ methods include Feature Extraction, Clustering, and Pattern Matching. Artificial neural networks use the static image properties. The online methods are divided into two methods, k-NN classifier and direction based algorithm. Thus, the scale of techniques available for scene text recognition deserves an admiration. This review gives a detail survey of use of artificial neural network in scene text recognition.},
	journal = {International Journal of Computer Applications},
	author = {Kumar, Sunil and Kumar, Krishan and Mishra, Rahul},
	month = apr,
	year = {2016},
	pages = {975--8887},
	file = {Kumar et al_2016_Scene Text Recognition using Artificial Neural Network.pdf:/Users/johannesreichle/Zotero/storage/8SIPHDJT/Kumar et al_2016_Scene Text Recognition using Artificial Neural Network.pdf:application/pdf},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Machine learning, Computer algorithms},
}

@inproceedings{chauhan_review_2018,
	title = {A {Review} on {Conventional} {Machine} {Learning} vs {Deep} {Learning}},
	doi = {10.1109/GUCON.2018.8675097},
	abstract = {In now days, deep learning has become a prominent and emerging research area in computer vision applications. Deep learning permits the multiple layers models for computation to learn representations of data by processing in their original form while it is not possible in conventional machine learning. These methods surprisingly improved the accuracy of various image processing domains such as speech recognition, face recognition, object detection and in biomedical applications. Deep neural networks (DNN) such as convolutional neural network (CNN) provide tremendous results in processing of images and videos, while another approach of deep network i.e. recurrent neural network (RNN) gives better performance with sequential data such as text and speech.},
	booktitle = {2018 {International} {Conference} on {Computing}, {Power} and {Communication} {Technologies} ({GUCON})},
	author = {Chauhan, Nitin Kumar and Singh, Krishna},
	month = sep,
	year = {2018},
	keywords = {Deep learning, Neural networks, CNN, ANN, Classification algorithms, DNN, DT, Fully connected layers, LDA, Machine learning algorithms, Neurons, PCA, Pooling layers, QDA, RBM, RNN, Support vector machines, SVM},
	pages = {347--352},
	file = {Chauhan_Singh_2018_A Review on Conventional Machine Learning vs Deep Learning.pdf:/Users/johannesreichle/Zotero/storage/J833HDNX/Chauhan_Singh_2018_A Review on Conventional Machine Learning vs Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/VAR38ZMB/8675097.html:text/html},
}

@book{richter_statistisches_2019,
	address = {Berlin, Heidelberg},
	title = {Statistisches und maschinelles {Lernen}: {Gängige} {Verfahren} im Überblick},
	isbn = {978-3-662-59353-0 978-3-662-59354-7},
	shorttitle = {Statistisches und maschinelles {Lernen}},
	url = {http://link.springer.com/10.1007/978-3-662-59354-7},
	language = {de},
	urldate = {2022-01-09},
	publisher = {Springer Berlin Heidelberg},
	author = {Richter, Stefan},
	year = {2019},
	doi = {10.1007/978-3-662-59354-7},
	file = {Richter - 2019 - Statistisches und maschinelles Lernen Gängige Ver.pdf:/Users/johannesreichle/Zotero/storage/8Y32ACBZ/Richter - 2019 - Statistisches und maschinelles Lernen Gängige Ver.pdf:application/pdf},
}

@book{geron_hands-machine_2017,
	address = {Beijing ; Boston},
	edition = {First edition},
	title = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4919-6229-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}},
	abstract = {"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" --},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	year = {2017},
	note = {OCLC: ocn953432302},
	keywords = {Artificial intelligence, Machine learning, Automatische Klassifikation, COMPUTERS / Computer Vision \& Pattern Recognition, COMPUTERS / Data Processing, COMPUTERS / Intelligence (AI) \& Semantics, COMPUTERS / Natural Language Processing, COMPUTERS / Neural Networks, Künstliche Intelligenz, Maschinelles Lernen, Nonfiction, Python 3.0},
	annote = {Includes index},
	file = {Géron - Hands-On Machine Learning with Scikit-Learn and Te.pdf:/Users/johannesreichle/Zotero/storage/CXQSL6VU/Géron - Hands-On Machine Learning with Scikit-Learn and Te.pdf:application/pdf},
}

@article{ye_least_nodate,
	title = {Least {Squares} {Linear} {Discriminant} {Analysis}},
	abstract = {Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classiﬁcation. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for binary-class classiﬁcations can be formulated as a least squares problem. Previous studies have shown certain relationship between multivariate linear regression and LDA for the multi-class case. Many of these studies show that multivariate linear regression with a speciﬁc class indicator matrix as the output can be applied as a preprocessing step for LDA. However, directly casting LDA as a least squares problem is challenging for the multi-class case. In this paper, a novel formulation for multivariate linear regression is proposed. The equivalence relationship between the proposed least squares formulation and LDA for multi-class classiﬁcations is rigorously established under a mild condition, which is shown empirically to hold in many applications involving high-dimensional data. Several LDA extensions based on the equivalence relationship are discussed.},
	language = {en},
	author = {Ye, Jieping},
	pages = {8},
	file = {Ye - Least Squares Linear Discriminant Analysis.pdf:/Users/johannesreichle/Zotero/storage/ARDF7AQJ/Ye - Least Squares Linear Discriminant Analysis.pdf:application/pdf},
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	annote = {Includes index},
}

@article{ho_real-world-weight_2020,
	title = {The {Real}-{World}-{Weight} {Cross}-{Entropy} {Loss} {Function}: {Modeling} the {Costs} of {Mislabeling}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {The {Real}-{World}-{Weight} {Cross}-{Entropy} {Loss} {Function}},
	doi = {10.1109/ACCESS.2019.2962617},
	abstract = {In this paper, we propose a new metric to measure goodness-of-fit for classifiers: the Real World Cost function. This metric factors in information about a real world problem, such as financial impact, that other measures like accuracy or F1 do not. This metric is also more directly interpretable for users. To optimize for this metric, we introduce the Real-World-Weight Cross-Entropy loss function, in both binary classification and single-label multiclass classification variants. Both variants allow direct input of real world costs as weights. For single-label, multiclass classification, our loss function also allows direct penalization of probabilistic false positives, weighted by label, during the training of a machine learning model. We compare the design of our loss function to the binary cross-entropy and categorical cross-entropy functions, as well as their weighted variants, to discuss the potential for improvement in handling a variety of known shortcomings of machine learning, ranging from imbalanced classes to medical diagnostic error to reinforcement of social bias. We create scenarios that emulate those issues using the MNIST data set and demonstrate empirical results of our new loss function. Finally, we discuss our intuition about why this approach works and sketch a proof based on Maximum Likelihood Estimation.},
	journal = {IEEE Access},
	author = {Ho, Yaoshiang and Wookey, Samuel},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Neural networks, Machine learning, Training, Measurement, class imbalance, cross-entropy, ethnic stereotypes, maximum likelihood estimation, Maximum likelihood estimation, oversampling, Probabilistic logic, social bias, softmax, Standards, undersampling},
	pages = {4806--4813},
	file = {Ho_Wookey_2020_The Real-World-Weight Cross-Entropy Loss Function.pdf:/Users/johannesreichle/Zotero/storage/U6H99L69/Ho_Wookey_2020_The Real-World-Weight Cross-Entropy Loss Function.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/TVP49BNC/8943952.html:text/html},
}

@article{wolpert_lack_1996,
	title = {The {Lack} of {A} {Priori} {Distinctions} {Between} {Learning} {Algorithms}},
	volume = {8},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1996.8.7.1341},
	doi = {10.1162/neco.1996.8.7.1341},
	abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
	number = {7},
	urldate = {2022-01-13},
	journal = {Neural Computation},
	author = {Wolpert, David H.},
	month = oct,
	year = {1996},
	pages = {1341--1390},
	file = {Snapshot:/Users/johannesreichle/Zotero/storage/KB5LF5VV/The-Lack-of-A-Priori-Distinctions-Between-Learning.html:text/html;Wolpert_ARTICLE Communicated by Steven Nowlan The Lack of A Priori Distinctions Between.pdf:/Users/johannesreichle/Zotero/storage/Y48F3CJS/Wolpert_ARTICLE Communicated by Steven Nowlan The Lack of A Priori Distinctions Between.pdf:application/pdf},
}

@article{boue_deep_2018,
	title = {Deep learning for pedestrians: backpropagation in {CNNs}},
	shorttitle = {Deep learning for pedestrians},
	url = {http://arxiv.org/abs/1811.11987},
	abstract = {The goal of this document is to provide a pedagogical introduction to the main concepts underpinning the training of deep neural networks using gradient descent; a process known as backpropagation. Although we focus on a very influential class of architectures called "convolutional neural networks" (CNNs) the approach is generic and useful to the machine learning community as a whole. Motivated by the observation that derivations of backpropagation are often obscured by clumsy index-heavy narratives that appear somewhat mathemagical, we aim to offer a conceptually clear, vectorized description that articulates well the higher level logic. Following the principle of "writing is nature's way of letting you know how sloppy your thinking is", we try to make the calculations meticulous, self-contained and yet as intuitive as possible. Taking nothing for granted, ample illustrations serve as visual guides and an extensive bibliography is provided for further explorations. (For the sake of clarity, long mathematical derivations and visualizations have been broken up into short "summarized views" and longer "detailed views" encoded into the PDF as optional content groups. Some figures contain animations designed to illustrate important concepts in a more engaging style. For these reasons, we advise to download the document locally and open it using Adobe Acrobat Reader. Other viewers were not tested and may not render the detailed views, animations correctly.)},
	urldate = {2022-01-20},
	journal = {arXiv:1811.11987 [cs, stat]},
	author = {Boué, Laurent},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.11987},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Symbolic Computation},
	file = {Boué_2018_Deep learning for pedestrians.pdf:/Users/johannesreichle/Zotero/storage/QAQQ4UWJ/Boué_2018_Deep learning for pedestrians.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/LHFH7XKK/1811.html:text/html},
}

@inproceedings{davis_relationship_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {The relationship between {Precision}-{Recall} and {ROC} curves},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
	doi = {10.1145/1143844.1143874},
	abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an eﬃcient algorithm for computing this curve. Finally, we also note diﬀerences in the two types of curves are signiﬁcant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
	language = {en},
	urldate = {2022-01-23},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	author = {Davis, Jesse and Goadrich, Mark},
	year = {2006},
	pages = {233--240},
	file = {Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:/Users/johannesreichle/Zotero/storage/ZMM8UG98/Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:application/pdf},
}

@inproceedings{liu_ssd_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	isbn = {978-3-319-46448-0},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300300×300300 {\textbackslash}times 300 input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512×512512×512512 {\textbackslash}times 512 input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional neural network, Real-time object detection},
	pages = {21--37},
	file = {Liu et al_2016_SSD.pdf:/Users/johannesreichle/Zotero/storage/JWLQEVVX/Liu et al_2016_SSD.pdf:application/pdf},
}

@article{qin_towards_nodate,
	title = {Towards {Unconstrained} {End}-to-{End} {Text} {Spotting}},
	abstract = {We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectiﬁcation. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to signiﬁcant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6\%, and on the Total-Text (curved) benchmark by more than 16\%.},
	language = {en},
	author = {Qin, Siyang and Bissacco, Alessandro and Raptis, Michalis and Fujii, Yasuhisa and Xiao, Ying},
	pages = {11},
	file = {Qin et al. - Towards Unconstrained End-to-End Text Spotting.pdf:/Users/johannesreichle/Zotero/storage/FSW8B4J8/Qin et al. - Towards Unconstrained End-to-End Text Spotting.pdf:application/pdf},
}

@inproceedings{su_relationship_2015,
	address = {Northampton Massachusetts USA},
	title = {A {Relationship} between the {Average} {Precision} and the {Area} {Under} the {ROC} {Curve}},
	isbn = {978-1-4503-3833-2},
	url = {https://dl.acm.org/doi/10.1145/2808194.2809481},
	doi = {10.1145/2808194.2809481},
	abstract = {For similar evaluation tasks, the area under the receiver operating characteristic curve (AUC) is often used by researchers in machine learning, whereas the average precision (AP) is used more often by the information retrieval community. We establish some results to explain why this is the case. Speciﬁcally, we show that, when both the AUC and the AP are rescaled to lie in [0,1], the AP is approximately the AUC times the initial precision of the system.},
	language = {en},
	urldate = {2022-01-23},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {The} {Theory} of {Information} {Retrieval}},
	publisher = {ACM},
	author = {Su, Wanhua and Yuan, Yan and Zhu, Mu},
	month = sep,
	year = {2015},
	pages = {349--352},
	file = {Su et al. - 2015 - A Relationship between the Average Precision and t.pdf:/Users/johannesreichle/Zotero/storage/CAT4EF9T/Su et al. - 2015 - A Relationship between the Average Precision and t.pdf:application/pdf},
}

@article{alzubi_machine_2018,
	title = {Machine {Learning} from {Theory} to {Algorithms}: {An} {Overview}},
	volume = {1142},
	issn = {1742-6588, 1742-6596},
	shorttitle = {Machine {Learning} from {Theory} to {Algorithms}},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1142/1/012012},
	doi = {10.1088/1742-6596/1142/1/012012},
	abstract = {The current SMAC (Social, Mobile, Analytic, Cloud) technology trend paves the way to a future in which intelligent machines, networked processes and big data are brought together. This virtual world has generated vast amount of data which is accelerating the adoption of machine learning solutions\& practices. Machine Learning enables computers to imitate and adapt human-like behaviour. Using machine learning, each interaction, each action performed, becomes something the system can learn and use as experience for the next time. This work is an overview of this data analytics method which enables computers to learn and do what comes naturally to humans, i.e. learn from experience. It includes the preliminaries of machine learning, the definition, nomenclature and applications’ describing it’s what, how and why. The technology roadmap of machine learning is discussed to understand and verify its potential as a market \& industry practice. The primary intent of this work is to give insight into why machine learning is the future.},
	language = {en},
	urldate = {2022-01-24},
	journal = {Journal of Physics: Conference Series},
	author = {Alzubi, Jafar and Nayyar, Anand and Kumar, Akshi},
	month = nov,
	year = {2018},
	pages = {012012},
	file = {Alzubi et al. - 2018 - Machine Learning from Theory to Algorithms An Ove.pdf:/Users/johannesreichle/Zotero/storage/LB5G9KJ7/Alzubi et al. - 2018 - Machine Learning from Theory to Algorithms An Ove.pdf:application/pdf},
}

@inproceedings{karatzas_icdar_2013,
	title = {{ICDAR} 2013 {Robust} {Reading} {Competition}},
	doi = {10.1109/ICDAR.2013.221},
	abstract = {This report presents the final results of the ICDAR 2013 Robust Reading Competition. The competition is structured in three Challenges addressing text extraction in different application domains, namely born-digital images, real scene images and real-scene videos. The Challenges are organised around specific tasks covering text localisation, text segmentation and word recognition. The competition took place in the first quarter of 2013, and received a total of 42 submissions over the different tasks offered. This report describes the datasets and ground truth specification, details the performance evaluation protocols used and presents the final results along with a brief summary of the participating methods.},
	booktitle = {2013 12th {International} {Conference} on {Document} {Analysis} and {Recognition}},
	author = {Karatzas, Dimosthenis and Shafait, Faisal and Uchida, Seiichi and Iwamura, Masakazu and Bigorda, Lluis Gomez i and Mestre, Sergi Robles and Mas, Joan and Mota, David Fernandez and Almazàn, Jon Almazàn and de las Heras, Lluís Pere},
	month = aug,
	year = {2013},
	note = {ISSN: 2379-2140},
	keywords = {Protocols, Text recognition, Robustness, Image segmentation, Performance evaluation, robust reading, scene text, text extraction, text localization, text recognition, text segmentation, video, Videos},
	pages = {1484--1493},
	file = {IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/AFPQUVHM/6628859.html:text/html;Karatzas et al_2013_ICDAR 2013 Robust Reading Competition.pdf:/Users/johannesreichle/Zotero/storage/7XLP3LY2/Karatzas et al_2013_ICDAR 2013 Robust Reading Competition.pdf:application/pdf},
}

@inproceedings{karatzas_icdar_2015,
	title = {{ICDAR} 2015 competition on {Robust} {Reading}},
	doi = {10.1109/ICDAR.2015.7333942},
	abstract = {Results of the ICDAR 2015 Robust Reading Competition are presented. A new Challenge 4 on Incidental Scene Text has been added to the Challenges on Born-Digital Images, Focused Scene Images and Video Text. Challenge 4 is run on a newly acquired dataset of 1,670 images evaluating Text Localisation, Word Recognition and End-to-End pipelines. In addition, the dataset for Challenge 3 on Video Text has been substantially updated with more video sequences and more accurate ground truth data. Finally, tasks assessing End-to-End system performance have been introduced to all Challenges. The competition took place in the first quarter of 2015, and received a total of 44 submissions. Only the tasks newly introduced in 2015 are reported on. The datasets, the ground truth specification and the evaluation protocols are presented together with the results and a brief summary of the participating methods.},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Karatzas, Dimosthenis and Gomez-Bigorda, Lluis and Nicolaou, Anguelos and Ghosh, Suman and Bagdanov, Andrew and Iwamura, Masakazu and Matas, Jiri and Neumann, Lukas and Chandrasekhar, Vijay Ramaseshan and Lu, Shijian and Shafait, Faisal and Uchida, Seiichi and Valveny, Ernest},
	month = aug,
	year = {2015},
	keywords = {IP networks, Yttrium},
	pages = {1156--1160},
	file = {IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/ZLUHKLPU/7333942.html:text/html;Karatzas et al_2015_ICDAR 2015 competition on Robust Reading.pdf:/Users/johannesreichle/Zotero/storage/2RGPKF65/Karatzas et al_2015_ICDAR 2015 competition on Robust Reading.pdf:application/pdf},
}

@article{long_unrealtext_2020,
	title = {{UnrealText}: {Synthesizing} {Realistic} {Scene} {Text} {Images} from the {Unreal} {World}},
	shorttitle = {{UnrealText}},
	url = {http://arxiv.org/abs/2003.10608},
	abstract = {Synthetic data has been a critical tool for training scene text detection and recognition models. On the one hand, synthetic word images have proven to be a successful substitute for real images in training scene text recognizers. On the other hand, however, scene text detectors still heavily rely on a large amount of manually annotated real-world images, which are expensive. In this paper, we introduce UnrealText, an efficient image synthesis method that renders realistic images via a 3D graphics engine. 3D synthetic engine provides realistic appearance by rendering scene and text as a whole, and allows for better text region proposals with access to precise scene information, e.g. normal and even object meshes. The comprehensive experiments verify its effectiveness on both scene text detection and recognition. We also generate a multilingual version for future research into multilingual scene text detection and recognition. Additionally, we re-annotate scene text recognition datasets in a case-sensitive way and include punctuation marks for more comprehensive evaluations. The code and the generated datasets are released at: https://github.com/Jyouhou/UnrealText/ .},
	urldate = {2022-01-24},
	journal = {arXiv:2003.10608 [cs]},
	author = {Long, Shangbang and Yao, Cong},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.10608},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: adding experiments with Mask-RCNN},
	file = {Long_Yao_2020_UnrealText.pdf:/Users/johannesreichle/Zotero/storage/ZV6L56IW/Long_Yao_2020_UnrealText.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/CAELYM4H/2003.html:text/html},
}

@article{chen_text_2021-1,
	title = {Text {Recognition} in the {Wild}: {A} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Text {Recognition} in the {Wild}},
	url = {https://dl.acm.org/doi/10.1145/3440756},
	doi = {10.1145/3440756},
	abstract = {The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition.},
	language = {en},
	number = {2},
	urldate = {2022-01-24},
	journal = {ACM Computing Surveys},
	author = {Chen, Xiaoxue and Jin, Lianwen and Zhu, Yuanzhi and Luo, Canjie and Wang, Tianwei},
	month = apr,
	year = {2021},
	pages = {1--35},
	file = {Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:/Users/johannesreichle/Zotero/storage/6D7YJ3DA/Chen et al. - 2021 - Text Recognition in the Wild A Survey.pdf:application/pdf},
}

@inproceedings{he_icpr2018_2018,
	title = {{ICPR2018} {Contest} on {Robust} {Reading} for {Multi}-{Type} {Web} {Images}},
	doi = {10.1109/ICPR.2018.8546143},
	abstract = {Electronic commerce has infiltrated every aspect of our daily lives, which offers great convenience for shopping, advertising, etc. Text in the web images is responsible to convey essential information for consumers. Algorithms that read text in these web images can facilitate applications of various types, such as goods surveillance, products classification, and intelligent retrieval or recommendation. Despite of various existing text reading tasks, this contest introduces a novel large-scale dataset named MTWI that contains 20,000 images, which is the first dataset that is mainly constructed by Chinese and English web text. Three tasks (web text recognition, web text detection, and end-to-end web text detection and recognition) were set up for encouraging more research on the web text reading problem. The contest was held from February 2, 2018 to May 26, 2018 with 289 valid submissions from 4,282 registered teams. Throughout this report, we describe the details of this new dataset, the purposes and definitions of the tasks, the evaluation protocols, and the summaries of the results.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {He, Mengchao and Liu, Yuliang and Yang, Zhibo and Zhang, Sheng and Luo, Canjie and Gao, Feiyu and Zheng, Qi and Wang, Yongpan and Zhang, Xin and Jin, Lianwen},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {Protocols, Text recognition, Training, Task analysis, Character recognition, Measurement, Training data},
	pages = {7--12},
	file = {He et al_2018_ICPR2018 Contest on Robust Reading for Multi-Type Web Images.pdf:/Users/johannesreichle/Zotero/storage/BWUMQI8E/He et al_2018_ICPR2018 Contest on Robust Reading for Multi-Type Web Images.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/IWXSK8A2/8546143.html:text/html},
}

@inproceedings{shi_icdar2017_2017,
	title = {{ICDAR2017} {Competition} on {Reading} {Chinese} {Text} in the {Wild} ({RCTW}-17)},
	volume = {01},
	doi = {10.1109/ICDAR.2017.233},
	abstract = {Chinese is the most widely used language in the world. Algorithms that read Chinese text in natural images facilitate applications of various kinds. Despite the large potential value, datasets and competitions in the past primarily focus on English, which bares very different characteristics than Chinese. This report introduces RCTW, a new competition that focuses on Chinese text reading. The competition features a large-scale dataset with over 12,000 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams. This report includes dataset description, task definitions, evaluation protocols, and results summaries and analysis. Through this competition, we call for more future research on the Chinese text reading problem.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Shi, Baoguang and Yao, Cong and Liao, Minghui and Yang, Mingkun and Xu, Pei and Cui, Linyan and Belongie, Serge and Lu, Shijian and Bai, Xiang},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Protocols, Text recognition, Training, Task analysis, Image segmentation, Measurement, Competition, Computer science, Dataset, Detection, Recognition, Text},
	pages = {1429--1434},
	file = {Shi et al_2017_ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17).pdf:/Users/johannesreichle/Zotero/storage/RDTS3GQX/Shi et al_2017_ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17).pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/SA6GPUWX/8270164.html:text/html},
}

@article{wolf_object_2006,
	title = {Object count/area graphs for the evaluation of object detection and segmentation algorithms},
	volume = {8},
	issn = {1433-2833},
	abstract = {Evaluation of object detection algorithms is a non-trivial task: a detection result is usually evaluated by comparing the bounding box of the detected object with the bounding box of the ground truth object. The commonly used precision and recall measures are computed from the overlap area of these two rectangles. However, these measures have several drawbacks: they don't give intuitive information about the proportion of the correctly detected objects and the number of false alarms, and they cannot be accumulated across multiple images without creating ambiguity in their interpretation. Furthermore, quantitative and qualitative evaluation is often mixed resulting in ambiguous measures. In this paper we propose a new approach which tackles these problems. The performance of a detection algorithm is illustrated intuitively by performance graphs which present object level precision and recall depending on constraints on detection quality. In order to compare different detection algorithms, a representative single performance value is computed from the graphs. The influence of the test database on the detection performance is illustrated by performance/generality graphs. The evaluation method can be applied to different types of object detection algorithms. It has been tested on different text detection algorithms, among which are the participants of the ICDAR 2003 text detection competition.},
	number = {4},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Wolf, Christian and Jolion, Jean-Michel},
	month = sep,
	year = {2006},
	keywords = {Object detection, Evaluation, Text detection},
	pages = {280--296},
	file = {Wolf_Jolion_2006_Object count-area graphs for the evaluation of object detection and.pdf:/Users/johannesreichle/Zotero/storage/9X9TR2NA/Wolf_Jolion_2006_Object count-area graphs for the evaluation of object detection and.pdf:application/pdf},
}

@inproceedings{sun_icdar_2019,
	title = {{ICDAR} 2019 {Competition} on {Large}-{Scale} {Street} {View} {Text} with {Partial} {Labeling} - {RRC}-{LSVT}},
	doi = {10.1109/ICDAR.2019.00250},
	abstract = {Robust text reading from street view images provides valuable information for various applications. Performance improvement of existing methods in such a challenging scenario heavily relies on the amount of fully annotated training data, which is costly and in-efficient to obtain. To scale up the amount of training data while keeping the labeling procedure cost-effective, this competition introduces a new challenge on Large-scale Street View Text with Partial Labeling (LSVT), providing 5,0000 and 400,000 images in full and weak annotations, respectively. This competition aims to explore the abilities of state-of-the-art methods to detect and recognize text instances from large-scale street view images, closing gaps between research benchmarks and real applications. During the competition period, a total number of 41 teams participate in the two tasks with 132 valid submissions, i.e., text detection and end-to-end text spotting. This paper includes dataset descriptions, task definitions, evaluation protocols and results summaries of ICDAR 2019-LSVT challenge.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Sun, Yipeng and Ni, Zihan and Chng, Chee-Kheng and Liu, Yuliang and Luo, Canjie and Ng, Chun Chet and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and Chan, Chee Seng and Jin, Lianwen},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Text recognition, Training, Task analysis, Training data, Benchmark testing, end-to-end text spotting, Labeling, large-scale street view text, text detection, weak annotations},
	pages = {1557--1562},
	file = {Sun et al_2019_ICDAR 2019 Competition on Large-Scale Street View Text with Partial Labeling -.pdf:/Users/johannesreichle/Zotero/storage/UWYIFUBA/Sun et al_2019_ICDAR 2019 Competition on Large-Scale Street View Text with Partial Labeling -.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/7HIGTB5B/8978143.html:text/html},
}

@misc{noauthor_iiit_nodate,
	title = {The {IIIT} {5K}-word dataset},
	url = {http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset},
	urldate = {2022-01-26},
	file = {The IIIT 5K-word dataset:/Users/johannesreichle/Zotero/storage/48I5YH7S/the-iiit-5k-word-dataset.html:text/html},
}

@misc{chan_total-text-dataset_2022,
	title = {Total-{Text}-{Dataset} ({Official} site)},
	copyright = {BSD-3-Clause},
	url = {https://github.com/cs-chan/Total-Text-Dataset},
	abstract = {Total Text Dataset. It consists of 1555 images with more than 3 different text orientations: Horizontal, Multi-Oriented, and Curved, one of a kind.},
	urldate = {2022-01-26},
	author = {Chan, Chee Seng},
	month = jan,
	year = {2022},
	note = {original-date: 2017-08-24T18:40:04Z},
}

@inproceedings{chng_total-text_2017,
	title = {Total-{Text}: {A} {Comprehensive} {Dataset} for {Scene} {Text} {Detection} and {Recognition}},
	volume = {01},
	shorttitle = {Total-{Text}},
	doi = {10.1109/ICDAR.2017.157},
	abstract = {Text in curve orientation, despite being one of the common text orientations in real world environment, has close to zero existence in well received scene text datasets such as ICDAR'13 and MSRA-TD500. The main motivation of Total-Text is to fill this gap and facilitate a new research direction for the scene text community. On top of conventional horizontal and multi-oriented text, it features curved-oriented text. Total-Text is highly diversified in orientations, more than half of its images have a combination of more than two orientations. Recently, a new breed of solutions that casted text detection as a segmentation problem has demonstrated their effectiveness against multi-oriented text. In order to evaluate its robustness against curved text, we fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with its annotation is available at https://github.com/cs-chan/Total-Text-Dataset.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Ch'ng, Chee Kheng and Chan, Chee Seng},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Algorithm design and analysis, Text recognition, Feature extraction, Robustness, Image segmentation, Image recognition, Curve-oriented text, Image color analysis, Scene text dataset, Segmentation-based text detection},
	pages = {935--942},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/9CSRS2HW/Ch'ng and Chan - 2017 - Total-Text A Comprehensive Dataset for Scene Text.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/36JBVGN5/8270088.html:text/html},
}

@inproceedings{nayef_icdar2019_2019,
	title = {{ICDAR2019} {Robust} {Reading} {Challenge} on {Multi}-lingual {Scene} {Text} {Detection} and {Recognition} — {RRC}-{MLT}-2019},
	doi = {10.1109/ICDAR.2019.00254},
	abstract = {With the growing cosmopolitan culture of modern cities, the need of robust Multi-Lingual scene Text (MLT) detection and recognition systems has never been more immense. With the goal to systematically benchmark and push the state-of-the-art forward, the proposed competition builds on top of the RRC-MLT-2017 with an additional end-to-end task, an additional language in the real images dataset, a large scale multi-lingual synthetic dataset to assist the training, and a baseline End-to-End recognition method. The real dataset consists of 20,000 images containing text from 10 languages. The challenge has 4 tasks covering various aspects of multi-lingual scene text: (a) text detection, (b) cropped word script classification, (c) joint text detection and script classification and (d) end-to-end detection and recognition. In total, the competition received 60 submissions from the research and industrial communities. This paper presents the dataset, the tasks and the findings of the presented RRC-MLT-2019 challenge.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Nayef, Nibal and Patel, Yash and Busta, Michal and Chowdhury, Pinaki Nath and Karatzas, Dimosthenis and Khlif, Wafa and Matas, Jiri and Pal, Umapada and Burie, Jean-Christophe and Liu, Cheng-lin and Ogier, Jean-Marc},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Protocols, Text recognition, Training, Task analysis, Benchmark testing, Rendering (computer graphics), Scene Text Detection, Multi-lingual Text, Script Identification, End-to-End Text Recognition},
	pages = {1582--1587},
	file = {IEEE Xplore Full Text PDF:/Users/johannesreichle/Zotero/storage/58U49WYB/Nayef et al. - 2019 - ICDAR2019 Robust Reading Challenge on Multi-lingua.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/7YNBWG5P/8978096.html:text/html},
}

@misc{liu_scut-ctw1500_2022,
	title = {{SCUT}-{CTW1500} {Datasets}},
	url = {https://github.com/Yuliang-Liu/Curve-Text-Detector},
	abstract = {This repository provides train＆test code, dataset, det.\&rec. annotation, evaluation script, annotation tool, and ranking.},
	urldate = {2022-01-26},
	author = {Liu, Yuliang},
	month = jan,
	year = {2022},
	note = {original-date: 2017-11-30T15:30:08Z},
	keywords = {deep-learning, document-analysis, object-detection, scene-text},
}

@article{yuliang_detecting_2017,
	title = {Detecting {Curve} {Text} in the {Wild}: {New} {Dataset} and {New} {Solution}},
	shorttitle = {Detecting {Curve} {Text} in the {Wild}},
	url = {http://arxiv.org/abs/1712.02170},
	abstract = {Scene text detection has been made great progress in recent years. The detection manners are evolving from axis-aligned rectangle to rotated rectangle and further to quadrangle. However, current datasets contain very little curve text, which can be widely observed in scene images such as signboard, product name and so on. To raise the concerns of reading curve text in the wild, in this paper, we construct a curve text dataset named CTW1500, which includes over 10k text annotations in 1,500 images (1000 for training and 500 for testing). Based on this dataset, we pioneering propose a polygon based curve text detector (CTD) which can directly detect curve text without empirical combination. Moreover, by seamlessly integrating the recurrent transverse and longitudinal offset connection (TLOC), the proposed method can be end-to-end trainable to learn the inherent connection among the position offsets. This allows the CTD to explore context information instead of predicting points independently, resulting in more smooth and accurate detection. We also propose two simple but effective post-processing methods named non-polygon suppress (NPS) and polygonal non-maximum suppression (PNMS) to further improve the detection accuracy. Furthermore, the proposed approach in this paper is designed in an universal manner, which can also be trained with rectangular or quadrilateral bounding boxes without extra efforts. Experimental results on CTW-1500 demonstrate our method with only a light backbone can outperform state-of-the-art methods with a large margin. By evaluating only in the curve or non-curve subset, the CTD + TLOC can still achieve the best results. Code is available at https://github.com/Yuliang-Liu/Curve-Text-Detector.},
	urldate = {2022-01-26},
	journal = {arXiv:1712.02170 [cs]},
	author = {Yuliang, Liu and Lianwen, Jin and Shuaitao, Zhang and Sheng, Zhang},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.02170
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages},
	file = {arXiv Fulltext PDF:/Users/johannesreichle/Zotero/storage/NEGCSST6/Yuliang et al. - 2017 - Detecting Curve Text in the Wild New Dataset and .pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/D3B636US/1712.html:text/html},
}

@inproceedings{mishra_scene_2012,
	address = {Surrey},
	title = {Scene {Text} {Recognition} using {Higher} {Order} {Language} {Priors}},
	isbn = {978-1-901725-46-9},
	url = {http://www.bmva.org/bmvc/2012/BMVC/paper127/index.html},
	doi = {10.5244/C.26.127},
	abstract = {The problem of recognizing text in images taken in the wild has gained signiﬁcant attention from the computer vision community in recent years. Contrary to recognition of printed documents, recognizing scene text is a challenging problem. We focus on the problem of recognizing text extracted from natural scene images and the web. Signiﬁcant attempts have been made to address this problem in the recent past. However, many of these works beneﬁt from the availability of strong context, which naturally limits their applicability. In this work we present a framework that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary. We show experimental results on publicly available datasets. Furthermore, we introduce a large challenging word dataset with ﬁve thousand words to evaluate various steps of our method exhaustively.},
	language = {en},
	urldate = {2022-01-26},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2012},
	publisher = {British Machine Vision Association},
	author = {Mishra, Anand and Alahari, Karteek and Jawahar, Cv},
	year = {2012},
	pages = {127.1--127.11},
	file = {Mishra et al. - 2012 - Scene Text Recognition using Higher Order Language.pdf:/Users/johannesreichle/Zotero/storage/U44FZ8MT/Mishra et al. - 2012 - Scene Text Recognition using Higher Order Language.pdf:application/pdf},
}

@article{liao_mask_2020,
	title = {Mask {TextSpotter} v3: {Segmentation} {Proposal} {Network} for {Robust} {Scene} {Text} {Spotting}},
	shorttitle = {Mask {TextSpotter} v3},
	url = {http://arxiv.org/abs/2007.09482},
	abstract = {Recent end-to-end trainable methods for scene text spotting, integrating detection and recognition, showed much progress. However, most of the current arbitrary-shape scene text spotters use region proposal networks (RPN) to produce proposals. RPN relies heavily on manually designed anchors and its proposals are represented with axis-aligned rectangles. The former presents difficulties in handling text instances of extreme aspect ratios or irregular shapes, and the latter often includes multiple neighboring instances into a single proposal, in cases of densely oriented text. To tackle these problems, we propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that adopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is anchor-free and gives accurate representations of arbitrary-shape proposals. It is therefore superior to RPN in detecting text instances of extreme aspect ratios or irregular shapes. Furthermore, the accurate proposals produced by SPN allow masked RoI features to be used for decoupling neighboring text instances. As a result, our Mask TextSpotter v3 can handle text instances of extreme aspect ratios or irregular shapes, and its recognition accuracy won't be affected by nearby text or background noise. Specifically, we outperform state-of-the-art methods by 21.9 percent on the Rotated ICDAR 2013 dataset (rotation robustness), 5.9 percent on the Total-Text dataset (shape robustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset (aspect ratio robustness). Code is available at: https://github.com/MhLiao/MaskTextSpotterV3},
	urldate = {2022-01-26},
	journal = {arXiv:2007.09482 [cs]},
	author = {Liao, Minghui and Pang, Guan and Huang, Jing and Hassner, Tal and Bai, Xiang},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.09482},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by ECCV 2020},
	file = {Liao et al_2020_Mask TextSpotter v3.pdf:/Users/johannesreichle/Zotero/storage/IX89Y52K/Liao et al_2020_Mask TextSpotter v3.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/8SQWB2AZ/2007.html:text/html},
}

@inproceedings{liu_accurate_2020,
	address = {Seattle, WA, USA},
	title = {An {Accurate} {Segmentation}-{Based} {Scene} {Text} {Detector} with {Context} {Attention} and {Repulsive} {Text} {Border}},
	isbn = {978-1-72819-360-1},
	url = {https://ieeexplore.ieee.org/document/9151062/},
	doi = {10.1109/CVPRW50498.2020.00283},
	abstract = {Scene text detection is one of the most challenging problems in computer vision and has attracted great interest. In general, scene text detection methods are divided into two categories: detection-based and segmentation-based methods. Recently, the segmentationbased methods are more and more popular due to their superior performances and the advantages of detecting arbitrary-shape texts. However, there still exist the following problems: (a) the misclassification of the unexpected texts, (b) the split of long text lines, (c) the failure of separating very close text instances. In this paper, we propose an accurate segmentation-based detector, which is equipped with context attention and repulsive text border. The context attention incorporates global channel attention, non-local self-attention and spatial attention to better exploit the global and local context, which can greatly increase the discriminative ability for pixels. Due to the enhancement of pixel-level features, false positives and the misdetections of long texts are reduced. Besides, for the purpose of solving very close text instance, a repulsive pixel link, which focuses on the relationships between pixels at the border, is proposed. Experiments on several standard benchmarks, including MSRA-TD500, ICDAR2015, ICDAR2017-MLT and CTW1500, validate the superiority of the proposed method.},
	language = {en},
	urldate = {2022-01-26},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Liu, Xi and Zhou, Gaojing and Zhang, Rui and Wei, Xiaolin},
	month = jun,
	year = {2020},
	pages = {2344--2352},
	file = {Liu et al. - 2020 - An Accurate Segmentation-Based Scene Text Detector.pdf:/Users/johannesreichle/Zotero/storage/FQKMUNYD/Liu et al. - 2020 - An Accurate Segmentation-Based Scene Text Detector.pdf:application/pdf},
}

@misc{noauthor_msra_nodate,
	title = {{MSRA} {Text} {Detection} 500 {Database} ({MSRA}-{TD500}) - {TC11}},
	url = {http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500)},
	urldate = {2022-01-27},
	file = {MSRA Text Detection 500 Database (MSRA-TD500) - TC11:/Users/johannesreichle/Zotero/storage/GLTPD4MI/MSRA_Text_Detection_500_Database_(MSRA-TD500).html:text/html},
}

@inproceedings{cong_yao_detecting_2012,
	address = {Providence, RI},
	title = {Detecting texts of arbitrary orientations in natural images},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6247787/},
	doi = {10.1109/CVPR.2012.6247787},
	abstract = {With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classiﬁcation scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves signiﬁcantly enhanced performance on texts of arbitrary orientations in complex natural scenes.},
	language = {en},
	urldate = {2022-01-27},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Cong Yao} and {Xiang Bai} and {Wenyu Liu} and {Yi Ma} and {Zhuowen Tu}},
	month = jun,
	year = {2012},
	pages = {1083--1090},
	file = {Cong Yao et al. - 2012 - Detecting texts of arbitrary orientations in natur.pdf:/Users/johannesreichle/Zotero/storage/N6M58UM3/Cong Yao et al. - 2012 - Detecting texts of arbitrary orientations in natur.pdf:application/pdf},
}

@misc{zdenek_east_2021,
	title = {{EAST}: {An} {Efficient} and {Accurate} {Scene} {Text} {Detector}},
	copyright = {GPL-3.0},
	shorttitle = {{EAST}},
	url = {https://github.com/janzd/EAST/blob/ba6c1ccae776935dc96e251c4a2bf790a49e6c0f/README.md},
	abstract = {Implementation of EAST scene text detector in Keras},
	urldate = {2022-01-27},
	author = {Zdenek, Jan},
	month = dec,
	year = {2021},
	note = {original-date: 2018-07-04T10:07:30Z},
}

@inproceedings{nayef_icdar2017_2017,
	title = {{ICDAR2017} {Robust} {Reading} {Challenge} on {Multi}-{Lingual} {Scene} {Text} {Detection} and {Script} {Identification} - {RRC}-{MLT}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.237},
	abstract = {Text detection and recognition in a natural environment are key components of many applications, ranging from business card digitization to shop indexation in a street. This competition aims at assessing the ability of state-of-the-art methods to detect Multi-Lingual Text (MLT) in scene images, such as in contents gathered from the Internet media and in modern cities where multiple cultures live and communicate together. This competition is an extension of the Robust Reading Competition (RRC) which has been held since 2003 both in ICDAR and in an online context. The proposed competition is presented as a new challenge of the RRC. The dataset built for this challenge largely extends the previous RRC editions in many aspects: the multi-lingual text, the size of the dataset, the multi-oriented text, the wide variety of scenes. The dataset is comprised of 18,000 images which contain text belonging to 9 languages. The challenge is comprised of three tasks related to text detection and script classification. We have received a total of 16 participations from the research and industrial communities. This paper presents the dataset, the tasks and the findings of this RRC-MLT challenge.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Nayef, Nibal and Yin, Fei and Bizid, Imen and Choi, Hyunsoo and Feng, Yuan and Karatzas, Dimosthenis and Luo, Zhenbo and Pal, Umapada and Rigaud, Christophe and Chazalon, Joseph and Khlif, Wafa and Luqman, Muhammad Muzzamil and Burie, Jean-Christophe and Liu, Cheng-lin and Ogier, Jean-Marc},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Text recognition, Training, Task analysis, Proposals, Robustness, Internet, Multi-lingual Text, Scene Text Detection, Script Identification},
	pages = {1454--1459},
	file = {Nayef et al_2017_ICDAR2017 Robust Reading Challenge on Multi-Lingual Scene Text Detection and.pdf:/Users/johannesreichle/Zotero/storage/H3MWHN8H/Nayef et al_2017_ICDAR2017 Robust Reading Challenge on Multi-Lingual Scene Text Detection and.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/V3IRJ44E/8270168.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {Conference Name: Neural Computation},
	pages = {1735--1780},
	file = {Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:/Users/johannesreichle/Zotero/storage/VPLXUVMT/Hochreiter and Schmidhuber - FORSCHUNGSBERICHTE KiJNSTLICHE INTELLIGENZ.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/QY9HG4JJ/6795963.html:text/html},
}

@article{sherstinsky_fundamentals_2020,
	title = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) {Network}},
	volume = {404},
	issn = {01672789},
	url = {http://arxiv.org/abs/1808.03314},
	doi = {10.1016/j.physd.2019.132306},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
	urldate = {2022-01-29},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	month = mar,
	year = {2020},
	note = {arXiv: 1808.03314},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {132306},
	annote = {Comment: 43 pages, 10 figures, 78 references},
	file = {Sherstinsky_2020_Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory.pdf:/Users/johannesreichle/Zotero/storage/KRG62QW8/Sherstinsky_2020_Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/VEYYUT2A/1808.html:text/html},
}

@article{greff_lstm_2017,
	title = {{LSTM}: {A} {Search} {Space} {Odyssey}},
	volume = {28},
	issn = {2162-237X, 2162-2388},
	shorttitle = {{LSTM}},
	url = {http://arxiv.org/abs/1503.04069},
	doi = {10.1109/TNNLS.2016.2582924},
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\${\textbackslash}approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	number = {10},
	urldate = {2022-01-30},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
	month = oct,
	year = {2017},
	note = {arXiv: 1503.04069},
	keywords = {Computer Science - Machine Learning, 68T10, Computer Science - Neural and Evolutionary Computing, H.5.5, I.2.6, I.2.7, I.5.1},
	pages = {2222--2232},
	annote = {Comment: 12 pages, 6 figures},
	file = {Greff et al_2017_LSTM.pdf:/Users/johannesreichle/Zotero/storage/PU4CKAJH/Greff et al_2017_LSTM.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/DB9M95WK/1503.html:text/html},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-01-30},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al_2015_Deep Residual Learning for Image Recognition.pdf:/Users/johannesreichle/Zotero/storage/BLSKKILW/He et al_2015_Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/WNWW7EKV/1512.html:text/html},
}

@article{gers_learning_1999,
	title = {Learning to {Forget}: {Continual} {Prediction} with {LSTM}},
	volume = {12},
	shorttitle = {Learning to {Forget}},
	abstract = {Long Short-Term Memory (LSTM, Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not  a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way. 1 Introduction  Recurrent neural networks (RNNs) constitute a very powerful class of computational models, capable of ...},
	journal = {Neural Computation},
	author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred},
	year = {1999},
	pages = {2451--2471},
	file = {Citeseer - Snapshot:/Users/johannesreichle/Zotero/storage/3L4JT5Z2/download.html:text/html;Gers et al_1999_Learning to Forget.pdf:/Users/johannesreichle/Zotero/storage/84MHX8E6/Gers et al_1999_Learning to Forget.pdf:application/pdf},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-01-31},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf:/Users/johannesreichle/Zotero/storage/VMXVYBDV/Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/9LY9AEVU/2010.html:text/html},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2022-01-31},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al_2014_GloVe.pdf:/Users/johannesreichle/Zotero/storage/WXRUMS39/Pennington et al_2014_GloVe.pdf:application/pdf},
}

@article{yu_review_2019,
	title = {A {Review} of {Recurrent} {Neural} {Networks}: {LSTM} {Cells} and {Network} {Architectures}},
	volume = {31},
	issn = {0899-7667, 1530-888X},
	shorttitle = {A {Review} of {Recurrent} {Neural} {Networks}},
	url = {https://direct.mit.edu/neco/article/31/7/1235-1270/8500},
	doi = {10.1162/neco_a_01199},
	abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
	language = {en},
	number = {7},
	urldate = {2022-01-31},
	journal = {Neural Computation},
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
	month = jul,
	year = {2019},
	pages = {1235--1270},
	file = {Yu et al. - 2019 - A Review of Recurrent Neural Networks LSTM Cells .pdf:/Users/johannesreichle/Zotero/storage/8DIVIDFN/Yu et al. - 2019 - A Review of Recurrent Neural Networks LSTM Cells .pdf:application/pdf},
}

@article{liu_rethinking_2021,
	title = {Rethinking {Skip} {Connection} with {Layer} {Normalization} in {Transformers} and {ResNets}},
	url = {http://arxiv.org/abs/2105.07205},
	abstract = {Skip connection, is a widely-used technique to improve the performance and the convergence of deep neural networks, which is believed to relieve the difficulty in optimization due to non-linearity by propagating a linear component through the neural network layers. However, from another point of view, it can also be seen as a modulating mechanism between the input and the output, with the input scaled by a pre-defined value one. In this work, we investigate how the scale factors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale will lead to spurious gradient exploding or vanishing in line with the deepness of the models, which could be addressed by normalization, in particular, layer normalization, which induces consistent improvements over the plain skip connection. Inspired by the findings, we further propose to adaptively adjust the scale of the input by recursively applying skip connection with layer normalization, which promotes the performance substantially and generalizes well across diverse tasks including both machine translation and image classification datasets.},
	urldate = {2022-02-01},
	journal = {arXiv:2105.07205 [cs]},
	author = {Liu, Fenglin and Ren, Xuancheng and Zhang, Zhiyuan and Sun, Xu and Zou, Yuexian},
	month = may,
	year = {2021},
	note = {arXiv: 2105.07205},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by COLING2020 (The 28th International Conference on Computational Linguistics (COLING 2020))},
	file = {Liu et al_2021_Rethinking Skip Connection with Layer Normalization in Transformers and ResNets.pdf:/Users/johannesreichle/Zotero/storage/P3YPNDDH/Liu et al_2021_Rethinking Skip Connection with Layer Normalization in Transformers and ResNets.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/9FYIUS35/2105.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-02-01},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al_2017_Attention Is All You Need.pdf:/Users/johannesreichle/Zotero/storage/KDUA8FGZ/Vaswani et al_2017_Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/MGMBEUVQ/1706.html:text/html},
}

@article{yin_scene_2017,
	title = {Scene {Text} {Recognition} with {Sliding} {Convolutional} {Character} {Models}},
	url = {http://arxiv.org/abs/1709.01727},
	abstract = {Scene text recognition has attracted great interests from the computer vision and pattern recognition community in recent years. State-of-the-art methods use concolutional neural networks (CNNs), recurrent neural networks with long short-term memory (RNN-LSTM) or the combination of them. In this paper, we investigate the intrinsic characteristics of text recognition, and inspired by human cognition mechanisms in reading texts, we propose a scene text recognition method with character models on convolutional feature map. The method simultaneously detects and recognizes characters by sliding the text line image with character models, which are learned end-to-end on text line images labeled with text transcripts. The character classifier outputs on the sliding windows are normalized and decoded with Connectionist Temporal Classification (CTC) based algorithm. Compared to previous methods, our method has a number of appealing properties: (1) It avoids the difficulty of character segmentation which hinders the performance of segmentation-based recognition methods; (2) The model can be trained simply and efficiently because it avoids gradient vanishing/exploding in training RNN-LSTM based models; (3) It bases on character models trained free of lexicon, and can recognize unknown words. (4) The recognition process is highly parallel and enables fast recognition. Our experiments on several challenging English and Chinese benchmarks, including the IIIT-5K, SVT, ICDAR03/13 and TRW15 datasets, demonstrate that the proposed method yields superior or comparable performance to state-of-the-art methods while the model size is relatively small.},
	urldate = {2022-02-01},
	journal = {arXiv:1709.01727 [cs]},
	author = {Yin, Fei and Wu, Yi-Chao and Zhang, Xu-Yao and Liu, Cheng-Lin},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01727},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages,4 figures},
	file = {Yin et al_2017_Scene Text Recognition with Sliding Convolutional Character Models.pdf:/Users/johannesreichle/Zotero/storage/5WHRELQ7/Yin et al_2017_Scene Text Recognition with Sliding Convolutional Character Models.pdf:application/pdf},
}

@inproceedings{liao_textboxes_2017,
	title = {{TextBoxes}: {A} {Fast} {Text} {Detector} with a {Single} {Deep} {Neural} {Network}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	shorttitle = {{TextBoxes}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14202},
	abstract = {This paper presents an end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efficiency in a single network forward pass, involving no post-process except for a standard non-maximum suppression. TextBoxes outperforms competing methods in terms of text localization accuracy and is much faster, taking only 0.09s per image in a fast implementation. Furthermore, combined with a text recognizer, TextBoxes significantly outperforms state-of-the-art approaches on word spotting and end-to-end text recognition tasks.},
	language = {en},
	urldate = {2022-02-04},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liao, Minghui and Shi, Baoguang and Bai, Xiang and Wang, Xinggang and Liu, Wenyu},
	month = feb,
	year = {2017},
	file = {Liao et al_2017_TextBoxes.pdf:/Users/johannesreichle/Zotero/storage/GN7XKTSE/Liao et al_2017_TextBoxes.pdf:application/pdf;Snapshot:/Users/johannesreichle/Zotero/storage/SUGZZF7A/14202.html:text/html},
}

@article{jiang_r2cnn_2017,
	title = {{R2CNN}: {Rotational} {Region} {CNN} for {Orientation} {Robust} {Scene} {Text} {Detection}},
	shorttitle = {{R2CNN}},
	url = {http://arxiv.org/abs/1706.09579},
	abstract = {In this paper, we propose a novel method called Rotational Region CNN (R2CNN) for detecting arbitrary-oriented texts in natural scene images. The framework is based on Faster R-CNN [1] architecture. First, we use the Region Proposal Network (RPN) to generate axis-aligned bounding boxes that enclose the texts with different orientations. Second, for each axis-aligned text box proposed by RPN, we extract its pooled features with different pooled sizes and the concatenated features are used to simultaneously predict the text/non-text score, axis-aligned box and inclined minimum area box. At last, we use an inclined non-maximum suppression to get the detection results. Our approach achieves competitive results on text detection benchmarks: ICDAR 2015 and ICDAR 2013.},
	urldate = {2022-02-04},
	journal = {arXiv:1706.09579 [cs]},
	author = {Jiang, Yingying and Zhu, Xiangyu and Wang, Xiaobing and Yang, Shuli and Li, Wei and Wang, Hua and Fu, Pei and Luo, Zhenbo},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09579},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages, 6 figures, 3 tables},
	file = {Jiang et al_2017_R2CNN.pdf:/Users/johannesreichle/Zotero/storage/T9YBJAUQ/Jiang et al_2017_R2CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HYTCD2UN/1706.html:text/html},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2022-02-04},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:/Users/johannesreichle/Zotero/storage/HDWMDF9V/Girshick et al_2014_Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/9IFY8NQX/1311.html:text/html},
}

@inproceedings{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	urldate = {2022-02-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Ren et al_2015_Faster R-CNN.pdf:/Users/johannesreichle/Zotero/storage/Q6VIP2R5/Ren et al_2015_Faster R-CNN.pdf:application/pdf},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	urldate = {2022-02-04},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
	file = {Redmon et al_2016_You Only Look Once.pdf:/Users/johannesreichle/Zotero/storage/XNGEMZG9/Redmon et al_2016_You Only Look Once.pdf:application/pdf;Snapshot:/Users/johannesreichle/Zotero/storage/P26GHBPB/Redmon_You_Only_Look_CVPR_2016_paper.html:text/html},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2022-02-04},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: open source; appendix on more results},
	file = {He et al_2018_Mask R-CNN.pdf:/Users/johannesreichle/Zotero/storage/C33RGZWY/He et al_2018_Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/737DGUVP/1703.html:text/html},
}

@inproceedings{hosang_learning_2017,
	address = {Honolulu, HI},
	title = {Learning {Non}-maximum {Suppression}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100168/},
	doi = {10.1109/CVPR.2017.685},
	abstract = {Object detectors have hugely proﬁted from moving towards an end-to-end learning paradigm: proposals, features, and the classiﬁer becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and — being based on greedy clustering with a ﬁxed distance threshold — forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
	language = {en},
	urldate = {2022-02-05},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
	month = jul,
	year = {2017},
	pages = {6469--6477},
	file = {Hosang et al. - 2017 - Learning Non-maximum Suppression.pdf:/Users/johannesreichle/Zotero/storage/7MDCKZYF/Hosang et al. - 2017 - Learning Non-maximum Suppression.pdf:application/pdf},
}

@inproceedings{tian_detecting_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Detecting {Text} in {Natural} {Image} with {Connectionist} {Text} {Proposal} {Network}},
	isbn = {978-3-319-46484-8},
	doi = {10.1007/978-3-319-46484-8_4},
	abstract = {We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8, 35] by a large margin. The CTPN is computationally efficient with 0.14 s/image, by using the very deep VGG16 model [27]. Online demo is available: http://textdet.com/.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Tian, Zhi and Huang, Weilin and He, Tong and He, Pan and Qiao, Yu},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Scene text detection, Anchor mechanism, Convolutional network, Recurrent neural network},
	pages = {56--72},
	file = {Tian et al_2016_Detecting Text in Natural Image with Connectionist Text Proposal Network.pdf:/Users/johannesreichle/Zotero/storage/5VHLCVYL/Tian et al_2016_Detecting Text in Natural Image with Connectionist Text Proposal Network.pdf:application/pdf},
}

@incollection{ferrari_textsnake_2018,
	address = {Cham},
	title = {{TextSnake}: {A} {Flexible} {Representation} for {Detecting} {Text} of {Arbitrary} {Shapes}},
	volume = {11206},
	isbn = {978-3-030-01215-1 978-3-030-01216-8},
	shorttitle = {{TextSnake}},
	url = {http://link.springer.com/10.1007/978-3-030-01216-8_2},
	abstract = {Driven by deep neural networks and large scale datasets, scene text detection methods have progressed substantially over the past years, continuously refreshing the performance records on various standard benchmarks. However, limited by the representations (axis-aligned rectangles, rotated rectangles or quadrangles) adopted to describe text, existing methods may fall short when dealing with much more free-form text instances, such as curved text, which are actually very common in real-world scenarios. To tackle this problem, we propose a more ﬂexible representation for scene text, termed as TextSnake, which is able to eﬀectively represent text instances in horizontal, oriented and curved forms. In TextSnake, a text instance is described as a sequence of ordered, overlapping disks centered at symmetric axes, each of which is associated with potentially variable radius and orientation. Such geometry attributes are estimated via a Fully Convolutional Network (FCN) model. In experiments, the text detector based on TextSnake achieves state-of-the-art or comparable performance on Total-Text and SCUTCTW1500, the two newly published benchmarks with special emphasis on curved text in natural images, as well as the widely-used datasets ICDAR 2015 and MSRA-TD500. Speciﬁcally, TextSnake outperforms the baseline on Total-Text by more than 40\% in F-measure.},
	language = {en},
	urldate = {2022-02-06},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Long, Shangbang and Ruan, Jiaqiang and Zhang, Wenjie and He, Xin and Wu, Wenhao and Yao, Cong},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01216-8_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {19--35},
	file = {Long et al. - 2018 - TextSnake A Flexible Representation for Detecting.pdf:/Users/johannesreichle/Zotero/storage/NFLF3DJS/Long et al. - 2018 - TextSnake A Flexible Representation for Detecting.pdf:application/pdf},
}

@article{deng_pixellink_2018,
	title = {{PixelLink}: {Detecting} {Scene} {Text} via {Instance} {Segmentation}},
	shorttitle = {{PixelLink}},
	url = {http://arxiv.org/abs/1801.01315},
	abstract = {Most state-of-the-art scene text detection algorithms are deep learning based methods that depend on bounding box regression and perform at least two kinds of predictions: text/non-text classification and location regression. Regression plays a key role in the acquisition of bounding boxes in these methods, but it is not indispensable because text/non-text prediction can also be considered as a kind of semantic segmentation that contains full location information in itself. However, text instances in scene images often lie very close to each other, making them very difficult to separate via semantic segmentation. Therefore, instance segmentation is needed to address this problem. In this paper, PixelLink, a novel scene text detection algorithm based on instance segmentation, is proposed. Text instances are first segmented out by linking pixels within the same instance together. Text bounding boxes are then extracted directly from the segmentation result without location regression. Experiments show that, compared with regression-based methods, PixelLink can achieve better or comparable performance on several benchmarks, while requiring many fewer training iterations and less training data.},
	urldate = {2022-02-06},
	journal = {arXiv:1801.01315 [cs]},
	author = {Deng, Dan and Liu, Haifeng and Li, Xuelong and Cai, Deng},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.01315},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: AAAI-2018},
	file = {Deng et al_2018_PixelLink.pdf:/Users/johannesreichle/Zotero/storage/5ZKB7JJK/Deng et al_2018_PixelLink.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/UV6UUVEN/1801.html:text/html},
}

@article{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	urldate = {2022-02-06},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = mar,
	year = {2015},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: to appear in CVPR (2015)},
	file = {Long et al_2015_Fully Convolutional Networks for Semantic Segmentation.pdf:/Users/johannesreichle/Zotero/storage/KBPTUAU8/Long et al_2015_Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/335QJC58/1411.html:text/html},
}

@inproceedings{noh_learning_2015,
	address = {Santiago, Chile},
	title = {Learning {Deconvolution} {Network} for {Semantic} {Segmentation}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410535/},
	doi = {10.1109/ICCV.2015.178},
	abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the ﬁnal semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identiﬁes detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
	language = {en},
	urldate = {2022-02-07},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
	month = dec,
	year = {2015},
	pages = {1520--1528},
	file = {Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf:/Users/johannesreichle/Zotero/storage/RXBCESAA/Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf:application/pdf},
}

@inproceedings{shi_detecting_2017,
	address = {Honolulu, HI},
	title = {Detecting {Oriented} {Text} in {Natural} {Images} by {Linking} {Segments}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099854/},
	doi = {10.1109/CVPR.2017.371},
	abstract = {Most state-of-the-art text detection methods are speciﬁc to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0\% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512×512 images. Moreover, without modiﬁcation, SegLink is able to detect long lines of non-Latin text, such as Chinese.},
	language = {en},
	urldate = {2022-02-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Baoguang and Bai, Xiang and Belongie, Serge},
	month = jul,
	year = {2017},
	pages = {3482--3490},
	file = {Shi et al. - 2017 - Detecting Oriented Text in Natural Images by Linki.pdf:/Users/johannesreichle/Zotero/storage/LQ3RXBFB/Shi et al. - 2017 - Detecting Oriented Text in Natural Images by Linki.pdf:application/pdf},
}

@article{shi_end--end_2017,
	title = {An {End}-to-{End} {Trainable} {Neural} {Network} for {Image}-{Based} {Sequence} {Recognition} and {Its} {Application} to {Scene} {Text} {Recognition}},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2646371},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for realworld application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Neural networks, Text recognition, Feature extraction, Image recognition, Context, Convolutional codes, convolutional neural network, Logic gates, long-short term memory, neural network, optical music recognition, scene text recognition, Sequence recognition},
	pages = {2298--2304},
	file = {Shi et al_2017_An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and.pdf:/Users/johannesreichle/Zotero/storage/WPMIWQVQ/Shi et al_2017_An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/F6KSSVNK/7801919.html:text/html},
}

@inproceedings{ghosh_visual_2017-1,
	title = {Visual {Attention} {Models} for {Scene} {Text} {Recognition}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.158},
	abstract = {In this paper we propose an approach to lexicon-free recognition of text in scene images. Our approach relies on a LSTM-based soft visual attention model learned from convolutional features. A set of feature vectors are derived from an intermediate convolutional layer corresponding to different areas of the image. This permits encoding of spatial information into the image representation. In this way, the framework is able to learn how to selectively focus on different parts of the image. At every time step the recognizer emits one character using a weighted combination of the convolutional feature vectors according to the learned attention model. Training can be done end-to-end using only word level annotations. In addition, we show that modifying the beam search algorithm by integrating an explicit language model leads to significantly better recognition results. We validate the performance of our approach on standard SVT and ICDAR'03 scene text datasets, showing state-of-the-art performance in unconstrained text recognition.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Ghosh, Suman K. and Valveny, Ernest and Bagdanov, Andrew D.},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Text recognition, Computational modeling, Adaptation models, Character recognition, Decoding, Image recognition, Visualization},
	pages = {943--948},
	file = {Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:/Users/johannesreichle/Zotero/storage/BTFTANIW/Ghosh et al_2017_Visual Attention Models for Scene Text Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/Y75GPGPY/8270089.html:text/html},
}

@article{cheng_aon_2018,
	title = {{AON}: {Towards} {Arbitrarily}-{Oriented} {Text} {Recognition}},
	shorttitle = {{AON}},
	url = {http://arxiv.org/abs/1711.04226},
	abstract = {Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.},
	urldate = {2022-02-08},
	journal = {arXiv:1711.04226 [cs]},
	author = {Cheng, Zhanzhan and Xu, Yangliu and Bai, Fan and Niu, Yi and Pu, Shiliang and Zhou, Shuigeng},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.04226},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR2018},
	file = {Cheng et al_2018_AON.pdf:/Users/johannesreichle/Zotero/storage/AYRF4K7P/Cheng et al_2018_AON.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/2RMUF6QB/1711.html:text/html},
}

@article{graves_connectionist_nodate,
	title = {Connectionist {Temporal} {Classiﬁcation}: {Labelling} {Unsegmented} {Sequence} {Data} with {Recurrent} {Neural} {Networks}},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
	language = {en},
	author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
	pages = {8},
	file = {Graves et al. - Connectionist Temporal Classiﬁcation Labelling Un.pdf:/Users/johannesreichle/Zotero/storage/TPUXA85X/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Un.pdf:application/pdf},
}

@inproceedings{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
	urldate = {2022-02-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	year = {2014},
	file = {Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf:/Users/johannesreichle/Zotero/storage/YWG58QKS/Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf:application/pdf},
}

@article{liao_scene_2018,
	title = {Scene {Text} {Recognition} from {Two}-{Dimensional} {Perspective}},
	url = {http://arxiv.org/abs/1809.06508},
	abstract = {Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.},
	urldate = {2022-02-08},
	journal = {arXiv:1809.06508 [cs]},
	author = {Liao, Minghui and Zhang, Jian and Wan, Zhaoyi and Xie, Fengming and Liang, Jiajun and Lyu, Pengyuan and Yao, Cong and Bai, Xiang},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.06508},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in AAAI 2019},
	file = {Liao et al_2018_Scene Text Recognition from Two-Dimensional Perspective.pdf:/Users/johannesreichle/Zotero/storage/SDCI2Z2H/Liao et al_2018_Scene Text Recognition from Two-Dimensional Perspective.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/XGU5YMYA/1809.html:text/html},
}

@article{xing_convolutional_2019-1,
	title = {Convolutional {Character} {Networks}},
	url = {http://arxiv.org/abs/1910.07954},
	abstract = {Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33\%-{\textgreater}71.08\% (with generic lexicon) on ICDAR 2015, and 54.0\%-{\textgreater}69.23\% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet.},
	urldate = {2022-02-08},
	journal = {arXiv:1910.07954 [cs]},
	author = {Xing, Linjie and Tian, Zhi and Huang, Weilin and Scott, Matthew R.},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07954},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2019},
	file = {Xing et al_2019_Convolutional Character Networks.pdf:/Users/johannesreichle/Zotero/storage/IA2M2QLK/Xing et al_2019_Convolutional Character Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/M96N342D/1910.html:text/html},
}

@article{wan_textscanner_2020,
	title = {{TextScanner}: {Reading} {Characters} in {Order} for {Robust} {Scene} {Text} {Recognition}},
	shorttitle = {{TextScanner}},
	url = {http://arxiv.org/abs/1912.12422},
	abstract = {Driven by deep learning and the large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention based methods have dominated this field, but suffer from the problem of {\textbackslash}textit\{attention drift\} in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such Chinese transcripts and aligning with target characters.},
	urldate = {2022-02-08},
	journal = {arXiv:1912.12422 [cs]},
	author = {Wan, Zhaoyi and He, Minghang and Chen, Haoran and Bai, Xiang and Yao, Cong},
	month = jan,
	year = {2020},
	note = {arXiv: 1912.12422},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted by AAAI-2020},
	file = {Wan et al_2020_TextScanner.pdf:/Users/johannesreichle/Zotero/storage/NMA7RDJL/Wan et al_2020_TextScanner.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/I4DDV968/1912.html:text/html},
}

@inproceedings{cong_comparative_2019,
	title = {A {Comparative} {Study} of {Attention}-{Based} {Encoder}-{Decoder} {Approaches} to {Natural} {Scene} {Text} {Recognition}},
	doi = {10.1109/ICDAR.2019.00151},
	abstract = {Attention-based encoder-decoder approaches have shown promising results in scene text recognition. In the literature, models with different encoders, decoders and attention mechanisms have been proposed and compared on isolated word recognition tasks, where the models are trained on either synthetic word images or a small set of real-world images. In this paper, we investigate different components of the attention based framework and compare its performance with a CNN-DBLSTM-CTC based approach on large-scale real-world scene text sentence recognition tasks. We train character models by using more than 1.6M real-world text lines and compare their performance on test sets collected from a variety of real-world scenarios. Our results show that (1) attention on a two-dimensional feature map can yield better performance than one-dimensional one and an RNN based decoder performs better than CNN based one; (2) attention-based approaches can achieve higher recognition accuracy than CNN-DBLSTM-CTC based approaches on isolated word recognition tasks, but perform worse on sentence recognition tasks; (3) it is more effective and efficient for CNN-DBLSTM-CTC based approaches to leverage an explicit language model to boost recognition accuracy.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Cong, Fuze and Hu, Wenping and Huo, Qiang and Guo, Li},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Attention, Connectionist Temporal Classification, Convolution, Decoding, Encoder-Decoder, Hidden Markov models, Kernel, Scene Text Recognition, Task analysis, Testing, Text recognition},
	pages = {916--921},
	file = {Cong et al_2019_A Comparative Study of Attention-Based Encoder-Decoder Approaches to Natural.pdf:/Users/johannesreichle/Zotero/storage/F5TII7YH/Cong et al_2019_A Comparative Study of Attention-Based Encoder-Decoder Approaches to Natural.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/johannesreichle/Zotero/storage/V4QR9UEE/8978138.html:text/html},
}

@inproceedings{xie_aggregation_2019,
	address = {Long Beach, CA, USA},
	title = {Aggregation {Cross}-{Entropy} for {Sequence} {Recognition}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953200/},
	doi = {10.1109/CVPR.2019.00670},
	abstract = {In this paper, we propose a novel method, aggregation cross-entropy (ACE), for sequence recognition from a brand new perspective. The ACE loss function exhibits competitive performance to CTC and the attention mechanism, with much quicker implementation (as it involves only four fundamental formulas), faster inference{\textbackslash}back-propagation (approximately O(1) in parallel), less storage requirement (no parameter and negligible runtime memory), and convenient employment (by replacing CTC with ACE). Furthermore, the proposed ACE loss function exhibits two noteworthy properties: (1) it can be directly applied for 2D prediction by ﬂattening the 2D prediction into 1D prediction as the input and (2) it requires only characters and their numbers in the sequence annotation for supervision, which allows it to advance beyond sequence recognition, e.g., counting problem. The code is publicly available at https://github.com/summerlvsong/Aggregation-CrossEntropy.},
	language = {en},
	urldate = {2022-02-08},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xie, Zecheng and Huang, Yaoxiong and Zhu, Yuanzhi and Jin, Lianwen and Liu, Yuliang and Xie, Lele},
	month = jun,
	year = {2019},
	pages = {6531--6540},
	file = {Xie et al. - 2019 - Aggregation Cross-Entropy for Sequence Recognition.pdf:/Users/johannesreichle/Zotero/storage/6ULNJIZM/Xie et al. - 2019 - Aggregation Cross-Entropy for Sequence Recognition.pdf:application/pdf},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2022-02-08},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/Users/johannesreichle/Zotero/storage/VTXBHHXJ/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/H9GPIR35/1409.html:text/html},
}

@book{imgrund_approaching_2018,
	title = {Approaching {Digitalization} with {Business} {Process} {Management}},
	abstract = {As digitalization sustainably alters industries and societies, small and medium-sized enterprises (SME) must initiate a digital transformation to remain competitive and to address the increasing complexity of customer needs. Although many enterprises encounter challenges in practice, research does not yet provide practicable recommendations to increase the feasibility of digitalization. Furthermore, SME frequently fail to fully realize the implications of digitalization for their organizational structures, strategies, and operations, and have difficulties to identify a suitable starting point for corresponding initiatives. In order to address these challenges, this paper uses the concept of Business Process Management (BPM) to define a set of capabilities for a management framework, which builds upon the paradigm of process orientation to cope with the various requirements of digital transformation. Our findings suggest that enterprises can use a functioning BPM as a starting point for digitalization, while establishing necessary digital capabilities subsequently.},
	author = {Imgrund, Florian and Fischer, Marcus and Janiesch, Christian and Winkelmann, Axel},
	month = mar,
	year = {2018},
}

@article{legner_digitalization_2017,
	title = {Digitalization: {Opportunity} and {Challenge} for the {Business} and {Information} {Systems} {Engineering} {Community}},
	volume = {59},
	issn = {2363-7005, 1867-0202},
	shorttitle = {Digitalization},
	url = {http://link.springer.com/10.1007/s12599-017-0484-2},
	doi = {10.1007/s12599-017-0484-2},
	language = {en},
	number = {4},
	urldate = {2022-02-09},
	journal = {Business \& Information Systems Engineering},
	author = {Legner, Christine and Eymann, Torsten and Hess, Thomas and Matt, Christian and Böhmann, Tilo and Drews, Paul and Mädche, Alexander and Urbach, Nils and Ahlemann, Frederik},
	month = aug,
	year = {2017},
	pages = {301--308},
	file = {Legner et al. - 2017 - Digitalization Opportunity and Challenge for the .pdf:/Users/johannesreichle/Zotero/storage/9URDQQSK/Legner et al. - 2017 - Digitalization Opportunity and Challenge for the .pdf:application/pdf},
}

@article{goodhue_impact_1992,
	title = {The {Impact} of {Data} {Integration} on the {Costs} and {Benefits} of {Information} {Systems}},
	volume = {16},
	issn = {02767783},
	url = {https://www.jstor.org/stable/249530?origin=crossref},
	doi = {10.2307/249530},
	language = {en},
	number = {3},
	urldate = {2022-02-09},
	journal = {MIS Quarterly},
	author = {Goodhue, Dale L. and Wybo, Michael D. and Kirsch, Laurie J.},
	month = sep,
	year = {1992},
	pages = {293},
	file = {Goodhue et al. - 1992 - The Impact of Data Integration on the Costs and Be.pdf:/Users/johannesreichle/Zotero/storage/NI3PSF9W/Goodhue et al. - 1992 - The Impact of Data Integration on the Costs and Be.pdf:application/pdf},
}

@article{wilson_accuracy_1999,
	title = {Accuracy of {Digitization} {Using} {Automated} and {Manual} {Methods}},
	volume = {79},
	issn = {0031-9023, 1538-6724},
	url = {https://academic.oup.com/ptj/article/79/6/558/2837067},
	doi = {10.1093/ptj/79.6.558},
	abstract = {Abstract
            Background and Purpose. Computerized 3-dimensional (3-D) motion measurement systems are used by those interested in human motion. The purposes of this study were (1) to determine the limits of accuracy in determining intersegmental angles during pendular motion at varying speeds and (2) to determine changes in accuracy introduced by autodigitization and digitization by experienced manual raters. Methods. Angular speed of a T-shaped pendulum was systematically increased by releasing the pendulum from 4 angles (0° [no movement], 45°, 90°, and 120°). Twelve reference angles calculated from markers placed on the pendulum were estimated over 20 frames for 10 trials at each release position. Results. Mean errors across trials and frames for intersegmental angles reconstructed by a 3-D motion measurement system were within ±1 degree across all release positions. An analysis of variance and a post hocTukey test revealed that the mean error for the autodigitized trials was larger than that for the manually digitized trials. For the autodigitized trials, the static trials (release position=0°) produced less mean error than the trials with movement produced. The ICCs showed a high degree of consistency among all raters, ranging from .707 to .999. Conclusion and Discussion. Our findings support the conclusion that under carefully controlled conditions, a 3-D motion measurement system can produce clinically acceptable measurements of accuracy across a range of angular speeds. Furthermore, acceptable accuracy is possible regardless of the digitization method.},
	language = {en},
	number = {6},
	urldate = {2022-02-09},
	journal = {Physical Therapy},
	author = {Wilson, Daniel J and Smith, Bryan K and Gibson, J Kyle and Choe, Byung K and Gaba, Brenda C and Voelz, John T},
	month = jun,
	year = {1999},
	pages = {558--566},
	file = {Wilson et al. - 1999 - Accuracy of Digitization Using Automated and Manua.pdf:/Users/johannesreichle/Zotero/storage/XM8V4SJ4/Wilson et al. - 1999 - Accuracy of Digitization Using Automated and Manua.pdf:application/pdf},
}

@article{xu_show_nodate,
	title = {Show, {Attend} and {Tell}: - {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	language = {en},
	author = {Xu, Kelvin},
	pages = {46},
	file = {Xu - Show, Attend and Tell - Neural Image Caption Gene.pdf:/Users/johannesreichle/Zotero/storage/I3C3WIL9/Xu - Show, Attend and Tell - Neural Image Caption Gene.pdf:application/pdf},
}

@article{xu_show_2016,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	shorttitle = {Show, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	urldate = {2022-02-10},
	journal = {arXiv:1502.03044 [cs]},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1502.03044},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Xu et al_2016_Show, Attend and Tell.pdf:/Users/johannesreichle/Zotero/storage/RF8UVIIR/Xu et al_2016_Show, Attend and Tell.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/S5YEAUTG/1502.html:text/html},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2022-02-10},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:/Users/johannesreichle/Zotero/storage/MKT4KAGC/Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf;arXiv.org Snapshot:/Users/johannesreichle/Zotero/storage/HWGCAWP6/1409.html:text/html},
}

@inproceedings{cheng_focusing_2017,
	address = {Venice},
	title = {Focusing {Attention}: {Towards} {Accurate} {Text} {Recognition} in {Natural} {Images}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {Focusing {Attention}},
	url = {http://ieeexplore.ieee.org/document/8237805/},
	doi = {10.1109/ICCV.2017.543},
	abstract = {Scene text recognition has been a hot research topic in computer vision due to its various applications. The state of the art is the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. However, we observe that existing attention-based methods perform poorly on complicated and/or low-quality images. One major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images. We call this phenomenon “attention drift”. To tackle this problem, in this paper we propose the FAN (the abbreviation of Focusing Attention Network) method that employs a focusing attention mechanism to automatically draw back the drifted attention. FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images. Furthermore, different from the existing methods, we adopt a ResNet-based network to enrich deep representations of scene text images. Extensive experiments on various benchmarks, including the IIIT5k, SVT and ICDAR datasets, show that the FAN method substantially outperforms the existing methods.},
	language = {en},
	urldate = {2022-02-11},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Cheng, Zhanzhan and Bai, Fan and Xu, Yunlu and Zheng, Gang and Pu, Shiliang and Zhou, Shuigeng},
	month = oct,
	year = {2017},
	pages = {5086--5094},
	file = {Cheng et al. - 2017 - Focusing Attention Towards Accurate Text Recognit.pdf:/Users/johannesreichle/Zotero/storage/RRVNUP3V/Cheng et al. - 2017 - Focusing Attention Towards Accurate Text Recognit.pdf:application/pdf},
}
